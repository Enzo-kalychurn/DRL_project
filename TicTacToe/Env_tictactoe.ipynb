{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "267e876c",
      "metadata": {
        "id": "267e876c"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import time\n",
        "import random"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "607fc62a",
      "metadata": {
        "id": "607fc62a"
      },
      "source": [
        "Implémentation de l'environnement TicTacToe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b9e4e9a8",
      "metadata": {
        "id": "b9e4e9a8"
      },
      "outputs": [],
      "source": [
        "class TicTacToe:\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.board = [' '] * 9\n",
        "        self.current_player = 'X'\n",
        "        return self.board.copy()\n",
        "\n",
        "    def step(self, action):\n",
        "        if self.board[action] != ' ':\n",
        "            return self.board.copy(), -10, True, {\"invalid\": True}\n",
        "\n",
        "        self.board[action] = self.current_player\n",
        "        if self.check_win(self.current_player):\n",
        "            return self.board.copy(), 1, True, {}\n",
        "\n",
        "        if ' ' not in self.board:\n",
        "            return self.board.copy(), 0, True, {}\n",
        "\n",
        "        # adversaire random\n",
        "        opponent_move = random.choice([i for i, v in enumerate(self.board) if v == ' '])\n",
        "        self.board[opponent_move] = 'O'\n",
        "        if self.check_win('O'):\n",
        "            return self.board.copy(), -1, True, {}\n",
        "\n",
        "        return self.board.copy(), 0, False, {}\n",
        "\n",
        "    def check_win(self, player):\n",
        "        wins = [(0,1,2), (3,4,5), (6,7,8), (0,3,6), (1,4,7), (2,5,8), (0,4,8), (2,4,6)]\n",
        "        return any(all(self.board[i] == player for i in combo) for combo in wins)\n",
        "\n",
        "    def render(self):\n",
        "        for i in range(3):\n",
        "            print(self.board[3*i:3*(i+1)])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cca5e06d",
      "metadata": {
        "id": "cca5e06d"
      },
      "source": [
        "Random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "565a071d",
      "metadata": {
        "id": "565a071d"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "class RandomAgent:\n",
        "    def select_action(self, state):\n",
        "        return random.choice([i for i, v in enumerate(state) if v == ' '])\n",
        "\n",
        "    def learn(self, *args, **kwargs):\n",
        "        pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f593c892",
      "metadata": {
        "id": "f593c892"
      },
      "outputs": [],
      "source": [
        "def train_agent(agent, env, episodes=1000):\n",
        "    rewards = []\n",
        "    lengths = []\n",
        "    for ep in range(episodes):\n",
        "        state = env.reset()\n",
        "        done = False\n",
        "        ep_reward = 0\n",
        "        steps = 0\n",
        "        while not done:\n",
        "            action = agent.select_action(state)\n",
        "            next_state, reward, done, info = env.step(action)\n",
        "            agent.learn(state, action, reward, next_state, done)\n",
        "            state = next_state\n",
        "            ep_reward += reward\n",
        "            steps += 1\n",
        "        rewards.append(ep_reward)\n",
        "        lengths.append(steps)\n",
        "        if (ep + 1) % 100 == 0:\n",
        "            print(f\"Episode {ep+1}/{episodes} - Moyenne des 100 derniers rewards : {np.mean(rewards[-100:]):.3f}\")\n",
        "    return rewards, lengths\n",
        "\n",
        "def evaluate_agent(agent, env, episodes=100):\n",
        "    rewards = []\n",
        "    lengths = []\n",
        "    action_times = []\n",
        "    for _ in range(episodes):\n",
        "        state = env.reset()\n",
        "        done = False\n",
        "        ep_reward = 0\n",
        "        steps = 0\n",
        "        while not done:\n",
        "            start = time.time()\n",
        "            action = agent.select_action(state)\n",
        "            action_times.append(time.time() - start)\n",
        "            state, reward, done, info = env.step(action)\n",
        "            ep_reward += reward\n",
        "            steps += 1\n",
        "        rewards.append(ep_reward)\n",
        "        lengths.append(steps)\n",
        "    return {\n",
        "        \"score_moyen\": np.mean(rewards),\n",
        "        \"longueur_moyenne\": np.mean(lengths),\n",
        "        \"temps_moyen_action\": np.mean(action_times) * 1000  # en ms\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "63c0dde6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 724
        },
        "id": "63c0dde6",
        "outputId": "6216afb6-0571-492f-f977-fcccfc86f110"
      },
      "outputs": [],
      "source": [
        "def plot_rewards(rewards, title=\"Récompenses par épisode\"):\n",
        "    plt.figure(figsize=(10, 4))\n",
        "    plt.plot(rewards, label='Reward par épisode')\n",
        "    plt.plot(np.convolve(rewards, np.ones(100)/100, mode='valid'),\n",
        "             label='Reward moyenne (fenêtre=50)', color='orange')\n",
        "    plt.xlabel('Épisodes')\n",
        "    plt.ylabel('Reward')\n",
        "    plt.title(title)\n",
        "    plt.grid()\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "env = TicTacToe()\n",
        "random_agent = RandomAgent()\n",
        "\n",
        "print(\" Entraînement de l'agent Random...\")\n",
        "rewards, lengths = train_agent(random_agent, env, episodes=1000)\n",
        "\n",
        "print(\"\\n Graphique des récompenses\")\n",
        "plot_rewards(rewards, title=\"Random Agent - TicTacToe\")\n",
        "\n",
        "print(\"\\n Évaluation de la policy Random\")\n",
        "results_random = evaluate_agent(random_agent, env, episodes=100)\n",
        "print(f\"Score moyen (ε=0): {results_random['score_moyen']:.3f}\")\n",
        "print(f\"Longueur moyenne : {results_random['longueur_moyenne']:.2f} steps\")\n",
        "print(f\"Temps moyen par action : {results_random['temps_moyen_action']:.3f} ms\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c3805d17",
      "metadata": {
        "id": "c3805d17"
      },
      "source": [
        "TabularQLearning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1d22c8bc",
      "metadata": {
        "id": "1d22c8bc"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "\n",
        "class TabularQLearningAgent:\n",
        "    def __init__(self, alpha=0.1, gamma=0.99, epsilon=1.0, epsilon_decay=0.995, epsilon_min=0.1):\n",
        "        self.q_table = defaultdict(lambda: np.zeros(9))\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon\n",
        "        self.epsilon_decay = epsilon_decay\n",
        "        self.epsilon_min = epsilon_min\n",
        "\n",
        "    def state_to_key(self, state):\n",
        "        return ''.join(state)\n",
        "\n",
        "    def select_action(self, state):\n",
        "        key = self.state_to_key(state)\n",
        "        if random.random() < self.epsilon:\n",
        "            available = [i for i, v in enumerate(state) if v == ' ']\n",
        "            return random.choice(available)\n",
        "        q_values = self.q_table[key].copy()\n",
        "        q_values = [q if state[i] == ' ' else -np.inf for i, q in enumerate(q_values)]\n",
        "        return int(np.argmax(q_values))\n",
        "\n",
        "    def learn(self, state, action, reward, next_state, done):\n",
        "        key = self.state_to_key(state)\n",
        "        next_key = self.state_to_key(next_state)\n",
        "\n",
        "        max_next_q = max([self.q_table[next_key][i] for i, v in enumerate(next_state) if v == ' '], default=0)\n",
        "        td_target = reward + self.gamma * max_next_q\n",
        "        td_error = td_target - self.q_table[key][action]\n",
        "        self.q_table[key][action] += self.alpha * td_error\n",
        "\n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon *= self.epsilon_decay\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3e5555fa",
      "metadata": {
        "id": "3e5555fa"
      },
      "outputs": [],
      "source": [
        "def train_agent(agent, env, episodes=1000):\n",
        "    rewards = []\n",
        "    lengths = []\n",
        "    for ep in range(episodes):\n",
        "        state = env.reset()\n",
        "        done = False\n",
        "        ep_reward = 0\n",
        "        steps = 0\n",
        "        while not done:\n",
        "            action = agent.select_action(state)\n",
        "            next_state, reward, done, info = env.step(action)\n",
        "            agent.learn(state, action, reward, next_state, done)\n",
        "            state = next_state\n",
        "            ep_reward += reward\n",
        "            steps += 1\n",
        "        rewards.append(ep_reward)\n",
        "        lengths.append(steps)\n",
        "        if (ep + 1) % 100 == 0:\n",
        "            print(f\"Episode {ep+1}/{episodes} - Moyenne des 100 derniers rewards : {np.mean(rewards[-100:]):.3f}\")\n",
        "    return rewards, lengths\n",
        "\n",
        "def evaluate_agent(agent, env, episodes=100):\n",
        "    rewards = []\n",
        "    lengths = []\n",
        "    action_times = []\n",
        "    \n",
        "    for _ in range(episodes):\n",
        "        state = env.reset()\n",
        "        done = False\n",
        "        ep_reward = 0\n",
        "        steps = 0\n",
        "        while not done:\n",
        "            start = time.time()\n",
        "            action = agent.select_action(state)\n",
        "            action_times.append(time.time() - start)\n",
        "            state, reward, done, info = env.step(action)\n",
        "            ep_reward += reward\n",
        "            steps += 1\n",
        "        rewards.append(ep_reward)\n",
        "        lengths.append(steps)\n",
        "    return {\n",
        "        \"score_moyen\": np.mean(rewards),\n",
        "        \"longueur_moyenne\": np.mean(lengths),\n",
        "        \"temps_moyen_action\": np.mean(action_times) * 1000  # ms\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "58201e66",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 724
        },
        "id": "58201e66",
        "outputId": "631c2ca2-a86c-4191-9ab2-a3e59b35fa76"
      },
      "outputs": [],
      "source": [
        "def plot_rewards(rewards, title=\"Récompenses par épisode\"):\n",
        "    plt.figure(figsize=(10, 4))\n",
        "    plt.plot(rewards, label='Reward par épisode')\n",
        "    plt.plot(np.convolve(rewards, np.ones(50)/50, mode='valid'),\n",
        "             label='Reward moyenne (fenêtre=50)', color='orange')\n",
        "    plt.xlabel('Épisodes')\n",
        "    plt.ylabel('Reward')\n",
        "    plt.title(title)\n",
        "    plt.grid()\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "env = TicTacToe()\n",
        "tabular_agent = TabularQLearningAgent()\n",
        "\n",
        "print(\" Entraînement de l'agent Tabular Q-Learning...\")\n",
        "rewards_tabular, lengths_tabular = train_agent(tabular_agent, env, episodes=1000)\n",
        "\n",
        "print(\"\\n Graphique des récompenses\")\n",
        "plot_rewards(rewards_tabular, title=\"Tabular Q-Learning - TicTacToe\")\n",
        "\n",
        "print(\"\\n Évaluation de la policy Tabular Q-Learning\")\n",
        "results_tabular = evaluate_agent(tabular_agent, env, episodes=100)\n",
        "print(f\"Score moyen (ε=0): {results_tabular['score_moyen']:.3f}\")\n",
        "print(f\"Longueur moyenne : {results_tabular['longueur_moyenne']:.2f} steps\")\n",
        "print(f\"Temps moyen par action : {results_tabular['temps_moyen_action']:.3f} ms\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "32599e5d",
      "metadata": {
        "id": "32599e5d"
      },
      "source": [
        "DeepQLearning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b0b4a979",
      "metadata": {},
      "outputs": [],
      "source": [
        "class DQNNetwork(nn.Module):\n",
        "    def __init__(self, input_dim=9, output_dim=9):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(input_dim, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, output_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "    \n",
        "def encode_board(board):\n",
        "    encoding = {' ': 0, 'X': 1, 'O': -1}\n",
        "    return np.array([encoding[cell] for cell in board], dtype=np.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3d5ae343",
      "metadata": {
        "id": "3d5ae343"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "class DQNAgent:\n",
        "    def __init__(self, lr=1e-3, gamma=0.99, epsilon=0.1, epsilon_decay=0.995, epsilon_min=0.1):\n",
        "        self.model = DQNNetwork()\n",
        "        self.optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
        "        self.criterion = nn.MSELoss()\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon\n",
        "        self.epsilon_decay = epsilon_decay\n",
        "        self.epsilon_min = epsilon_min\n",
        "\n",
        "    def select_action(self, state):\n",
        "        available = [i for i, v in enumerate(state) if v == ' ']\n",
        "        if random.random() < self.epsilon:\n",
        "            return random.choice(available)\n",
        "        with torch.no_grad():\n",
        "            state_encoded = torch.tensor(encode_board(state)).unsqueeze(0)\n",
        "            q_values = self.model(state_encoded)[0].numpy()\n",
        "            q_values = [q if state[i] == ' ' else -np.inf for i, q in enumerate(q_values)]\n",
        "            return int(np.argmax(q_values))\n",
        "\n",
        "    def learn(self, state, action, reward, next_state, done):\n",
        "        state_tensor = torch.tensor(encode_board(state)).unsqueeze(0)\n",
        "        next_state_tensor = torch.tensor(encode_board(next_state)).unsqueeze(0)\n",
        "        reward_tensor = torch.tensor(reward, dtype=torch.float32)\n",
        "\n",
        "        q_values = self.model(state_tensor)\n",
        "        next_q_values = self.model(next_state_tensor)\n",
        "\n",
        "        target = q_values.clone().detach()\n",
        "        if done:\n",
        "            target[0][action] = reward_tensor\n",
        "        else:\n",
        "            target[0][action] = reward_tensor + self.gamma * torch.max(next_q_values)\n",
        "\n",
        "        loss = self.criterion(q_values, target)\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon *= self.epsilon_decay\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ajcmfEltosF",
      "metadata": {
        "id": "5ajcmfEltosF"
      },
      "outputs": [],
      "source": [
        "def trainDeepQLearningAgent(env, agent, episodes=1000):\n",
        "    total_rewards = []\n",
        "    total_steps = []\n",
        "\n",
        "    for episode in range(episodes):\n",
        "        state = env.reset()\n",
        "        done = False\n",
        "        episode_reward = 0\n",
        "        steps = 0\n",
        "\n",
        "        while not done:\n",
        "            action = agent.select_action(state)\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "            agent.learn(state, action, reward, next_state, done)\n",
        "            state = next_state\n",
        "            episode_reward += reward\n",
        "            steps += 1\n",
        "\n",
        "        total_rewards.append(episode_reward)\n",
        "        total_steps.append(steps)\n",
        "\n",
        "        if (episode + 1) % 100 == 0:\n",
        "            print(f\"Épisode {episode+1}/{episodes} - Moyenne reward (100 derniers) : {np.mean(total_rewards[-100:]):.3f}\")\n",
        "\n",
        "    return total_rewards, total_steps\n",
        "\n",
        "def evaluateDeepQLearningAgent(agent, env, episodes=100):\n",
        "    original_epsilon = agent.epsilon\n",
        "    agent.epsilon = 0.0\n",
        "\n",
        "    total_rewards = []\n",
        "    total_steps = []\n",
        "    action_times = []\n",
        "\n",
        "    for _ in range(episodes):\n",
        "        state = env.reset()\n",
        "        done = False\n",
        "        reward_sum = 0\n",
        "        steps = 0\n",
        "\n",
        "        while not done:\n",
        "            start = time.time()\n",
        "            action = agent.select_action(state)\n",
        "            action_times.append(time.time() - start)\n",
        "            state, reward, done, _ = env.step(action)\n",
        "            reward_sum += reward\n",
        "            steps += 1\n",
        "\n",
        "        total_rewards.append(reward_sum)\n",
        "        total_steps.append(steps)\n",
        "\n",
        "    agent.epsilon = original_epsilon\n",
        "\n",
        "    print(\"\\n Évaluation de la policy DQN (ε = 0) :\")\n",
        "    print(f\"  - Score moyen : {np.mean(total_rewards):.3f}\")\n",
        "    print(f\"  - Longueur moyenne : {np.mean(total_steps):.2f} steps\")\n",
        "    print(f\"  - Temps moyen par action : {np.mean(action_times) * 1000:.3f} ms\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "TS0l870OuA0r",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 759
        },
        "id": "TS0l870OuA0r",
        "outputId": "d2fe99df-d845-46eb-ddd9-5b4955370207"
      },
      "outputs": [],
      "source": [
        "def plot_rewards(rewards, title=\"Deep Q-Learning - Récompenses par épisode\"):\n",
        "    plt.figure(figsize=(10, 4))\n",
        "    plt.plot(rewards, label=\"Reward brut\", alpha=0.3)\n",
        "    if len(rewards) >= 100:\n",
        "        plt.plot(np.convolve(rewards, np.ones(100)/100, mode=\"valid\"),\n",
        "                 label=\"Moyenne glissante (100)\", color=\"orange\")\n",
        "    plt.xlabel(\"Épisodes\")\n",
        "    plt.ylabel(\"Récompense\")\n",
        "    plt.ylim(-1.5, 1.5)\n",
        "    plt.title(title)\n",
        "    plt.legend()\n",
        "    plt.grid()\n",
        "    plt.show()\n",
        "\n",
        "env = TicTacToe()\n",
        "dqn_agent = DQNAgent()\n",
        "\n",
        "print(\" Entraînement de l'agent Deep Q-Learning...\")\n",
        "rewards_dqn, steps_dqn = trainDeepQLearningAgent(env, dqn_agent, episodes=1000)\n",
        "\n",
        "print(\"\\n Affichage du graphique des récompenses\")\n",
        "plot_rewards(rewards_dqn, title=\"Deep Q-Learning - TicTacToe\")\n",
        "\n",
        "print(\"\\n Résultats d'évaluation de la policy DQN\")\n",
        "evaluateDeepQLearningAgent(dqn_agent, env, episodes=100)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "745c5e67",
      "metadata": {
        "id": "745c5e67"
      },
      "source": [
        "DoubleDeepQLearningWithExperienceReplay"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5e4aeb26",
      "metadata": {
        "id": "5e4aeb26"
      },
      "outputs": [],
      "source": [
        "from collections import deque\n",
        "\n",
        "class ReplayBuffer:\n",
        "    def __init__(self, capacity=10000):\n",
        "        self.buffer = deque(maxlen=capacity)\n",
        "\n",
        "    def push(self, state, action, reward, next_state, done):\n",
        "        self.buffer.append((state.copy(), action, reward, next_state.copy(), done))\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        samples = random.sample(self.buffer, batch_size)\n",
        "        states, actions, rewards, next_states, dones = zip(*samples)\n",
        "        return list(states), list(actions), list(rewards), list(next_states), list(dones)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.buffer)\n",
        "\n",
        "class DoubleDeepQLearningWithExperienceReplay:\n",
        "    def __init__(self, gamma=0.99, epsilon=1.0, epsilon_decay=0.995, epsilon_min=0.1, lr=1e-3, batch_size=32):\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon\n",
        "        self.epsilon_decay = epsilon_decay\n",
        "        self.epsilon_min = epsilon_min\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        self.model = DQNNetwork()\n",
        "        self.target_model = DQNNetwork()\n",
        "        self.target_model.load_state_dict(self.model.state_dict())\n",
        "\n",
        "        self.optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
        "        self.criterion = nn.MSELoss()\n",
        "        self.replay_buffer = ReplayBuffer()\n",
        "        self.update_counter = 0\n",
        "\n",
        "    def board_to_tensor(self, board):\n",
        "        return torch.FloatTensor([(1 if x == 'X' else -1 if x == 'O' else 0) for x in board])\n",
        "\n",
        "    def select_action(self, board):\n",
        "        valid_moves = [i for i, v in enumerate(board) if v == ' ']\n",
        "        if random.random() < self.epsilon:\n",
        "            return random.choice(valid_moves)\n",
        "        with torch.no_grad():\n",
        "            state_tensor = self.board_to_tensor(board).unsqueeze(0)\n",
        "            q_values = self.model(state_tensor).squeeze()\n",
        "            q_values = [q_values[i] if i in valid_moves else -1e9 for i in range(9)]\n",
        "            return int(torch.argmax(torch.tensor(q_values)).item())\n",
        "\n",
        "    def learn(self, state, action, reward, next_state, done):\n",
        "        self.replay_buffer.push(state, action, reward, next_state, done)\n",
        "        if len(self.replay_buffer) < self.batch_size:\n",
        "            return\n",
        "\n",
        "        states, actions, rewards, next_states, dones = self.replay_buffer.sample(self.batch_size)\n",
        "\n",
        "        state_tensors = torch.stack([self.board_to_tensor(s) for s in states])\n",
        "        next_state_tensors = torch.stack([self.board_to_tensor(ns) for ns in next_states])\n",
        "        actions_tensor = torch.tensor(actions, dtype=torch.long)\n",
        "        rewards_tensor = torch.tensor(rewards, dtype=torch.float32)\n",
        "        dones_tensor = torch.tensor(dones, dtype=torch.float32)\n",
        "\n",
        "        q_values = self.model(state_tensors).gather(1, actions_tensor.unsqueeze(1)).squeeze()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            next_actions = torch.argmax(self.model(next_state_tensors), dim=1)\n",
        "            next_q_values = self.target_model(next_state_tensors).gather(1, next_actions.unsqueeze(1)).squeeze()\n",
        "            target_q_values = rewards_tensor + (1 - dones_tensor) * self.gamma * next_q_values\n",
        "\n",
        "        loss = self.criterion(q_values, target_q_values)\n",
        "\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        self.update_counter += 1\n",
        "        if self.update_counter % 10 == 0:\n",
        "            self.target_model.load_state_dict(self.model.state_dict())\n",
        "\n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon *= self.epsilon_decay\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "GjIPpKvUn-s-",
      "metadata": {
        "id": "GjIPpKvUn-s-"
      },
      "outputs": [],
      "source": [
        "def trainDoubleDQNReplayAgent(env, agent, episodes=1000):\n",
        "    rewards = []\n",
        "    steps = []\n",
        "\n",
        "    for ep in range(episodes):\n",
        "        state = env.reset()\n",
        "        done = False\n",
        "        ep_reward = 0\n",
        "        ep_steps = 0\n",
        "\n",
        "        while not done:\n",
        "            action = agent.select_action(state)\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "            agent.learn(state, action, reward, next_state, done)\n",
        "            state = next_state\n",
        "            ep_reward += reward\n",
        "            ep_steps += 1\n",
        "\n",
        "        rewards.append(ep_reward)\n",
        "        steps.append(ep_steps)\n",
        "\n",
        "        if (ep + 1) % 100 == 0:\n",
        "            print(f\"Épisode {ep+1}/{episodes} - Moyenne des 100 derniers : {np.mean(rewards[-100:]):.3f}\")\n",
        "\n",
        "    return rewards, steps\n",
        "\n",
        "def evaluateDoubleDQNReplayAgent(agent, env, episodes=100):\n",
        "    original_epsilon = agent.epsilon\n",
        "    agent.epsilon = 0.0\n",
        "\n",
        "    rewards = []\n",
        "    lengths = []\n",
        "    action_times = []\n",
        "\n",
        "    for _ in range(episodes):\n",
        "        state = env.reset()\n",
        "        done = False\n",
        "        ep_reward = 0\n",
        "        ep_steps = 0\n",
        "\n",
        "        while not done:\n",
        "            start = time.time()\n",
        "            action = agent.select_action(state)\n",
        "            action_times.append(time.time() - start)\n",
        "            state, reward, done, _ = env.step(action)\n",
        "            ep_reward += reward\n",
        "            ep_steps += 1\n",
        "\n",
        "        rewards.append(ep_reward)\n",
        "        lengths.append(ep_steps)\n",
        "\n",
        "    agent.epsilon = original_epsilon\n",
        "\n",
        "    print(\"\\n Évaluation de la policy Double DQN + Replay (ε = 0) :\")\n",
        "    print(f\"  - Score moyen : {np.mean(rewards):.3f}\")\n",
        "    print(f\"  - Longueur moyenne : {np.mean(lengths):.2f} steps\")\n",
        "    print(f\"  - Temps moyen par action : {np.mean(action_times) * 1000:.3f} ms\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ZXreFBsoLal",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 759
        },
        "id": "5ZXreFBsoLal",
        "outputId": "6d809575-a243-4cfd-97e2-5b7f630b2d42"
      },
      "outputs": [],
      "source": [
        "def plot_rewards(rewards, title=\"Double DQN + Replay - Récompenses par épisode\"):\n",
        "    plt.figure(figsize=(10, 4))\n",
        "    plt.plot(rewards, label=\"Reward brut\", alpha=0.3)\n",
        "    if len(rewards) >= 100:\n",
        "        plt.plot(np.convolve(rewards, np.ones(100)/100, mode=\"valid\"),\n",
        "                 label=\"Moyenne glissante (100)\", color=\"orange\")\n",
        "    plt.xlabel(\"Épisodes\")\n",
        "    plt.ylabel(\"Récompense\")\n",
        "    plt.ylim(-1.5, 1.5)\n",
        "    plt.title(title)\n",
        "    plt.legend()\n",
        "    plt.grid()\n",
        "    plt.show()\n",
        "\n",
        "env = TicTacToe()\n",
        "ddqn_replay_agent = DoubleDeepQLearningWithExperienceReplay()\n",
        "\n",
        "print(\" Entraînement de l'agent Double DQN avec Experience Replay...\")\n",
        "rewards_ddqn, steps_ddqn = trainDoubleDQNReplayAgent(env, ddqn_replay_agent, episodes=1000)\n",
        "\n",
        "print(\"\\n Affichage du graphique\")\n",
        "plot_rewards(rewards_ddqn, title=\"Double DQN avec Experience Replay - TicTacToe\")\n",
        "\n",
        "print(\"\\n Évaluation finale\")\n",
        "evaluateDoubleDQNReplayAgent(ddqn_replay_agent, env, episodes=100)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a66c7648",
      "metadata": {
        "id": "a66c7648"
      },
      "source": [
        "DoubleDeepQLearningWithPrioritizedExperienceReplay\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d37b348c",
      "metadata": {},
      "source": [
        "Buffer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9ee124f0",
      "metadata": {},
      "outputs": [],
      "source": [
        "class PrioritizedReplayBuffer:\n",
        "    def __init__(self, capacity=10000, alpha=0.6):\n",
        "        self.capacity = capacity\n",
        "        self.buffer = []\n",
        "        self.priorities = []\n",
        "        self.alpha = alpha\n",
        "\n",
        "    def push(self, state, action, reward, next_state, done):\n",
        "        max_priority = max(self.priorities, default=1.0)\n",
        "        self.buffer.append((state.copy(), action, reward, next_state.copy(), done))\n",
        "        self.priorities.append(max_priority)\n",
        "\n",
        "        if len(self.buffer) > self.capacity:\n",
        "            self.buffer.pop(0)\n",
        "            self.priorities.pop(0)\n",
        "\n",
        "    def sample(self, batch_size, beta=0.4):\n",
        "        if len(self.buffer) == 0:\n",
        "            return [], [], [], [], [], []\n",
        "\n",
        "        priorities = np.array(self.priorities, dtype=np.float32)\n",
        "        probs = priorities ** self.alpha\n",
        "        probs /= probs.sum()\n",
        "\n",
        "        indices = np.random.choice(len(self.buffer), batch_size, p=probs)\n",
        "        samples = [self.buffer[i] for i in indices]\n",
        "        total = len(self.buffer)\n",
        "        weights = (total * probs[indices]) ** (-beta)\n",
        "        weights /= weights.max()\n",
        "\n",
        "        states, actions, rewards, next_states, dones = zip(*samples)\n",
        "        return list(states), list(actions), list(rewards), list(next_states), list(dones), list(weights), indices\n",
        "\n",
        "    def update_priorities(self, indices, errors, eps=1e-6):\n",
        "        for i, err in zip(indices, errors):\n",
        "            self.priorities[i] = abs(err) + eps\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.buffer)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b8396e48",
      "metadata": {
        "id": "b8396e48"
      },
      "outputs": [],
      "source": [
        "class DoubleDQNWithPERAgent:\n",
        "    def __init__(self, gamma=0.99, epsilon=1.0, epsilon_decay=0.995, epsilon_min=0.1, lr=1e-3, batch_size=32):\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon\n",
        "        self.epsilon_decay = epsilon_decay\n",
        "        self.epsilon_min = epsilon_min\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        self.model = DQNNetwork()\n",
        "        self.target_model = DQNNetwork()\n",
        "        self.target_model.load_state_dict(self.model.state_dict())\n",
        "\n",
        "        self.optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
        "        self.criterion = nn.MSELoss(reduction='none')\n",
        "        self.replay_buffer = PrioritizedReplayBuffer()\n",
        "        self.update_counter = 0\n",
        "\n",
        "    def board_to_tensor(self, board):\n",
        "        return torch.FloatTensor([(1 if x == 'X' else -1 if x == 'O' else 0) for x in board])\n",
        "\n",
        "    def select_action(self, board):\n",
        "        valid_moves = [i for i, v in enumerate(board) if v == ' ']\n",
        "        if random.random() < self.epsilon:\n",
        "            return random.choice(valid_moves)\n",
        "        with torch.no_grad():\n",
        "            state_tensor = self.board_to_tensor(board).unsqueeze(0)\n",
        "            q_values = self.model(state_tensor).squeeze()\n",
        "            q_values = [q_values[i] if i in valid_moves else -1e9 for i in range(9)]\n",
        "            return int(torch.argmax(torch.tensor(q_values)).item())\n",
        "\n",
        "    def learn(self, state, action, reward, next_state, done):\n",
        "        self.replay_buffer.push(state, action, reward, next_state, done)\n",
        "        if len(self.replay_buffer) < self.batch_size:\n",
        "            return\n",
        "\n",
        "        states, actions, rewards, next_states, dones, weights, indices = self.replay_buffer.sample(self.batch_size)\n",
        "\n",
        "        state_tensors = torch.stack([self.board_to_tensor(s) for s in states])\n",
        "        next_state_tensors = torch.stack([self.board_to_tensor(ns) for ns in next_states])\n",
        "        actions_tensor = torch.tensor(actions, dtype=torch.long)\n",
        "        rewards_tensor = torch.tensor(rewards, dtype=torch.float32)\n",
        "        dones_tensor = torch.tensor(dones, dtype=torch.float32)\n",
        "        weights_tensor = torch.tensor(weights, dtype=torch.float32)\n",
        "\n",
        "        q_values = self.model(state_tensors).gather(1, actions_tensor.unsqueeze(1)).squeeze()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            next_actions = torch.argmax(self.model(next_state_tensors), dim=1)\n",
        "            next_q_values = self.target_model(next_state_tensors).gather(1, next_actions.unsqueeze(1)).squeeze()\n",
        "            target_q_values = rewards_tensor + (1 - dones_tensor) * self.gamma * next_q_values\n",
        "\n",
        "        loss_elements = self.criterion(q_values, target_q_values)\n",
        "        loss = (loss_elements * weights_tensor).mean()\n",
        "\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        errors = (q_values - target_q_values).detach().abs().tolist()\n",
        "        self.replay_buffer.update_priorities(indices, errors)\n",
        "\n",
        "        self.update_counter += 1\n",
        "        if self.update_counter % 10 == 0:\n",
        "            self.target_model.load_state_dict(self.model.state_dict())\n",
        "\n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon *= self.epsilon_decay\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ae33b379",
      "metadata": {},
      "outputs": [],
      "source": [
        "def trainPERAgent(env, agent, episodes=1000):\n",
        "    rewards = []\n",
        "    steps = []\n",
        "\n",
        "    for ep in range(episodes):\n",
        "        state = env.reset()\n",
        "        done = False\n",
        "        ep_reward = 0\n",
        "        ep_steps = 0\n",
        "\n",
        "        while not done:\n",
        "            action = agent.select_action(state)\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "            agent.learn(state, action, reward, next_state, done)\n",
        "            state = next_state\n",
        "            ep_reward += reward\n",
        "            ep_steps += 1\n",
        "\n",
        "        rewards.append(ep_reward)\n",
        "        steps.append(ep_steps)\n",
        "\n",
        "        if (ep + 1) % 100 == 0:\n",
        "            print(f\"Épisode {ep+1}/{episodes} - Moyenne reward (100 derniers) : {np.mean(rewards[-100:]):.3f}\")\n",
        "\n",
        "    return rewards, steps\n",
        "\n",
        "def evaluatePERAgent(agent, env, episodes=100):\n",
        "    original_epsilon = agent.epsilon\n",
        "    agent.epsilon = 0.0\n",
        "\n",
        "    rewards = []\n",
        "    steps = []\n",
        "    times = []\n",
        "    \n",
        "\n",
        "    for _ in range(episodes):\n",
        "        state = env.reset()\n",
        "        done = False\n",
        "        rsum = 0   \n",
        "        count = 0\n",
        "\n",
        "        while not done:\n",
        "            t0 = time.time()\n",
        "            action = agent.select_action(state)\n",
        "            times.append(time.time() - t0)\n",
        "            state, reward, done, _ = env.step(action)\n",
        "            rsum += reward\n",
        "            count += 1\n",
        "\n",
        "        rewards.append(rsum)\n",
        "        steps.append(count)\n",
        "\n",
        "    agent.epsilon = original_epsilon\n",
        "\n",
        "    print(\"\\n Évaluation Double DQN avec Prioritized Replay (ε = 0)\")\n",
        "    print(f\"  - Score moyen : {np.mean(rewards):.3f}\")\n",
        "    print(f\"  - Longueur moyenne : {np.mean(steps):.2f} steps\")\n",
        "    print(f\"  - Temps moyen par action : {np.mean(times)*1000:.3f} ms\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "16cdfac8",
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_rewards(rewards, title=\"Double DQN avec Prioritized Replay - Récompenses par épisode\"):\n",
        "    plt.figure(figsize=(10, 4))\n",
        "    plt.plot(rewards, label=\"Reward brut\", alpha=0.3)\n",
        "    if len(rewards) >= 100:\n",
        "        plt.plot(np.convolve(rewards, np.ones(100)/100, mode=\"valid\"),\n",
        "                 label=\"Moyenne glissante (100)\", color=\"orange\")\n",
        "    plt.xlabel(\"Épisodes\")\n",
        "    plt.ylabel(\"Récompense\")\n",
        "    plt.ylim(-1.5, 1.5)\n",
        "    plt.title(title)\n",
        "    plt.legend()\n",
        "    plt.grid()\n",
        "    plt.show()\n",
        "\n",
        "env = TicTacToe()\n",
        "ddqn_per_agent = DoubleDQNWithPERAgent()\n",
        "\n",
        "print(\"Entraînement de Double DQN avec Prioritized Experience Replay...\")\n",
        "rewards_per, steps_per = trainPERAgent(env, ddqn_per_agent, episodes=10000)\n",
        "\n",
        "print(\"\\n Graphe des récompenses\")\n",
        "plot_rewards(rewards_per, title=\"Double DQN avec Prioritized Replay - TicTacToe\")\n",
        "\n",
        "print(\"\\n Évaluation finale\")\n",
        "evaluatePERAgent(ddqn_per_agent, env, episodes=100)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0bd7a686",
      "metadata": {
        "id": "0bd7a686"
      },
      "source": [
        "REINFORCE\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2c4e7387",
      "metadata": {},
      "outputs": [],
      "source": [
        "class PolicyNetwork(nn.Module):\n",
        "    def __init__(self, input_dim=9, output_dim=9):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(input_dim, 128), nn.ReLU(),\n",
        "            nn.Linear(128, output_dim),\n",
        "            nn.Softmax(dim=-1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1f718b0c",
      "metadata": {
        "id": "1f718b0c"
      },
      "outputs": [],
      "source": [
        "class REINFORCEAgent:\n",
        "    def __init__(self, lr=1e-3, gamma=0.99):\n",
        "        self.policy = PolicyNetwork()\n",
        "        self.optimizer = optim.Adam(self.policy.parameters(), lr=lr)\n",
        "        self.gamma = gamma\n",
        "        self.trajectory = []\n",
        "\n",
        "    def board_to_tensor(self, board):\n",
        "        return torch.FloatTensor([(1 if x == 'X' else -1 if x == 'O' else 0) for x in board])\n",
        "\n",
        "    def select_action(self, board):\n",
        "        state_tensor = self.board_to_tensor(board).unsqueeze(0)\n",
        "        probs = self.policy(state_tensor).squeeze()\n",
        "        mask = torch.tensor([0.0 if board[i] == ' ' else -float('inf') for i in range(9)])\n",
        "        masked_probs = torch.softmax(probs + mask, dim=-1)\n",
        "        dist = torch.distributions.Categorical(masked_probs)\n",
        "        action = dist.sample()\n",
        "        self.trajectory.append((state_tensor, action, dist.log_prob(action)))\n",
        "        return int(action.item())\n",
        "\n",
        "    def learn(self, state, action, reward, next_state, done):\n",
        "        # handled at end of episode via update_policy\n",
        "        pass\n",
        "\n",
        "    def update_policy(self, rewards):\n",
        "        G = 0\n",
        "        returns = []\n",
        "        for r in reversed(rewards):\n",
        "            G = r + self.gamma * G\n",
        "            returns.insert(0, G)\n",
        "        returns = torch.tensor(returns)\n",
        "        returns = (returns - returns.mean()) / (returns.std() + 1e-9)\n",
        "\n",
        "        loss = 0\n",
        "        for (_, _, log_prob), Gt in zip(self.trajectory, returns):\n",
        "            loss -= log_prob * Gt\n",
        "\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "        self.trajectory = []\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7d9d78a7",
      "metadata": {},
      "outputs": [],
      "source": [
        "def trainREINFORCEAgent(env, agent, episodes=1000):\n",
        "    rewards = []\n",
        "    lengths = []\n",
        "\n",
        "    for ep in range(episodes):\n",
        "        state = env.reset()\n",
        "        done = False\n",
        "        ep_reward = 0\n",
        "        steps = 0\n",
        "        episode_rewards = []\n",
        "\n",
        "        while not done:\n",
        "            action = agent.select_action(state)\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "            episode_rewards.append(reward)\n",
        "            state = next_state\n",
        "            ep_reward += reward\n",
        "            steps += 1\n",
        "\n",
        "        agent.update_policy(episode_rewards)\n",
        "        rewards.append(ep_reward)\n",
        "        lengths.append(steps)\n",
        "\n",
        "        if (ep + 1) % 100 == 0:\n",
        "            print(f\"Épisode {ep+1}/{episodes} - Moyenne reward (100 derniers) : {np.mean(rewards[-100:]):.3f}\")\n",
        "\n",
        "    return rewards, lengths\n",
        "\n",
        "def evaluateREINFORCEAgent(agent, env, episodes=100):\n",
        "    rewards = []\n",
        "    steps = []\n",
        "    times = []\n",
        "\n",
        "    for _ in range(episodes):\n",
        "        state = env.reset()\n",
        "        done = False\n",
        "        total_r = 0\n",
        "        count = 0\n",
        "\n",
        "        while not done:\n",
        "            start = time.time()\n",
        "            action = agent.select_action(state)\n",
        "            times.append(time.time() - start)\n",
        "            state, reward, done, _ = env.step(action)\n",
        "            total_r += reward\n",
        "            count += 1\n",
        "\n",
        "        rewards.append(total_r)\n",
        "        steps.append(count)\n",
        "\n",
        "    print(\"\\n Évaluation de REINFORCE (ε = 0)\")\n",
        "    print(f\"  - Score moyen : {np.mean(rewards):.3f}\")\n",
        "    print(f\"  - Longueur moyenne : {np.mean(steps):.2f} steps\")\n",
        "    print(f\"  - Temps moyen par action : {np.mean(times)*1000:.3f} ms\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "743af913",
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_rewards(rewards, title=\"REINFORCE - Récompenses par épisode\"):\n",
        "    plt.figure(figsize=(10, 4))\n",
        "    plt.plot(rewards, label=\"Reward brut\", alpha=0.3)\n",
        "    if len(rewards) >= 100:\n",
        "        plt.plot(np.convolve(rewards, np.ones(100)/100, mode=\"valid\"),\n",
        "                 label=\"Moyenne glissante (100)\", color=\"orange\")\n",
        "    plt.xlabel(\"Épisodes\")\n",
        "    plt.ylabel(\"Récompense\")\n",
        "    plt.ylim(-1.5, 1.5)\n",
        "    plt.title(title)\n",
        "    plt.legend()\n",
        "    plt.grid()\n",
        "    plt.show()\n",
        "\n",
        "env = TicTacToe()\n",
        "reinforce_agent = REINFORCEAgent()\n",
        "\n",
        "print(\" Entraînement de l'agent REINFORCE...\")\n",
        "rewards_reinforce, steps_reinforce = trainREINFORCEAgent(env, reinforce_agent, episodes=10000)\n",
        "\n",
        "print(\"\\n Graphe des récompenses\")\n",
        "plot_rewards(rewards_reinforce, title=\"REINFORCE - TicTacToe\")\n",
        "\n",
        "print(\"\\n Évaluation finale\")\n",
        "evaluateREINFORCEAgent(reinforce_agent, env, episodes=100)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "536f8234",
      "metadata": {
        "id": "536f8234"
      },
      "source": [
        "REINFORCE with mean baseline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "22723567",
      "metadata": {
        "id": "22723567"
      },
      "outputs": [],
      "source": [
        "class REINFORCEWithMeanBaseline:\n",
        "    def __init__(self, lr=1e-3, gamma=0.99):\n",
        "        self.policy = PolicyNetwork()\n",
        "        self.optimizer = optim.Adam(self.policy.parameters(), lr=lr)\n",
        "        self.gamma = gamma\n",
        "        self.trajectory = []\n",
        "        self.return_history = []\n",
        "\n",
        "    def board_to_tensor(self, board):\n",
        "        return torch.FloatTensor([(1 if x == 'X' else -1 if x == 'O' else 0) for x in board])\n",
        "\n",
        "    def select_action(self, board):\n",
        "        state_tensor = self.board_to_tensor(board).unsqueeze(0)\n",
        "        probs = self.policy(state_tensor).squeeze()\n",
        "        mask = torch.tensor([0.0 if board[i] == ' ' else -float('inf') for i in range(9)])\n",
        "        masked_probs = torch.softmax(probs + mask, dim=-1)\n",
        "        dist = torch.distributions.Categorical(masked_probs)\n",
        "        action = dist.sample()\n",
        "        self.trajectory.append((state_tensor, action, dist.log_prob(action)))\n",
        "        return int(action.item())\n",
        "\n",
        "    def learn(self, rewards):\n",
        "        G = 0\n",
        "        returns = []\n",
        "        for r in reversed(rewards):\n",
        "            G = r + self.gamma * G\n",
        "            returns.insert(0, G)\n",
        "        returns = torch.tensor(returns, dtype=torch.float32)\n",
        "\n",
        "        self.return_history.extend(returns.tolist())\n",
        "        baseline = np.mean(self.return_history)\n",
        "\n",
        "        loss = 0\n",
        "        for (_, _, log_prob), Gt in zip(self.trajectory, returns):\n",
        "            advantage = Gt - baseline\n",
        "            loss -= log_prob * advantage\n",
        "\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "        self.trajectory = []\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "268d750a",
      "metadata": {},
      "outputs": [],
      "source": [
        "def trainForREINFORCEAgent(env, agent, episodes=1000):\n",
        "    rewards = []\n",
        "    steps = []\n",
        "\n",
        "    for ep in range(episodes):\n",
        "        state = env.reset()\n",
        "        done = False\n",
        "        ep_reward = 0\n",
        "        ep_steps = 0\n",
        "        ep_rewards = []\n",
        "\n",
        "        while not done:\n",
        "            action = agent.select_action(state)\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "            ep_rewards.append(reward)\n",
        "            state = next_state\n",
        "            ep_reward += reward\n",
        "            ep_steps += 1\n",
        "\n",
        "        agent.learn(ep_rewards)\n",
        "        rewards.append(ep_reward)\n",
        "        steps.append(ep_steps)\n",
        "\n",
        "        if (ep + 1) % 100 == 0:\n",
        "            print(f\"Épisode {ep+1}/{episodes} - Moyenne (100) : {np.mean(rewards[-100:]):.3f}\")\n",
        "\n",
        "    return rewards, steps\n",
        "\n",
        "def evaluateForREINFORCEAgent(agent, env, episodes=100):\n",
        "    rewards = []\n",
        "    steps = []\n",
        "    action_time = []\n",
        "\n",
        "    for _ in range(episodes):\n",
        "        state = env.reset()\n",
        "        done = False\n",
        "        rsum = 0\n",
        "        n = 0\n",
        "        while not done:\n",
        "            start = time.time()\n",
        "            action = agent.select_action(state)\n",
        "            action_time.append(time.time() - start)\n",
        "            state, reward, done, _ = env.step(action)\n",
        "            rsum += reward\n",
        "            n += 1\n",
        "        rewards.append(rsum)\n",
        "        steps.append(n)\n",
        "\n",
        "    print(\"\\n Évaluation :\")\n",
        "    print(f\"  - Score moyen : {np.mean(rewards):.3f}\")\n",
        "    print(f\"  - Longueur moyenne : {np.mean(steps):.2f} steps\")\n",
        "    print(f\"  - Temps moyen par action : {np.mean(action_time)*1000:.3f} ms\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f5ded283",
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_rewards(rewards, title=\"REINFORCE - Récompenses par épisode\"):\n",
        "    plt.figure(figsize=(10, 4))\n",
        "    plt.plot(rewards, label=\"Reward brut\", alpha=0.3)\n",
        "    if len(rewards) >= 100:\n",
        "        plt.plot(np.convolve(rewards, np.ones(100)/100, mode=\"valid\"),\n",
        "                 label=\"Moyenne glissante (100)\", color=\"orange\")\n",
        "    plt.xlabel(\"Épisodes\")\n",
        "    plt.ylabel(\"Récompense\")\n",
        "    plt.ylim(-1.5, 1.5)\n",
        "    plt.title(title)\n",
        "    plt.legend()\n",
        "    plt.grid()\n",
        "    plt.show()\n",
        "\n",
        "env = TicTacToe()\n",
        "agent_mb = REINFORCEWithMeanBaseline()\n",
        "\n",
        "rewards_mb, steps_mb = trainForREINFORCEAgent(env, agent_mb, episodes=1000)\n",
        "plot_rewards(rewards_mb, title=\"REINFORCE avec baseline moyenne - TicTacToe\")\n",
        "evaluateForREINFORCEAgent(agent_mb, env, episodes=100)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "84251585",
      "metadata": {
        "id": "84251585"
      },
      "source": [
        "REINFORCE with Baseline Learned by a Critic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b0a46068",
      "metadata": {
        "id": "b0a46068"
      },
      "outputs": [],
      "source": [
        "class ValueNetwork(nn.Module):\n",
        "    def __init__(self, input_dim=9):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(input_dim, 64), nn.ReLU(),\n",
        "            nn.Linear(64, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class REINFORCEWithCritic:\n",
        "    def __init__(self, lr_policy=1e-3, lr_value=1e-3, gamma=0.99):\n",
        "        self.policy = PolicyNetwork()\n",
        "        self.value_net = ValueNetwork()\n",
        "        self.policy_optimizer = optim.Adam(self.policy.parameters(), lr=lr_policy)\n",
        "        self.value_optimizer = optim.Adam(self.value_net.parameters(), lr=lr_value)\n",
        "        self.gamma = gamma\n",
        "        self.trajectory = []\n",
        "\n",
        "    def board_to_tensor(self, board):\n",
        "        return torch.FloatTensor([(1 if x == 'X' else -1 if x == 'O' else 0) for x in board])\n",
        "\n",
        "    def select_action(self, board):\n",
        "        state_tensor = self.board_to_tensor(board).unsqueeze(0)\n",
        "        probs = self.policy(state_tensor).squeeze()\n",
        "        mask = torch.tensor([0.0 if board[i] == ' ' else -float('inf') for i in range(9)])\n",
        "        masked_probs = torch.softmax(probs + mask, dim=-1)\n",
        "        dist = torch.distributions.Categorical(masked_probs)\n",
        "        action = dist.sample()\n",
        "        self.trajectory.append((state_tensor, action, dist.log_prob(action)))\n",
        "        return int(action.item())\n",
        "\n",
        "    def learn(self, rewards):\n",
        "        G = 0\n",
        "        returns = []\n",
        "        for r in reversed(rewards):\n",
        "            G = r + self.gamma * G\n",
        "            returns.insert(0, G)\n",
        "        returns = torch.tensor(returns, dtype=torch.float32)\n",
        "\n",
        "        # Update critic\n",
        "        for (state_tensor, _, _), Gt in zip(self.trajectory, returns):\n",
        "            value = self.value_net(state_tensor).squeeze()\n",
        "            loss_v = (Gt - value) ** 2\n",
        "            self.value_optimizer.zero_grad()\n",
        "            loss_v.backward()\n",
        "            self.value_optimizer.step()\n",
        "\n",
        "        # Update policy using advantage\n",
        "        loss = 0\n",
        "        for (state_tensor, _, log_prob), Gt in zip(self.trajectory, returns):\n",
        "            advantage = Gt - self.value_net(state_tensor).squeeze().detach()\n",
        "            loss -= log_prob * advantage\n",
        "\n",
        "        self.policy_optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.policy_optimizer.step()\n",
        "        self.trajectory = []\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f0a57a7d",
      "metadata": {},
      "outputs": [],
      "source": [
        "def trainForREINFORCEAgent(env, agent, episodes=1000):\n",
        "    rewards = []\n",
        "    steps = []\n",
        "\n",
        "    for ep in range(episodes):\n",
        "        state = env.reset()\n",
        "        done = False\n",
        "        ep_reward = 0\n",
        "        ep_steps = 0\n",
        "        ep_rewards = []\n",
        "\n",
        "        while not done:\n",
        "            action = agent.select_action(state)\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "            ep_rewards.append(reward)\n",
        "            state = next_state\n",
        "            ep_reward += reward\n",
        "            ep_steps += 1\n",
        "\n",
        "        agent.learn(ep_rewards)\n",
        "        rewards.append(ep_reward)\n",
        "        steps.append(ep_steps)\n",
        "\n",
        "        if (ep + 1) % 100 == 0:\n",
        "            print(f\"Épisode {ep+1}/{episodes} - Moyenne (100) : {np.mean(rewards[-100:]):.3f}\")\n",
        "\n",
        "    return rewards, steps\n",
        "\n",
        "def evaluateForREINFORCEAgent(agent, env, episodes=100):\n",
        "    rewards = []\n",
        "    steps = []\n",
        "    action_time = []\n",
        "\n",
        "    for _ in range(episodes):\n",
        "        state = env.reset()\n",
        "        done = False\n",
        "        rsum = 0\n",
        "        n = 0\n",
        "        while not done:\n",
        "            start = time.time()\n",
        "            action = agent.select_action(state)\n",
        "            action_time.append(time.time() - start)\n",
        "            state, reward, done, _ = env.step(action)\n",
        "            rsum += reward\n",
        "            n += 1\n",
        "        rewards.append(rsum)\n",
        "        steps.append(n)\n",
        "\n",
        "    print(\"\\n🎯 Évaluation :\")\n",
        "    print(f\"  - Score moyen : {np.mean(rewards):.3f}\")\n",
        "    print(f\"  - Longueur moyenne : {np.mean(steps):.2f} steps\")\n",
        "    print(f\"  - Temps moyen par action : {np.mean(action_time)*1000:.3f} ms\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "93955918",
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_rewards(rewards, title=\"REINFORCE - Récompenses par épisode\"):\n",
        "    plt.figure(figsize=(10, 4))\n",
        "    plt.plot(rewards, label=\"Reward brut\", alpha=0.3)\n",
        "    if len(rewards) >= 100:\n",
        "        plt.plot(np.convolve(rewards, np.ones(100)/100, mode=\"valid\"),\n",
        "                 label=\"Moyenne glissante (100)\", color=\"orange\")\n",
        "    plt.xlabel(\"Épisodes\")\n",
        "    plt.ylabel(\"Récompense\")\n",
        "    plt.ylim(-1.5, 1.5)\n",
        "    plt.title(title)\n",
        "    plt.legend()\n",
        "    plt.grid()\n",
        "    plt.show()\n",
        "\n",
        "env = TicTacToe()\n",
        "agent_mb = REINFORCEWithMeanBaseline()\n",
        "\n",
        "rewards_mb, steps_mb = trainForREINFORCEAgent(env, agent_mb, episodes=1000)\n",
        "plot_rewards(rewards_mb, title=\"REINFORCE avec baseline moyenne - TicTacToe\")\n",
        "evaluateForREINFORCEAgent(agent_mb, env, episodes=100)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2490ed4f",
      "metadata": {
        "id": "2490ed4f"
      },
      "source": [
        "PPO A2C style"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6f8d995d",
      "metadata": {
        "id": "6f8d995d"
      },
      "outputs": [],
      "source": [
        "class ActorCriticNetwork(nn.Module):\n",
        "    def __init__(self, input_dim=9, output_dim=9):\n",
        "        super().__init__()\n",
        "        self.shared = nn.Sequential(nn.Linear(input_dim, 128), nn.ReLU())\n",
        "        self.policy_head = nn.Sequential(nn.Linear(128, output_dim), nn.Softmax(dim=-1))\n",
        "        self.value_head = nn.Linear(128, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        shared = self.shared(x)\n",
        "        return self.policy_head(shared), self.value_head(shared)\n",
        "\n",
        "class A2CStyleAgent:\n",
        "    def __init__(self, gamma=0.99, lr=1e-3):\n",
        "        self.model = ActorCriticNetwork()\n",
        "        self.optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
        "        self.gamma = gamma\n",
        "        self.trajectory = []\n",
        "\n",
        "    def board_to_tensor(self, board):\n",
        "        return torch.FloatTensor([(1 if x == 'X' else -1 if x == 'O' else 0) for x in board])\n",
        "\n",
        "    def select_action(self, board):\n",
        "        state_tensor = self.board_to_tensor(board).unsqueeze(0)\n",
        "        probs, _ = self.model(state_tensor)\n",
        "        mask = torch.tensor([0.0 if board[i] == ' ' else -float('inf') for i in range(9)])\n",
        "        masked_probs = torch.softmax(probs.squeeze() + mask, dim=-1)\n",
        "        dist = torch.distributions.Categorical(masked_probs)\n",
        "        action = dist.sample()\n",
        "        self.trajectory.append((state_tensor, action, dist.log_prob(action)))\n",
        "        return int(action.item())\n",
        "\n",
        "    def learn(self, rewards):\n",
        "        G = 0\n",
        "        returns = []\n",
        "        for r in reversed(rewards):\n",
        "            G = r + self.gamma * G\n",
        "            returns.insert(0, G)\n",
        "        returns = torch.tensor(returns, dtype=torch.float32)\n",
        "\n",
        "        loss = 0\n",
        "        for (state_tensor, action, log_prob), Gt in zip(self.trajectory, returns):\n",
        "            probs, value = self.model(state_tensor)\n",
        "            advantage = Gt - value.squeeze()\n",
        "            loss += -log_prob * advantage.detach() + advantage.pow(2)\n",
        "\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "        self.trajectory = []\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1693cd88",
      "metadata": {},
      "source": [
        "Train pour RandomRollout, UCT et ExpertApprentice"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "83904b47",
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_basic_agent(env, agent, episodes=10000):\n",
        "    rewards = []\n",
        "    steps = []\n",
        "    for ep in range(episodes):\n",
        "        state = env.reset()\n",
        "        done = False\n",
        "        total_reward = 0\n",
        "        count = 0\n",
        "        while not done:\n",
        "            action = agent.select_action(state)\n",
        "            state, reward, done, _ = env.step(action)\n",
        "            total_reward += reward\n",
        "            count += 1\n",
        "        rewards.append(total_reward)\n",
        "        steps.append(count)\n",
        "        if (ep + 1) % 100 == 0:\n",
        "            print(f\"Épisode {ep+1}/ {episodes}- Moyenne reward (100 derniers) : {np.mean(rewards[-100:]):.3f}\")\n",
        "    return rewards, steps\n",
        "\n",
        "def evaluate_basic_agent(agent, env, episodes=100):\n",
        "    rewards = []\n",
        "    steps = []\n",
        "    times = []\n",
        "    for _ in range(episodes):\n",
        "        state = env.reset()\n",
        "        done = False\n",
        "        total_r = 0\n",
        "        count = 0\n",
        "        while not done:\n",
        "            t0 = time.time()\n",
        "            action = agent.select_action(state)\n",
        "            times.append(time.time() - t0)\n",
        "            state, reward, done, _ = env.step(action)\n",
        "            total_r += reward\n",
        "            count += 1\n",
        "        rewards.append(total_r)\n",
        "        steps.append(count)\n",
        "\n",
        "    print(\"\\n🧪 Évaluation\")\n",
        "    print(f\"  - Score moyen : {np.mean(rewards):.3f}\")\n",
        "    print(f\"  - Longueur moyenne : {np.mean(steps):.2f} steps\")\n",
        "    print(f\"  - Temps moyen par action : {np.mean(times)*1000:.3f} ms\")\n",
        "\n",
        "def plot_basic_rewards(rewards, title=\"Agent - Récompenses par épisode\"):\n",
        "    plt.figure(figsize=(10, 4))\n",
        "    plt.plot(rewards, label=\"Reward brut\", alpha=0.3)\n",
        "    if len(rewards) >= 100:\n",
        "        plt.plot(np.convolve(rewards, np.ones(100)/100, mode=\"valid\"),\n",
        "                 label=\"Moyenne glissante (100)\", color=\"orange\")\n",
        "    plt.xlabel(\"Épisodes\")\n",
        "    plt.ylabel(\"Récompense\")\n",
        "    plt.ylim(-1.5, 1.5)\n",
        "    plt.title(title)\n",
        "    plt.legend()\n",
        "    plt.grid()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "99257e4f",
      "metadata": {
        "id": "99257e4f"
      },
      "source": [
        "RandomRollout"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ff574250",
      "metadata": {
        "id": "ff574250"
      },
      "outputs": [],
      "source": [
        "class RandomRolloutAgent:\n",
        "    def __init__(self, simulations=20):\n",
        "        self.simulations = simulations\n",
        "\n",
        "    def simulate(self, board, move):\n",
        "        wins = 0\n",
        "        for _ in range(self.simulations):\n",
        "            sim_board = board[:]\n",
        "            sim_board[move] = 'X'\n",
        "            done = False\n",
        "            while not done:\n",
        "                empty = [i for i, v in enumerate(sim_board) if v == ' ']\n",
        "                if not empty:\n",
        "                    break\n",
        "                sim_board[random.choice(empty)] = 'O'\n",
        "                if TicTacToe().check_win('O'):\n",
        "                    break\n",
        "                empty = [i for i, v in enumerate(sim_board) if v == ' ']\n",
        "                if not empty:\n",
        "                    break\n",
        "                sim_board[random.choice(empty)] = 'X'\n",
        "                if TicTacToe().check_win('X'):\n",
        "                    wins += 1\n",
        "                    break\n",
        "        return wins\n",
        "\n",
        "    def select_action(self, board):\n",
        "        moves = [i for i, v in enumerate(board) if v == ' ']\n",
        "        scores = {m: self.simulate(board, m) for m in moves}\n",
        "        return max(scores, key=scores.get)\n",
        "\n",
        "    def learn(self, *args, **kwargs):\n",
        "        pass\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8d0adab9",
      "metadata": {},
      "outputs": [],
      "source": [
        "env = TicTacToe()\n",
        "agent_rollout = RandomRolloutAgent()\n",
        "\n",
        "print(\"Entraînement de l'agent Random Rollout...\")\n",
        "rewards_rollout, steps_rollout = train_basic_agent(env, agent_rollout, episodes=1000)\n",
        "\n",
        "print(\"\\n Graphique des récompenses\")\n",
        "plot_basic_rewards(rewards_rollout, title=\"Random Rollout - TicTacToe\")\n",
        "\n",
        "print(\"\\n Évaluation\")\n",
        "evaluate_basic_agent(agent_rollout, env)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b64d45a1",
      "metadata": {
        "id": "b64d45a1"
      },
      "source": [
        "Monte Carlo Tree Search (UCT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a6ed2028",
      "metadata": {
        "id": "a6ed2028"
      },
      "outputs": [],
      "source": [
        "class MCTSNode:\n",
        "    def __init__(self, board, parent=None, move=None):\n",
        "        self.board = board\n",
        "        self.parent = parent\n",
        "        self.move = move\n",
        "        self.children = []\n",
        "        self.wins = 0\n",
        "        self.visits = 0\n",
        "\n",
        "    def ucb1(self, total_simulations, exploration=1.41):\n",
        "        if self.visits == 0:\n",
        "            return float('inf')\n",
        "        return (self.wins / self.visits) + exploration * np.sqrt(np.log(total_simulations) / self.visits)\n",
        "\n",
        "class MCTSAgent:\n",
        "    def __init__(self, simulations=100):\n",
        "        self.simulations = simulations\n",
        "\n",
        "    def select_action(self, board):\n",
        "        root = MCTSNode(board[:])\n",
        "        for _ in range(self.simulations):\n",
        "            node = root\n",
        "            sim_board = board[:]\n",
        "            # Selection\n",
        "            while node.children:\n",
        "                node = max(node.children, key=lambda c: c.ucb1(root.visits + 1))\n",
        "                sim_board[node.move] = 'X'\n",
        "                if TicTacToe().check_win('X'):\n",
        "                    break\n",
        "            # Expansion\n",
        "            if not TicTacToe().check_win('X'):\n",
        "                available = [i for i, v in enumerate(sim_board) if v == ' ']\n",
        "                for move in available:\n",
        "                    new_board = sim_board[:]\n",
        "                    new_board[move] = 'X'\n",
        "                    node.children.append(MCTSNode(new_board, parent=node, move=move))\n",
        "            # Simulation\n",
        "            for child in node.children:\n",
        "                result = self.rollout(child.board)\n",
        "                self.backpropagate(child, result)\n",
        "        return max(root.children, key=lambda c: c.visits).move\n",
        "\n",
        "    def rollout(self, board):\n",
        "        sim_board = board[:]\n",
        "        while True:\n",
        "            moves = [i for i, v in enumerate(sim_board) if v == ' ']\n",
        "            if not moves:\n",
        "                return 0\n",
        "            sim_board[random.choice(moves)] = 'O'\n",
        "            if TicTacToe().check_win('O'):\n",
        "                return -1\n",
        "            moves = [i for i, v in enumerate(sim_board) if v == ' ']\n",
        "            if not moves:\n",
        "                return 0\n",
        "            sim_board[random.choice(moves)] = 'X'\n",
        "            if TicTacToe().check_win('X'):\n",
        "                return 1\n",
        "\n",
        "    def backpropagate(self, node, result):\n",
        "        while node:\n",
        "            node.visits += 1\n",
        "            node.wins += result\n",
        "            node = node.parent\n",
        "\n",
        "    def learn(self, *args, **kwargs):\n",
        "        pass\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "34179da7",
      "metadata": {},
      "outputs": [],
      "source": [
        "env = TicTacToe()\n",
        "agent_mcts = MCTSAgent(simulations=50)\n",
        "\n",
        "print(\"🔧 Entraînement de l'agent MCTS...\")\n",
        "rewards_mcts, steps_mcts = train_basic_agent(env, agent_mcts, episodes=1000)\n",
        "\n",
        "print(\"\\n📈 Graphique des récompenses\")\n",
        "plot_basic_rewards(rewards_mcts, title=\"MCTS (UCT) - TicTacToe\")\n",
        "\n",
        "print(\"\\n🧪 Évaluation\")\n",
        "evaluate_basic_agent(agent_mcts, env)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ae44080a",
      "metadata": {
        "id": "ae44080a"
      },
      "source": [
        "Expert Apprentice"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bfcb8771",
      "metadata": {
        "id": "bfcb8771"
      },
      "outputs": [],
      "source": [
        "class ExpertApprenticeAgent:\n",
        "    def __init__(self, expert_policy):\n",
        "        self.expert_policy = expert_policy\n",
        "\n",
        "    def select_action(self, board):\n",
        "        return self.expert_policy(board)\n",
        "\n",
        "    def learn(self, *args, **kwargs):\n",
        "        pass\n",
        "\n",
        "# Exemple de politique experte très simple\n",
        "def heuristic_expert(board):\n",
        "    for i in range(9):\n",
        "        if board[i] == ' ':\n",
        "            return i\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aad96e0a",
      "metadata": {},
      "outputs": [],
      "source": [
        "env = TicTacToe()\n",
        "expert = ExpertApprenticeAgent(expert_policy=heuristic_expert)\n",
        "\n",
        "print(\"🔧 Entraînement de l'agent ExpertApprentice...\")\n",
        "rewards_expert, steps_expert = train_basic_agent(env, expert, episodes=1000)\n",
        "\n",
        "print(\"\\n📈 Graphique des récompenses\")\n",
        "plot_basic_rewards(rewards_expert, title=\"Expert Apprentice - TicTacToe\")\n",
        "\n",
        "print(\"\\n🧪 Évaluation\")\n",
        "evaluate_basic_agent(expert, env)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9d828dce",
      "metadata": {
        "id": "9d828dce"
      },
      "source": [
        "Joueur VS Bot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d724e425",
      "metadata": {
        "id": "d724e425"
      },
      "outputs": [],
      "source": [
        "def play_against_agent(agent, env):\n",
        "    print(\"Bienvenue dans TicTacToe contre l'agent entraîné !\")\n",
        "    print(\"Vous êtes 'X' (humain), l'agent est 'O'\")\n",
        "    env.reset()\n",
        "    human_turn = True\n",
        "\n",
        "    while True:\n",
        "        env.render()\n",
        "        if human_turn:\n",
        "            try:\n",
        "                move = int(input(\"Entrez votre coup (0-8) : \"))\n",
        "                if env.board[move] != ' ':\n",
        "                    print(\"Case occupée. Réessayez.\")\n",
        "                    continue\n",
        "                env.board[move] = 'X'\n",
        "            except (ValueError, IndexError):\n",
        "                print(\"Entrée invalide. Entrez un nombre entre 0 et 8.\")\n",
        "                continue\n",
        "\n",
        "            if env.check_win('X'):\n",
        "                env.render()\n",
        "                print(\"Vous avez gagné !\")\n",
        "                break\n",
        "            if ' ' not in env.board:\n",
        "                env.render()\n",
        "                print(\"Match nul.\")\n",
        "                break\n",
        "\n",
        "            human_turn = False\n",
        "\n",
        "        else:\n",
        "            state = env.board.copy()\n",
        "            action = agent.select_action(state)\n",
        "            print(f\"L'agent joue en case {action}.\")\n",
        "            env.board[action] = 'O'\n",
        "\n",
        "            if env.check_win('O'):\n",
        "                env.render()\n",
        "                print(\" L'agent a gagné.\")\n",
        "                break\n",
        "            if ' ' not in env.board:\n",
        "                env.render()\n",
        "                print(\" Match nul.\")\n",
        "                break\n",
        "\n",
        "            human_turn = True\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0114aba5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0114aba5",
        "outputId": "725f3c4d-7ce5-4f8f-81a3-4915f4c2666d"
      },
      "outputs": [],
      "source": [
        "play_against_agent(agent_mcts, TicTacToe())"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".conda",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
