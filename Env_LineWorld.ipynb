{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HNTb5A2iYRCk"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AwW9kuy_Y3xo"
      },
      "source": [
        "Implémentation de environnement LineWorld"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NQ8hg8HOYsLJ"
      },
      "outputs": [],
      "source": [
        "class LineWorld:\n",
        "    def __init__(self, length=5):\n",
        "        self.length = length\n",
        "        self.start_state = 0\n",
        "        self.end_state = length - 1\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.state = self.start_state\n",
        "        return self.state\n",
        "\n",
        "    def step(self, action):\n",
        "        # 0 = gauche, 1 = droite\n",
        "        if action == 0:\n",
        "            self.state = max(0, self.state - 1)\n",
        "        elif action == 1:\n",
        "            self.state = min(self.length - 1, self.state + 1)\n",
        "\n",
        "        reward = 1 if self.state == self.end_state else 0\n",
        "        done = self.state == self.end_state\n",
        "        return self.state, reward, done, {}\n",
        "\n",
        "    def get_valid_actions(self):\n",
        "        return [0, 1]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y7jLpPEZZTfg"
      },
      "source": [
        "Lancement des Agents test sur l'environnement LinWorld :"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EH4RO15pZcH5"
      },
      "source": [
        "Agent Random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uklAV2j6Zoit",
        "outputId": "e7e05e0d-4de2-4e0c-a20c-23085f682ede"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "def run_random_agent(env, episodes=1000):\n",
        "    total_rewards = []\n",
        "    total_steps = []\n",
        "\n",
        "    for episode in range(episodes):\n",
        "        state = env.reset()\n",
        "        done = False\n",
        "        episode_reward = 0\n",
        "        steps = 0\n",
        "\n",
        "        while not done:\n",
        "            action = random.choice(env.get_valid_actions())\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "            episode_reward += reward\n",
        "            steps += 1\n",
        "\n",
        "        total_rewards.append(episode_reward)\n",
        "        total_steps.append(steps)\n",
        "\n",
        "    avg_reward = sum(total_rewards) / episodes\n",
        "    avg_steps = sum(total_steps) / episodes\n",
        "    print(f\"Random Agent on LineWorld ({env.length} states)\")\n",
        "    print(f\"Average reward: {avg_reward:.2f}\")\n",
        "    print(f\"Average episode length: {avg_steps:.2f} steps\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    env = LineWorld(length=5)\n",
        "    run_random_agent(env)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rYw6iV_Wblqp"
      },
      "source": [
        "Agent TabularQLearning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cEsWh728eaWQ"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "from collections import defaultdict\n",
        "\n",
        "class TabularQLearningAgent:\n",
        "    def __init__(self, n_states, n_actions, alpha=0.1, gamma=0.99, epsilon=0.9):\n",
        "        self.q_table = defaultdict(lambda: np.zeros(n_actions))\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon\n",
        "        self.n_actions = n_actions\n",
        "\n",
        "    def select_action(self, state):\n",
        "        if random.random() < self.epsilon:\n",
        "            return random.randint(0, self.n_actions - 1)\n",
        "        return np.argmax(self.q_table[state])\n",
        "\n",
        "    def learn(self, state, action, reward, next_state, done):\n",
        "        q_predict = self.q_table[state][action]\n",
        "        q_target = reward if done else reward + self.gamma * np.max(self.q_table[next_state])\n",
        "        self.q_table[state][action] += self.alpha * (q_target - q_predict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def trainTabularQLearningAgent(env, episodes=1000):\n",
        "    agent = TabularQLearningAgent(n_states=env.length, n_actions=2)\n",
        "    total_rewards = []\n",
        "    total_steps = []\n",
        "\n",
        "    for episode in range(episodes):\n",
        "        state = env.reset()\n",
        "        done = False\n",
        "        episode_reward = 0\n",
        "        steps = 0\n",
        "\n",
        "        while not done:\n",
        "            action = agent.select_action(state)\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "            agent.learn(state, action, reward, next_state, done)\n",
        "            state = next_state\n",
        "            episode_reward += reward\n",
        "            steps += 1\n",
        "\n",
        "        total_rewards.append(episode_reward)\n",
        "        total_steps.append(steps)\n",
        "\n",
        "    \n",
        "        if (episode + 1) % 50 == 0:\n",
        "            avg_last_50 = np.mean(total_rewards[-50:])\n",
        "            print(f\"Épisode {episode+1}/{episodes} - Moyenne des 50 derniers épisodes : {avg_last_50:.3f}\")\n",
        "\n",
        "    return agent, total_rewards, total_steps\n",
        "\n",
        "def evaluateTabularQLearningPolicy(agent, env, episodes=100):\n",
        "    original_epsilon = agent.epsilon\n",
        "    agent.epsilon = 0.0\n",
        "\n",
        "    total_rewards = []\n",
        "    total_steps = []\n",
        "    action_times = []\n",
        "\n",
        "    for _ in range(episodes):\n",
        "        state = env.reset()\n",
        "        done = False\n",
        "        reward_sum = 0\n",
        "        steps = 0\n",
        "\n",
        "        while not done:\n",
        "            start = time.time()\n",
        "            action = agent.select_action(state)\n",
        "            action_times.append(time.time() - start)\n",
        "            state, reward, done, _ = env.step(action)\n",
        "            reward_sum += reward\n",
        "            steps += 1\n",
        "\n",
        "        total_rewards.append(reward_sum)\n",
        "        total_steps.append(steps)\n",
        "\n",
        "    agent.epsilon = original_epsilon  # restaurer\n",
        "\n",
        "    print(\"\\n Évaluation de la policy finale (ε = 0) :\")\n",
        "    print(f\"  - Score moyen : {np.mean(total_rewards):.3f}\")\n",
        "    print(f\"  - Longueur moyenne : {np.mean(total_steps):.2f} steps\")\n",
        "    print(f\"  - Temps moyen par action : {np.mean(action_times) * 1000:.3f} ms\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_rewards(rewards, title=\"Rewards par episodes\"):\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.plot(rewards, label=\"Reward par épisode\")\n",
        "    plt.plot(np.convolve(rewards, np.ones(50)/50, mode=\"valid\"), color=\"orange\")\n",
        "    plt.title(title)\n",
        "    plt.xlabel(\"Episode\")\n",
        "    plt.ylabel(\"Total Reward\")\n",
        "    plt.legend()\n",
        "    plt.grid()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    env = LineWorld(length=5)\n",
        "    agent, total_rewards, total_steps = trainTabularQLearningAgent(env, episodes=10000)\n",
        "    plot_rewards(total_rewards, title=\"Tabular Q-Learning sur LineWorld\")\n",
        "    evaluateTabularQLearningPolicy(agent, env, episodes=100)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZbeV2hXxbswb"
      },
      "source": [
        "Agent DeepQLearning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yw8-nY2AhaJs"
      },
      "outputs": [],
      "source": [
        "class DQNNetwork(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(input_dim, 64), nn.ReLU(),\n",
        "            nn.Linear(64, 64), nn.ReLU(),\n",
        "            nn.Linear(64, output_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class DQNAgent:\n",
        "    def __init__(self, state_dim, n_actions, gamma=0.8, epsilon=0.99, epsilon_decay=0.995, epsilon_min=0.01, lr=1e-3):\n",
        "        self.state_dim = state_dim\n",
        "        self.n_actions = n_actions\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon\n",
        "        self.epsilon_decay = epsilon_decay\n",
        "        self.epsilon_min = epsilon_min\n",
        "\n",
        "        self.model = DQNNetwork(state_dim, n_actions)\n",
        "        self.optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
        "        self.criterion = nn.MSELoss()\n",
        "\n",
        "    def select_action(self, state):\n",
        "        if random.random() < self.epsilon:\n",
        "            return random.randint(0, self.n_actions - 1)\n",
        "        with torch.no_grad():\n",
        "            # state_tensor = torch.FloatTensor([state])\n",
        "            state_tensor = torch.FloatTensor(state).unsqueeze(0)  \n",
        "            q_values = self.model(state_tensor)\n",
        "            return torch.argmax(q_values).item()\n",
        "\n",
        "    def learn(self, state, action, reward, next_state, done):\n",
        "        state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
        "        next_state_tensor = torch.FloatTensor(next_state).unsqueeze(0)\n",
        "        reward_tensor = torch.tensor(reward, dtype=torch.float32)\n",
        "        done_tensor = torch.tensor(done, dtype=torch.float32)\n",
        "\n",
        "        q_values = self.model(state_tensor)\n",
        "        next_q_values = self.model(next_state_tensor)\n",
        "\n",
        "        target = reward_tensor if done else reward_tensor + self.gamma * torch.max(next_q_values).detach()\n",
        "\n",
        "        loss = self.criterion(q_values[0][action], target)\n",
        "\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon *= self.epsilon_decay"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_dqn(agent, env, episodes=1000):\n",
        "    rewards_per_episode = []\n",
        "    steps_per_episode = []\n",
        "\n",
        "    for episode in range(episodes):\n",
        "        state = env.reset()\n",
        "        total_reward = 0\n",
        "        steps = 0\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            action = agent.select_action([state])\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "            agent.learn([state], action, reward, [next_state], done)\n",
        "            state = next_state\n",
        "            total_reward += reward\n",
        "            steps += 1\n",
        "\n",
        "        rewards_per_episode.append(total_reward)\n",
        "        steps_per_episode.append(steps)\n",
        "\n",
        "        if (episode + 1) % 50 == 0:\n",
        "            avg_reward = np.mean(rewards_per_episode[-50:])\n",
        "            print(f\"Épisode {episode+1}/{episodes} - Moyenne des 50 derniers épisodes : Récompense = {avg_reward:.3f}\")\n",
        "\n",
        "    avg_steps_final = np.mean(steps_per_episode)\n",
        "    print(f\"\\nNombre moyen de steps par épisode à la fin de l'entraînement : {avg_steps_final:.2f}\")\n",
        "\n",
        "    return rewards_per_episode\n",
        "\n",
        "def evaluate_policy(agent, env, n_eval_episodes=100, epsilon_eval=0.0):\n",
        "    original_epsilon = agent.epsilon\n",
        "    agent.epsilon = epsilon_eval\n",
        "\n",
        "    total_rewards = []\n",
        "    total_steps = []\n",
        "    action_times = []\n",
        "\n",
        "    for _ in range(n_eval_episodes):\n",
        "        state = env.reset()\n",
        "        done = False\n",
        "        ep_reward = 0\n",
        "        ep_steps = 0\n",
        "\n",
        "        while not done:\n",
        "            start = time.time()\n",
        "            action = agent.select_action([state])\n",
        "            action_times.append(time.time() - start)\n",
        "\n",
        "            state, reward, done, _ = env.step(action)\n",
        "            ep_reward += reward\n",
        "            ep_steps += 1\n",
        "\n",
        "        total_rewards.append(ep_reward)\n",
        "        total_steps.append(ep_steps)\n",
        "\n",
        "    agent.epsilon = original_epsilon\n",
        "\n",
        "    print(f\"\\n Évaluation sur {n_eval_episodes} épisodes avec ε=0 :\")\n",
        "    print(f\"  - Score moyen final : {np.mean(total_rewards):.2f}\")\n",
        "    print(f\"  - Longueur moyenne : {np.mean(total_steps):.2f} steps\")\n",
        "    print(f\"  - Temps moyen par action : {np.mean(action_times) * 1000:.3f} ms\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_rewards(rewards, title=\"Récompense par épisode\"):\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.plot(rewards, label='Reward par épisode')\n",
        "    plt.plot(\n",
        "        np.convolve(rewards, np.ones(50)/50, mode='valid'),\n",
        "        label='Reward moyenne (fenêtre=50)', color='orange'\n",
        "    )\n",
        "    plt.xlabel('Épisodes')\n",
        "    plt.ylabel('Récompense')\n",
        "    plt.title(title)\n",
        "    plt.legend()\n",
        "    plt.grid()\n",
        "    plt.show()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    env = LineWorld(length=5)\n",
        "    agent = DQNAgent(state_dim=1, n_actions=2)\n",
        "    rewards = train_dqn(agent, env, episodes=1000)\n",
        "    plot_rewards(rewards, title=\"DQN sur LineWorld\")\n",
        "\n",
        "    evaluate_policy(agent, env, n_eval_episodes=100)  \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qOWhQUP-bw7C"
      },
      "source": [
        "Agent DoubleDeepQLearning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 153,
      "metadata": {
        "id": "jdSBSFXWhiBL"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "class DoubleDQNNetwork(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(input_dim, 64), nn.ReLU(),\n",
        "            nn.Linear(64, 64), nn.ReLU(),\n",
        "            nn.Linear(64, output_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class DoubleDQNAgent:\n",
        "    def __init__(self, state_dim, n_actions, gamma=0.99, epsilon=1.0, epsilon_decay=0.995, epsilon_min=0.01, lr=1e-3):\n",
        "        self.state_dim = state_dim\n",
        "        self.n_actions = n_actions\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon\n",
        "        self.epsilon_decay = epsilon_decay\n",
        "        self.epsilon_min = epsilon_min\n",
        "\n",
        "        self.policy_net = DoubleDQNNetwork(state_dim, n_actions)\n",
        "        self.target_net = DoubleDQNNetwork(state_dim, n_actions)\n",
        "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
        "        self.target_net.eval()\n",
        "\n",
        "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=lr)\n",
        "        self.criterion = nn.MSELoss()\n",
        "        self.update_counter = 0\n",
        "\n",
        "    def select_action(self, state):\n",
        "        if random.random() < self.epsilon:\n",
        "            return random.randint(0, self.n_actions - 1)\n",
        "        with torch.no_grad():\n",
        "            state_tensor = torch.tensor([state], dtype=torch.float32).unsqueeze(0)\n",
        "            q_values = self.policy_net(state_tensor)\n",
        "            return torch.argmax(q_values).item()\n",
        "\n",
        "    def learn(self, state, action, reward, next_state, done):\n",
        "        state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
        "        next_state_tensor = torch.tensor(next_state, dtype=torch.float32).unsqueeze(0)\n",
        "        reward_tensor = torch.tensor(reward, dtype=torch.float32)\n",
        "\n",
        "\n",
        "        current_q = self.policy_net(state_tensor)[0][action]\n",
        "\n",
        "        with torch.no_grad():\n",
        "            next_action = torch.argmax(self.policy_net(next_state_tensor), dim=1)\n",
        "            next_q = self.target_net(next_state_tensor)[0][next_action]\n",
        "            target_q = reward_tensor if done else reward_tensor + self.gamma * next_q\n",
        "\n",
        "        loss = self.criterion(current_q, target_q)\n",
        "\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon *= self.epsilon_decay\n",
        "\n",
        "        self.update_counter += 1\n",
        "        if self.update_counter % 10 == 0:\n",
        "            self.target_net.load_state_dict(self.policy_net.state_dict())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 154,
      "metadata": {},
      "outputs": [],
      "source": [
        "def trainDoubleDQNAgent(env, agent, episodes=1000):\n",
        "    total_rewards = []\n",
        "    total_steps = []\n",
        "\n",
        "    for episode in range(episodes):\n",
        "        state = env.reset()\n",
        "        done = False\n",
        "        ep_reward = 0\n",
        "        steps = 0\n",
        "\n",
        "        while not done:\n",
        "            action = agent.select_action([state])\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "            agent.learn([state], action, reward, [next_state], done)\n",
        "            state = next_state\n",
        "            ep_reward += reward\n",
        "            steps += 1\n",
        "\n",
        "        total_rewards.append(ep_reward)\n",
        "        total_steps.append(steps)\n",
        "\n",
        "        if (episode + 1) % 50 == 0:\n",
        "            print(f\"Épisode {episode + 1}/{episodes} - Moyenne des 50 derniers : {np.mean(total_rewards[-50:]):.3f}\")\n",
        "\n",
        "    return agent, total_rewards, total_steps\n",
        "\n",
        "def evaluateDoubleDQNPolicy(agent, env, episodes=100):\n",
        "    original_epsilon = agent.epsilon\n",
        "    agent.epsilon = 0.0\n",
        "\n",
        "    total_rewards = []\n",
        "    total_steps = []\n",
        "    action_times = []\n",
        "\n",
        "    for _ in range(episodes):\n",
        "        state = env.reset()\n",
        "        done = False\n",
        "        reward_sum = 0\n",
        "        steps = 0\n",
        "\n",
        "        while not done:\n",
        "            start = time.time()\n",
        "            action = agent.select_action([state])\n",
        "            action_times.append(time.time() - start)\n",
        "            state, reward, done, _ = env.step(action)\n",
        "            reward_sum += reward\n",
        "            steps += 1\n",
        "\n",
        "        total_rewards.append(reward_sum)\n",
        "        total_steps.append(steps)\n",
        "\n",
        "    agent.epsilon = original_epsilon\n",
        "\n",
        "    print(\"\\n Évaluation de la policy finale (ε = 0) :\")\n",
        "    print(f\"  - Score moyen : {np.mean(total_rewards):.3f}\")\n",
        "    print(f\"  - Longueur moyenne : {np.mean(total_steps):.2f} steps\")\n",
        "    print(f\"  - Temps moyen par action : {np.mean(action_times) * 1000:.3f} ms\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Épisode 50/10000 - Moyenne des 50 derniers : 1.000\n",
            "Épisode 100/10000 - Moyenne des 50 derniers : 1.000\n",
            "Épisode 150/10000 - Moyenne des 50 derniers : 1.000\n",
            "Épisode 200/10000 - Moyenne des 50 derniers : 1.000\n",
            "Épisode 250/10000 - Moyenne des 50 derniers : 1.000\n",
            "Épisode 300/10000 - Moyenne des 50 derniers : 1.000\n",
            "Épisode 350/10000 - Moyenne des 50 derniers : 1.000\n",
            "Épisode 400/10000 - Moyenne des 50 derniers : 1.000\n",
            "Épisode 450/10000 - Moyenne des 50 derniers : 1.000\n",
            "Épisode 500/10000 - Moyenne des 50 derniers : 1.000\n",
            "Épisode 550/10000 - Moyenne des 50 derniers : 1.000\n",
            "Épisode 600/10000 - Moyenne des 50 derniers : 1.000\n",
            "Épisode 650/10000 - Moyenne des 50 derniers : 1.000\n",
            "Épisode 700/10000 - Moyenne des 50 derniers : 1.000\n",
            "Épisode 750/10000 - Moyenne des 50 derniers : 1.000\n",
            "Épisode 800/10000 - Moyenne des 50 derniers : 1.000\n",
            "Épisode 850/10000 - Moyenne des 50 derniers : 1.000\n",
            "Épisode 900/10000 - Moyenne des 50 derniers : 1.000\n",
            "Épisode 950/10000 - Moyenne des 50 derniers : 1.000\n",
            "Épisode 1000/10000 - Moyenne des 50 derniers : 1.000\n",
            "Épisode 1050/10000 - Moyenne des 50 derniers : 1.000\n",
            "Épisode 1100/10000 - Moyenne des 50 derniers : 1.000\n",
            "Épisode 1150/10000 - Moyenne des 50 derniers : 1.000\n",
            "Épisode 1200/10000 - Moyenne des 50 derniers : 1.000\n",
            "Épisode 1250/10000 - Moyenne des 50 derniers : 1.000\n",
            "Épisode 1300/10000 - Moyenne des 50 derniers : 1.000\n",
            "Épisode 1350/10000 - Moyenne des 50 derniers : 1.000\n",
            "Épisode 1400/10000 - Moyenne des 50 derniers : 1.000\n",
            "Épisode 1450/10000 - Moyenne des 50 derniers : 1.000\n",
            "Épisode 1500/10000 - Moyenne des 50 derniers : 1.000\n",
            "Épisode 1550/10000 - Moyenne des 50 derniers : 1.000\n",
            "Épisode 1600/10000 - Moyenne des 50 derniers : 1.000\n",
            "Épisode 1650/10000 - Moyenne des 50 derniers : 1.000\n",
            "Épisode 1700/10000 - Moyenne des 50 derniers : 1.000\n",
            "Épisode 1750/10000 - Moyenne des 50 derniers : 1.000\n",
            "Épisode 1800/10000 - Moyenne des 50 derniers : 1.000\n",
            "Épisode 1850/10000 - Moyenne des 50 derniers : 1.000\n",
            "Épisode 1900/10000 - Moyenne des 50 derniers : 1.000\n",
            "Épisode 1950/10000 - Moyenne des 50 derniers : 1.000\n",
            "Épisode 2000/10000 - Moyenne des 50 derniers : 1.000\n",
            "Épisode 2050/10000 - Moyenne des 50 derniers : 1.000\n",
            "Épisode 2100/10000 - Moyenne des 50 derniers : 1.000\n",
            "Épisode 2150/10000 - Moyenne des 50 derniers : 1.000\n",
            "Épisode 2200/10000 - Moyenne des 50 derniers : 1.000\n",
            "Épisode 2250/10000 - Moyenne des 50 derniers : 1.000\n",
            "Épisode 2300/10000 - Moyenne des 50 derniers : 1.000\n",
            "Épisode 2350/10000 - Moyenne des 50 derniers : 1.000\n",
            "Épisode 2400/10000 - Moyenne des 50 derniers : 1.000\n",
            "Épisode 2450/10000 - Moyenne des 50 derniers : 1.000\n",
            "Épisode 2500/10000 - Moyenne des 50 derniers : 1.000\n",
            "Épisode 2550/10000 - Moyenne des 50 derniers : 1.000\n",
            "Épisode 2600/10000 - Moyenne des 50 derniers : 1.000\n",
            "Épisode 2650/10000 - Moyenne des 50 derniers : 1.000\n",
            "Épisode 2700/10000 - Moyenne des 50 derniers : 1.000\n",
            "Épisode 2750/10000 - Moyenne des 50 derniers : 1.000\n",
            "Épisode 2800/10000 - Moyenne des 50 derniers : 1.000\n",
            "Épisode 2850/10000 - Moyenne des 50 derniers : 1.000\n",
            "Épisode 2900/10000 - Moyenne des 50 derniers : 1.000\n",
            "Épisode 2950/10000 - Moyenne des 50 derniers : 1.000\n",
            "Épisode 3000/10000 - Moyenne des 50 derniers : 1.000\n",
            "Épisode 3050/10000 - Moyenne des 50 derniers : 1.000\n",
            "Épisode 3100/10000 - Moyenne des 50 derniers : 1.000\n",
            "Épisode 3150/10000 - Moyenne des 50 derniers : 1.000\n",
            "Épisode 3200/10000 - Moyenne des 50 derniers : 1.000\n",
            "Épisode 3250/10000 - Moyenne des 50 derniers : 1.000\n",
            "Épisode 3300/10000 - Moyenne des 50 derniers : 1.000\n",
            "Épisode 3350/10000 - Moyenne des 50 derniers : 1.000\n",
            "Épisode 3400/10000 - Moyenne des 50 derniers : 1.000\n",
            "Épisode 3450/10000 - Moyenne des 50 derniers : 1.000\n",
            "Épisode 3500/10000 - Moyenne des 50 derniers : 1.000\n",
            "Épisode 3550/10000 - Moyenne des 50 derniers : 1.000\n",
            "Épisode 3600/10000 - Moyenne des 50 derniers : 1.000\n",
            "Épisode 3650/10000 - Moyenne des 50 derniers : 1.000\n",
            "Épisode 3700/10000 - Moyenne des 50 derniers : 1.000\n",
            "Épisode 3750/10000 - Moyenne des 50 derniers : 1.000\n",
            "Épisode 3800/10000 - Moyenne des 50 derniers : 1.000\n",
            "Épisode 3850/10000 - Moyenne des 50 derniers : 1.000\n",
            "Épisode 3900/10000 - Moyenne des 50 derniers : 1.000\n",
            "Épisode 3950/10000 - Moyenne des 50 derniers : 1.000\n",
            "Épisode 4000/10000 - Moyenne des 50 derniers : 1.000\n",
            "Épisode 4050/10000 - Moyenne des 50 derniers : 1.000\n",
            "Épisode 4100/10000 - Moyenne des 50 derniers : 1.000\n",
            "Épisode 4150/10000 - Moyenne des 50 derniers : 1.000\n",
            "Épisode 4200/10000 - Moyenne des 50 derniers : 1.000\n",
            "Épisode 4250/10000 - Moyenne des 50 derniers : 1.000\n",
            "Épisode 4300/10000 - Moyenne des 50 derniers : 1.000\n",
            "Épisode 4350/10000 - Moyenne des 50 derniers : 1.000\n",
            "Épisode 4400/10000 - Moyenne des 50 derniers : 1.000\n",
            "Épisode 4450/10000 - Moyenne des 50 derniers : 1.000\n",
            "Épisode 4500/10000 - Moyenne des 50 derniers : 1.000\n",
            "Épisode 4550/10000 - Moyenne des 50 derniers : 1.000\n",
            "Épisode 4600/10000 - Moyenne des 50 derniers : 1.000\n",
            "Épisode 4650/10000 - Moyenne des 50 derniers : 1.000\n",
            "Épisode 4700/10000 - Moyenne des 50 derniers : 1.000\n",
            "Épisode 4750/10000 - Moyenne des 50 derniers : 1.000\n",
            "Épisode 4800/10000 - Moyenne des 50 derniers : 1.000\n",
            "Épisode 4850/10000 - Moyenne des 50 derniers : 1.000\n",
            "Épisode 4900/10000 - Moyenne des 50 derniers : 1.000\n",
            "Épisode 4950/10000 - Moyenne des 50 derniers : 1.000\n",
            "Épisode 5000/10000 - Moyenne des 50 derniers : 1.000\n",
            "Épisode 5050/10000 - Moyenne des 50 derniers : 1.000\n",
            "Épisode 5100/10000 - Moyenne des 50 derniers : 1.000\n",
            "Épisode 5150/10000 - Moyenne des 50 derniers : 1.000\n",
            "Épisode 5200/10000 - Moyenne des 50 derniers : 1.000\n",
            "Épisode 5250/10000 - Moyenne des 50 derniers : 1.000\n",
            "Épisode 5300/10000 - Moyenne des 50 derniers : 1.000\n",
            "Épisode 5350/10000 - Moyenne des 50 derniers : 1.000\n",
            "Épisode 5400/10000 - Moyenne des 50 derniers : 1.000\n",
            "Épisode 5450/10000 - Moyenne des 50 derniers : 1.000\n",
            "Épisode 5500/10000 - Moyenne des 50 derniers : 1.000\n",
            "Épisode 5550/10000 - Moyenne des 50 derniers : 1.000\n",
            "Épisode 5600/10000 - Moyenne des 50 derniers : 1.000\n",
            "Épisode 5650/10000 - Moyenne des 50 derniers : 1.000\n",
            "Épisode 5700/10000 - Moyenne des 50 derniers : 1.000\n",
            "Épisode 5750/10000 - Moyenne des 50 derniers : 1.000\n",
            "Épisode 5800/10000 - Moyenne des 50 derniers : 1.000\n",
            "Épisode 5850/10000 - Moyenne des 50 derniers : 1.000\n",
            "Épisode 5900/10000 - Moyenne des 50 derniers : 1.000\n",
            "Épisode 5950/10000 - Moyenne des 50 derniers : 1.000\n",
            "Épisode 6000/10000 - Moyenne des 50 derniers : 1.000\n",
            "Épisode 6050/10000 - Moyenne des 50 derniers : 1.000\n",
            "Épisode 6100/10000 - Moyenne des 50 derniers : 1.000\n",
            "Épisode 6150/10000 - Moyenne des 50 derniers : 1.000\n",
            "Épisode 6200/10000 - Moyenne des 50 derniers : 1.000\n",
            "Épisode 6250/10000 - Moyenne des 50 derniers : 1.000\n",
            "Épisode 6300/10000 - Moyenne des 50 derniers : 1.000\n",
            "Épisode 6350/10000 - Moyenne des 50 derniers : 1.000\n",
            "Épisode 6400/10000 - Moyenne des 50 derniers : 1.000\n",
            "Épisode 6450/10000 - Moyenne des 50 derniers : 1.000\n",
            "Épisode 6500/10000 - Moyenne des 50 derniers : 1.000\n",
            "Épisode 6550/10000 - Moyenne des 50 derniers : 1.000\n",
            "Épisode 6600/10000 - Moyenne des 50 derniers : 1.000\n",
            "Épisode 6650/10000 - Moyenne des 50 derniers : 1.000\n",
            "Épisode 6700/10000 - Moyenne des 50 derniers : 1.000\n",
            "Épisode 6750/10000 - Moyenne des 50 derniers : 1.000\n",
            "Épisode 6800/10000 - Moyenne des 50 derniers : 1.000\n",
            "Épisode 6850/10000 - Moyenne des 50 derniers : 1.000\n",
            "Épisode 6900/10000 - Moyenne des 50 derniers : 1.000\n",
            "Épisode 6950/10000 - Moyenne des 50 derniers : 1.000\n",
            "Épisode 7000/10000 - Moyenne des 50 derniers : 1.000\n",
            "Épisode 7050/10000 - Moyenne des 50 derniers : 1.000\n",
            "Épisode 7100/10000 - Moyenne des 50 derniers : 1.000\n",
            "Épisode 7150/10000 - Moyenne des 50 derniers : 1.000\n",
            "Épisode 7200/10000 - Moyenne des 50 derniers : 1.000\n",
            "Épisode 7250/10000 - Moyenne des 50 derniers : 1.000\n",
            "Épisode 7300/10000 - Moyenne des 50 derniers : 1.000\n",
            "Épisode 7350/10000 - Moyenne des 50 derniers : 1.000\n",
            "Épisode 7400/10000 - Moyenne des 50 derniers : 1.000\n",
            "Épisode 7450/10000 - Moyenne des 50 derniers : 1.000\n",
            "Épisode 7500/10000 - Moyenne des 50 derniers : 1.000\n",
            "Épisode 7550/10000 - Moyenne des 50 derniers : 1.000\n",
            "Épisode 7600/10000 - Moyenne des 50 derniers : 1.000\n",
            "Épisode 7650/10000 - Moyenne des 50 derniers : 1.000\n",
            "Épisode 7700/10000 - Moyenne des 50 derniers : 1.000\n",
            "Épisode 7750/10000 - Moyenne des 50 derniers : 1.000\n",
            "Épisode 7800/10000 - Moyenne des 50 derniers : 1.000\n",
            "Épisode 7850/10000 - Moyenne des 50 derniers : 1.000\n",
            "Épisode 7900/10000 - Moyenne des 50 derniers : 1.000\n",
            "Épisode 7950/10000 - Moyenne des 50 derniers : 1.000\n",
            "Épisode 8000/10000 - Moyenne des 50 derniers : 1.000\n",
            "Épisode 8050/10000 - Moyenne des 50 derniers : 1.000\n",
            "Épisode 8100/10000 - Moyenne des 50 derniers : 1.000\n",
            "Épisode 8150/10000 - Moyenne des 50 derniers : 1.000\n",
            "Épisode 8200/10000 - Moyenne des 50 derniers : 1.000\n",
            "Épisode 8250/10000 - Moyenne des 50 derniers : 1.000\n",
            "Épisode 8300/10000 - Moyenne des 50 derniers : 1.000\n",
            "Épisode 8350/10000 - Moyenne des 50 derniers : 1.000\n",
            "Épisode 8400/10000 - Moyenne des 50 derniers : 1.000\n",
            "Épisode 8450/10000 - Moyenne des 50 derniers : 1.000\n",
            "Épisode 8500/10000 - Moyenne des 50 derniers : 1.000\n",
            "Épisode 8550/10000 - Moyenne des 50 derniers : 1.000\n",
            "Épisode 8600/10000 - Moyenne des 50 derniers : 1.000\n",
            "Épisode 8650/10000 - Moyenne des 50 derniers : 1.000\n",
            "Épisode 8700/10000 - Moyenne des 50 derniers : 1.000\n",
            "Épisode 8750/10000 - Moyenne des 50 derniers : 1.000\n",
            "Épisode 8800/10000 - Moyenne des 50 derniers : 1.000\n",
            "Épisode 8850/10000 - Moyenne des 50 derniers : 1.000\n",
            "Épisode 8900/10000 - Moyenne des 50 derniers : 1.000\n",
            "Épisode 8950/10000 - Moyenne des 50 derniers : 1.000\n",
            "Épisode 9000/10000 - Moyenne des 50 derniers : 1.000\n",
            "Épisode 9050/10000 - Moyenne des 50 derniers : 1.000\n",
            "Épisode 9100/10000 - Moyenne des 50 derniers : 1.000\n",
            "Épisode 9150/10000 - Moyenne des 50 derniers : 1.000\n",
            "Épisode 9200/10000 - Moyenne des 50 derniers : 1.000\n",
            "Épisode 9250/10000 - Moyenne des 50 derniers : 1.000\n",
            "Épisode 9300/10000 - Moyenne des 50 derniers : 1.000\n",
            "Épisode 9350/10000 - Moyenne des 50 derniers : 1.000\n",
            "Épisode 9400/10000 - Moyenne des 50 derniers : 1.000\n",
            "Épisode 9450/10000 - Moyenne des 50 derniers : 1.000\n",
            "Épisode 9500/10000 - Moyenne des 50 derniers : 1.000\n",
            "Épisode 9550/10000 - Moyenne des 50 derniers : 1.000\n",
            "Épisode 9600/10000 - Moyenne des 50 derniers : 1.000\n",
            "Épisode 9650/10000 - Moyenne des 50 derniers : 1.000\n",
            "Épisode 9700/10000 - Moyenne des 50 derniers : 1.000\n",
            "Épisode 9750/10000 - Moyenne des 50 derniers : 1.000\n",
            "Épisode 9800/10000 - Moyenne des 50 derniers : 1.000\n",
            "Épisode 9850/10000 - Moyenne des 50 derniers : 1.000\n",
            "Épisode 9900/10000 - Moyenne des 50 derniers : 1.000\n",
            "Épisode 9950/10000 - Moyenne des 50 derniers : 1.000\n",
            "Épisode 10000/10000 - Moyenne des 50 derniers : 1.000\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1000x500 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " Évaluation de la policy finale (ε = 0) :\n",
            "  - Score moyen : 1.000\n",
            "  - Longueur moyenne : 4.00 steps\n",
            "  - Temps moyen par action : 0.043 ms\n"
          ]
        }
      ],
      "source": [
        "def plot_rewards(rewards, title=\"Double DQN - Rewards over episodes\"):\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.plot(rewards, label=\"Reward par épisode\")\n",
        "    plt.plot(np.convolve(rewards, np.ones(50)/50, mode=\"valid\"),\n",
        "             label=\"Moyenne glissante (50)\", color=\"orange\")\n",
        "    plt.title(title)\n",
        "    plt.xlabel(\"Épisodes\")\n",
        "    plt.ylabel(\"Récompense\")\n",
        "    plt.legend()\n",
        "    plt.grid()\n",
        "    plt.show()\n",
        "\n",
        "env = LineWorld(length=5)\n",
        "agent = DoubleDQNAgent(state_dim=1, n_actions=2)\n",
        "\n",
        "agent, rewards, steps = trainDoubleDQNAgent(env, agent, episodes=1000)\n",
        "plot_rewards(rewards, title=\"Double DQN sur LineWorld\")\n",
        "evaluateDoubleDQNPolicy(agent, env, episodes=100)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-0RlY1Xvb4ET"
      },
      "source": [
        "Agent DoubleDeepQLearningWithExperienceReplay :"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pLzMxTieh_hj"
      },
      "source": [
        "Buffer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iMAQu0q-h3D1"
      },
      "outputs": [],
      "source": [
        "from collections import deque\n",
        "\n",
        "class ReplayBuffer:\n",
        "    def __init__(self, capacity=10000):\n",
        "        self.buffer = deque(maxlen=capacity)\n",
        "\n",
        "    def add(self, state, action, reward, next_state, done):\n",
        "        self.buffer.append((state, action, reward, next_state, done))\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        return random.sample(self.buffer, batch_size)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.buffer)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PpesRJesiBW4"
      },
      "source": [
        "DDQL ER"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bVN6142zh37D"
      },
      "outputs": [],
      "source": [
        "class DoubleDQNWithReplayAgent:\n",
        "    def __init__(self, state_dim, n_actions, gamma=0.99, epsilon=1.0, epsilon_decay=0.995, epsilon_min=0.01, lr=1e-3, buffer_size=10000, batch_size=64):\n",
        "        self.state_dim = state_dim\n",
        "        self.n_actions = n_actions\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon\n",
        "        self.epsilon_decay = epsilon_decay\n",
        "        self.epsilon_min = epsilon_min\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        self.policy_net = DoubleDQNNetwork(state_dim, n_actions)\n",
        "        self.target_net = DoubleDQNNetwork(state_dim, n_actions)\n",
        "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
        "        self.target_net.eval()\n",
        "\n",
        "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=lr)\n",
        "        self.criterion = nn.MSELoss()\n",
        "\n",
        "        self.memory = ReplayBuffer(capacity=buffer_size)\n",
        "        self.update_counter = 0\n",
        "\n",
        "    def select_action(self, state):\n",
        "        if random.random() < self.epsilon:\n",
        "            return random.randint(0, self.n_actions - 1)\n",
        "        with torch.no_grad():\n",
        "            state_tensor = torch.FloatTensor([state])\n",
        "            q_values = self.policy_net(state_tensor)\n",
        "            return torch.argmax(q_values).item()\n",
        "\n",
        "    def store_transition(self, state, action, reward, next_state, done):\n",
        "        self.memory.add(state, action, reward, next_state, done)\n",
        "\n",
        "    def learn(self):\n",
        "        if len(self.memory) < self.batch_size:\n",
        "            return\n",
        "\n",
        "        batch = self.memory.sample(self.batch_size)\n",
        "        states, actions, rewards, next_states, dones = zip(*batch)\n",
        "\n",
        "        states = torch.FloatTensor(states)\n",
        "        actions = torch.LongTensor(actions)\n",
        "        rewards = torch.FloatTensor(rewards)\n",
        "        next_states = torch.FloatTensor(next_states)\n",
        "        dones = torch.FloatTensor(dones)\n",
        "\n",
        "        current_q = self.policy_net(states).gather(1, actions.unsqueeze(1)).squeeze()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            next_actions = torch.argmax(self.policy_net(next_states), dim=1)\n",
        "            next_q = self.target_net(next_states).gather(1, next_actions.unsqueeze(1)).squeeze()\n",
        "            target_q = rewards + self.gamma * next_q * (1 - dones)\n",
        "\n",
        "        loss = self.criterion(current_q, target_q)\n",
        "\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon *= self.epsilon_decay\n",
        "\n",
        "        self.update_counter += 1\n",
        "        if self.update_counter % 10 == 0:\n",
        "            self.target_net.load_state_dict(self.policy_net.state_dict())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "be3Feyhlb81m"
      },
      "source": [
        "Agent DoubleDeepQLearningWithPrioritizedExperienceReplay"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Buffer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class PrioritizedReplayBuffer:\n",
        "    def __init__(self, capacity=10000, alpha=0.6):\n",
        "        self.capacity = capacity\n",
        "        self.buffer = []\n",
        "        self.priorities = []\n",
        "        self.alpha = alpha\n",
        "        self.pos = 0\n",
        "\n",
        "    def add(self, state, action, reward, next_state, done, td_error=1.0):\n",
        "        priority = (abs(td_error) + 1e-5) ** self.alpha\n",
        "        if len(self.buffer) < self.capacity:\n",
        "            self.buffer.append((state, action, reward, next_state, done))\n",
        "            self.priorities.append(priority)\n",
        "        else:\n",
        "            self.buffer[self.pos] = (state, action, reward, next_state, done)\n",
        "            self.priorities[self.pos] = priority\n",
        "            self.pos = (self.pos + 1) % self.capacity\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        probs = np.array(self.priorities) / sum(self.priorities)\n",
        "        indices = np.random.choice(len(self.buffer), batch_size, p=probs)\n",
        "        samples = [self.buffer[i] for i in indices]\n",
        "        return samples\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.buffer)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "DDQLPER"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class DoubleDQNWithPrioritizedReplayAgent:\n",
        "    def __init__(self, state_dim, n_actions, gamma=0.99, epsilon=1.0, epsilon_decay=0.995, epsilon_min=0.01, lr=1e-3, buffer_size=10000, batch_size=64):\n",
        "        self.state_dim = state_dim\n",
        "        self.n_actions = n_actions\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon\n",
        "        self.epsilon_decay = epsilon_decay\n",
        "        self.epsilon_min = epsilon_min\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        self.policy_net = DoubleDQNNetwork(state_dim, n_actions)\n",
        "        self.target_net = DoubleDQNNetwork(state_dim, n_actions)\n",
        "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
        "        self.target_net.eval()\n",
        "\n",
        "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=lr)\n",
        "        self.criterion = nn.MSELoss()\n",
        "\n",
        "        self.memory = PrioritizedReplayBuffer(capacity=buffer_size)\n",
        "        self.update_counter = 0\n",
        "\n",
        "    def select_action(self, state):\n",
        "        if random.random() < self.epsilon:\n",
        "            return random.randint(0, self.n_actions - 1)\n",
        "        with torch.no_grad():\n",
        "            state_tensor = torch.FloatTensor([state])\n",
        "            q_values = self.policy_net(state_tensor)\n",
        "            return torch.argmax(q_values).item()\n",
        "\n",
        "    def store_transition(self, state, action, reward, next_state, done):\n",
        "        with torch.no_grad():\n",
        "            s_tensor = torch.FloatTensor([state])\n",
        "            ns_tensor = torch.FloatTensor([next_state])\n",
        "            q_val = self.policy_net(s_tensor)[0][action]\n",
        "            next_action = torch.argmax(self.policy_net(ns_tensor)).item()\n",
        "            next_q = self.target_net(ns_tensor)[0][next_action]\n",
        "            target = reward + self.gamma * next_q * (1 - int(done))\n",
        "            td_error = abs(q_val.item() - target.item())\n",
        "\n",
        "        self.memory.add(state, action, reward, next_state, done, td_error)\n",
        "\n",
        "    def learn(self):\n",
        "        if len(self.memory) < self.batch_size:\n",
        "            return\n",
        "\n",
        "        batch = self.memory.sample(self.batch_size)\n",
        "        states, actions, rewards, next_states, dones = zip(*batch)\n",
        "\n",
        "        states = torch.FloatTensor(states)\n",
        "        actions = torch.LongTensor(actions)\n",
        "        rewards = torch.FloatTensor(rewards)\n",
        "        next_states = torch.FloatTensor(next_states)\n",
        "        dones = torch.FloatTensor(dones)\n",
        "\n",
        "        q_values = self.policy_net(states).gather(1, actions.unsqueeze(1)).squeeze()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            next_actions = torch.argmax(self.policy_net(next_states), dim=1)\n",
        "            next_q = self.target_net(next_states).gather(1, next_actions.unsqueeze(1)).squeeze()\n",
        "            targets = rewards + self.gamma * next_q * (1 - dones)\n",
        "\n",
        "        loss = self.criterion(q_values, targets)\n",
        "\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon *= self.epsilon_decay\n",
        "\n",
        "        self.update_counter += 1\n",
        "        if self.update_counter % 10 == 0:\n",
        "            self.target_net.load_state_dict(self.policy_net.state_dict())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "REINFORCE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class PolicyNetwork(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(state_dim, 32),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(32, action_dim),\n",
        "            nn.Softmax(dim=-1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "    \n",
        "class REINFORCEAgent:\n",
        "    def __init__(self, state_dim, action_dim, gamma=0.99, lr=1e-2):\n",
        "        self.policy = PolicyNetwork(state_dim, action_dim)\n",
        "        self.optimizer = optim.Adam(self.policy.parameters(), lr=lr)\n",
        "        self.gamma = gamma\n",
        "        self.trajectory = []\n",
        "\n",
        "    def select_action(self, state):\n",
        "        state_tensor = torch.FloatTensor([state])\n",
        "        probs = self.policy(state_tensor)\n",
        "        action_dist = torch.distributions.Categorical(probs)\n",
        "        action = action_dist.sample()\n",
        "        self.trajectory.append((state_tensor, action, action_dist.log_prob(action)))\n",
        "        return action.item()\n",
        "\n",
        "    def update_policy(self, rewards):\n",
        "        returns = []\n",
        "        G = 0\n",
        "        for r in reversed(rewards):\n",
        "            G = r + self.gamma * G\n",
        "            returns.insert(0, G)\n",
        "        returns = torch.tensor(returns)\n",
        "\n",
        "        loss = 0\n",
        "        for (_, _, log_prob), G in zip(self.trajectory, returns):\n",
        "            loss -= log_prob * G  # gradient ascent\n",
        "\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "        self.trajectory = []\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "REINFORCE with mean baseline\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class REINFORCEWithBaselineAgent:\n",
        "    def __init__(self, state_dim, action_dim, gamma=0.99, lr=1e-2):\n",
        "        self.policy = PolicyNetwork(state_dim, action_dim)\n",
        "        self.optimizer = optim.Adam(self.policy.parameters(), lr=lr)\n",
        "        self.gamma = gamma\n",
        "        self.trajectory = []\n",
        "        self.returns = []\n",
        "\n",
        "    def select_action(self, state):\n",
        "        state_tensor = torch.FloatTensor([state])\n",
        "        probs = self.policy(state_tensor)\n",
        "        dist = torch.distributions.Categorical(probs)\n",
        "        action = dist.sample()\n",
        "        self.trajectory.append((state_tensor, action, dist.log_prob(action)))\n",
        "        return action.item()\n",
        "\n",
        "    def update_policy(self, rewards):\n",
        "        returns = []\n",
        "        G = 0\n",
        "        for r in reversed(rewards):\n",
        "            G = r + self.gamma * G\n",
        "            returns.insert(0, G)\n",
        "\n",
        "        self.returns.extend(returns)\n",
        "        baseline = np.mean(self.returns)\n",
        "\n",
        "        loss = 0\n",
        "        for (_, _, log_prob), G in zip(self.trajectory, returns):\n",
        "            loss -= log_prob * (G - baseline)\n",
        "\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "        self.trajectory = []\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "REINFORCE with Baseline Learned by a Critic\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ValueNetwork(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super().__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(input_dim, 32),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(32, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "class REINFORCEWithCriticAgent:\n",
        "    def __init__(self, state_dim, action_dim, gamma=0.99, lr=1e-2):\n",
        "        self.policy = PolicyNetwork(state_dim, action_dim)\n",
        "        self.value = ValueNetwork(state_dim)\n",
        "        self.policy_opt = optim.Adam(self.policy.parameters(), lr=lr)\n",
        "        self.value_opt = optim.Adam(self.value.parameters(), lr=lr)\n",
        "        self.gamma = gamma\n",
        "        self.trajectory = []\n",
        "\n",
        "    def select_action(self, state):\n",
        "        state_tensor = torch.FloatTensor([state])\n",
        "        probs = self.policy(state_tensor)\n",
        "        dist = torch.distributions.Categorical(probs)\n",
        "        action = dist.sample()\n",
        "        self.trajectory.append((state_tensor, action, dist.log_prob(action)))\n",
        "        return action.item()\n",
        "\n",
        "    def update_policy(self, rewards):\n",
        "        returns = []\n",
        "        G = 0\n",
        "        for r in reversed(rewards):\n",
        "            G = r + self.gamma * G\n",
        "            returns.insert(0, G)\n",
        "\n",
        "        loss_policy = 0\n",
        "        loss_value = 0\n",
        "        for (state, _, log_prob), G in zip(self.trajectory, returns):\n",
        "            baseline = self.value(state).squeeze()\n",
        "            advantage = G - baseline\n",
        "            loss_policy -= log_prob * advantage.detach()\n",
        "            loss_value += (baseline - G) ** 2\n",
        "\n",
        "        self.policy_opt.zero_grad()\n",
        "        loss_policy.backward()\n",
        "        self.policy_opt.step()\n",
        "\n",
        "        self.value_opt.zero_grad()\n",
        "        loss_value.backward()\n",
        "        self.value_opt.step()\n",
        "\n",
        "        self.trajectory = []\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "PPO A2C style\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class A2CPolicyNetwork(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim):\n",
        "        super().__init__()\n",
        "        self.fc = nn.Linear(state_dim, 32)\n",
        "        self.action_head = nn.Linear(32, action_dim)\n",
        "        self.value_head = nn.Linear(32, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc(x))\n",
        "        return torch.softmax(self.action_head(x), dim=-1), self.value_head(x)\n",
        "\n",
        "class PPOA2CAgent:\n",
        "    def __init__(self, state_dim, action_dim, gamma=0.99, lr=1e-2):\n",
        "        self.model = A2CPolicyNetwork(state_dim, action_dim)\n",
        "        self.optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
        "        self.gamma = gamma\n",
        "        self.trajectory = []\n",
        "\n",
        "    def select_action(self, state):\n",
        "        state_tensor = torch.FloatTensor([state])\n",
        "        probs, _ = self.model(state_tensor)\n",
        "        dist = torch.distributions.Categorical(probs)\n",
        "        action = dist.sample()\n",
        "        self.trajectory.append((state_tensor, action, dist.log_prob(action)))\n",
        "        return action.item()\n",
        "\n",
        "    def update_policy(self, rewards):\n",
        "        returns = []\n",
        "        G = 0\n",
        "        for r in reversed(rewards):\n",
        "            G = r + self.gamma * G\n",
        "            returns.insert(0, G)\n",
        "        returns = torch.FloatTensor(returns)\n",
        "\n",
        "        loss = 0\n",
        "        for (state, _, log_prob), G in zip(self.trajectory, returns):\n",
        "            _, value = self.model(state)\n",
        "            advantage = G - value.item()\n",
        "            loss -= log_prob * advantage\n",
        "\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "        self.trajectory = []\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "RandomRollout"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class RandomRolloutAgent:\n",
        "    def __init__(self, n_actions):\n",
        "        self.n_actions = n_actions\n",
        "\n",
        "    def select_action(self, state):\n",
        "        return random.randint(0, self.n_actions - 1)\n",
        "\n",
        "    def update_policy(self, rewards):\n",
        "        pass  # sans training\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Monte Carlo Tree Search (UCT)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MCTSNode:\n",
        "    def __init__(self, state, parent=None):\n",
        "        self.state = state\n",
        "        self.parent = parent\n",
        "        self.children = {}\n",
        "        self.visits = 0\n",
        "        self.value = 0.0\n",
        "\n",
        "class MCTSAgent:\n",
        "    def __init__(self, env, n_actions, simulations=100, c=1.4):\n",
        "        self.env = env\n",
        "        self.n_actions = n_actions\n",
        "        self.simulations = simulations\n",
        "        self.c = c\n",
        "\n",
        "    def select_action(self, state):\n",
        "        root = MCTSNode(state)\n",
        "        for _ in range(self.simulations):\n",
        "            self.simulate(root)\n",
        "        return max(root.children.items(), key=lambda item: item[1].visits)[0]\n",
        "\n",
        "    def simulate(self, node):\n",
        "        env_copy = LineWorld(length=self.env.length)\n",
        "        env_copy.state = int(node.state)\n",
        "        path = []\n",
        "        while True:\n",
        "            if node.children == {}:\n",
        "                for a in range(self.n_actions):\n",
        "                    env_copy.state = int(node.state)\n",
        "                    s_, r, d, _ = env_copy.step(a)\n",
        "                    node.children[a] = MCTSNode(s_, node)\n",
        "                break\n",
        "            action, node = max(\n",
        "                node.children.items(),\n",
        "                key=lambda item: item[1].value / (1 + item[1].visits) +\n",
        "                self.c * np.sqrt(np.log(node.visits + 1) / (item[1].visits + 1e-5))\n",
        "            )\n",
        "            path.append(node)\n",
        "            env_copy.state = int(node.state)\n",
        "            _, r, done, _ = env_copy.step(action)\n",
        "            if done:\n",
        "                break\n",
        "        for n in path:\n",
        "            n.visits += 1\n",
        "            n.value += r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Expert Apprentice"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ExpertApprenticeAgent:\n",
        "    def __init__(self, env):\n",
        "        self.env = env\n",
        "\n",
        "    def select_action(self, state):\n",
        "        return 1 if state < self.env.end_state else 0  # Va toujours à droite\n",
        "\n",
        "    def update_policy(self, rewards):\n",
        "        pass  # sans training"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".conda",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
