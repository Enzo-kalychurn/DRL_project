{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HNTb5A2iYRCk"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AwW9kuy_Y3xo"
      },
      "source": [
        "Implémentation de environnement LineWorld"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NQ8hg8HOYsLJ"
      },
      "outputs": [],
      "source": [
        "class LineWorld:\n",
        "    def __init__(self, length=5):\n",
        "        self.length = length\n",
        "        self.start_state = 0\n",
        "        self.end_state = length - 1\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.state = self.start_state\n",
        "        return self.state\n",
        "\n",
        "    def step(self, action):\n",
        "        # 0 = gauche, 1 = droite\n",
        "        if action == 0:\n",
        "            self.state = max(0, self.state - 1)\n",
        "        elif action == 1:\n",
        "            self.state = min(self.length - 1, self.state + 1)\n",
        "\n",
        "        reward = 1 if self.state == self.end_state else 0\n",
        "        done = self.state == self.end_state\n",
        "        return self.state, reward, done, {}\n",
        "\n",
        "    def get_valid_actions(self):\n",
        "        return [0, 1]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y7jLpPEZZTfg"
      },
      "source": [
        "Lancement des Agents test sur l'environnement LinWorld :"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EH4RO15pZcH5"
      },
      "source": [
        "Agent Random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uklAV2j6Zoit",
        "outputId": "e7e05e0d-4de2-4e0c-a20c-23085f682ede"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "def run_random_agent(env, num_episodes=1000):\n",
        "    total_rewards = []\n",
        "    total_steps = []\n",
        "\n",
        "    for episode in range(num_episodes):\n",
        "        state = env.reset()\n",
        "        done = False\n",
        "        episode_reward = 0\n",
        "        steps = 0\n",
        "\n",
        "        while not done:\n",
        "            action = random.choice(env.get_valid_actions())\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "            episode_reward += reward\n",
        "            steps += 1\n",
        "\n",
        "        total_rewards.append(episode_reward)\n",
        "        total_steps.append(steps)\n",
        "\n",
        "    avg_reward = sum(total_rewards) / num_episodes\n",
        "    avg_steps = sum(total_steps) / num_episodes\n",
        "    print(f\"Random Agent on LineWorld ({env.length} states)\")\n",
        "    print(f\"Average reward: {avg_reward:.2f}\")\n",
        "    print(f\"Average episode length: {avg_steps:.2f} steps\")\n",
        "\n",
        "# Exécution\n",
        "if __name__ == \"__main__\":\n",
        "    env = LineWorld(length=5)\n",
        "    run_random_agent(env)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rYw6iV_Wblqp"
      },
      "source": [
        "Agent TabularQLearning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cEsWh728eaWQ"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "from collections import defaultdict\n",
        "\n",
        "class TabularQLearningAgent:\n",
        "    def __init__(self, n_states, n_actions, alpha=0.1, gamma=0.99, epsilon=0.1):\n",
        "        self.q_table = defaultdict(lambda: np.zeros(n_actions))\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon\n",
        "        self.n_actions = n_actions\n",
        "\n",
        "    def select_action(self, state):\n",
        "        if random.random() < self.epsilon:\n",
        "            return random.randint(0, self.n_actions - 1)\n",
        "        return np.argmax(self.q_table[state])\n",
        "\n",
        "    def learn(self, state, action, reward, next_state, done):\n",
        "        q_predict = self.q_table[state][action]\n",
        "        q_target = reward if done else reward + self.gamma * np.max(self.q_table[next_state])\n",
        "        self.q_table[state][action] += self.alpha * (q_target - q_predict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZbeV2hXxbswb"
      },
      "source": [
        "Agent DeepQLearning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yw8-nY2AhaJs"
      },
      "outputs": [],
      "source": [
        "class DQNNetwork(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(input_dim, 64), nn.ReLU(),\n",
        "            nn.Linear(64, 64), nn.ReLU(),\n",
        "            nn.Linear(64, output_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class DQNAgent:\n",
        "    def __init__(self, state_dim, n_actions, gamma=0.8, epsilon=0.99, epsilon_decay=0.995, epsilon_min=0.01, lr=1e-3):\n",
        "        self.state_dim = state_dim\n",
        "        self.n_actions = n_actions\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon\n",
        "        self.epsilon_decay = epsilon_decay\n",
        "        self.epsilon_min = epsilon_min\n",
        "\n",
        "        self.model = DQNNetwork(state_dim, n_actions)\n",
        "        self.optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
        "        self.criterion = nn.MSELoss()\n",
        "\n",
        "    def select_action(self, state):\n",
        "        if random.random() < self.epsilon:\n",
        "            return random.randint(0, self.n_actions - 1)\n",
        "        with torch.no_grad():\n",
        "            # state_tensor = torch.FloatTensor([state])\n",
        "            state_tensor = torch.FloatTensor(state).unsqueeze(0)  \n",
        "            q_values = self.model(state_tensor)\n",
        "            return torch.argmax(q_values).item()\n",
        "\n",
        "    def learn(self, state, action, reward, next_state, done):\n",
        "        state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
        "        next_state_tensor = torch.FloatTensor(next_state).unsqueeze(0)\n",
        "        reward_tensor = torch.tensor(reward, dtype=torch.float32)\n",
        "        done_tensor = torch.tensor(done, dtype=torch.float32)\n",
        "\n",
        "        q_values = self.model(state_tensor)\n",
        "        next_q_values = self.model(next_state_tensor)\n",
        "\n",
        "        target = reward_tensor if done else reward_tensor + self.gamma * torch.max(next_q_values).detach()\n",
        "\n",
        "        loss = self.criterion(q_values[0][action], target)\n",
        "\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon *= self.epsilon_decay\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_dqn(agent, env, episodes=1000):\n",
        "    rewards_per_episode = []\n",
        "\n",
        "    for episode in range(episodes):\n",
        "        state = env.reset()\n",
        "        total_reward = 0\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            action = agent.select_action([state])\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "            agent.learn([state], action, reward, [next_state], done)\n",
        "            state = next_state\n",
        "            total_reward += reward\n",
        "\n",
        "        rewards_per_episode.append(total_reward)\n",
        "\n",
        "        if (episode + 1) % 50 == 0:\n",
        "            avg_reward = np.mean(rewards_per_episode[-50:])\n",
        "            print(f\"Episode {episode+1}/{episodes} - Moyenne sur 50 derniers épisodes : {avg_reward:.3f}\")\n",
        "\n",
        "    return rewards_per_episode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_rewards(rewards, title=\"Récompense par épisode\"):\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.plot(rewards, label='Reward par épisode')\n",
        "    plt.plot(\n",
        "        np.convolve(rewards, np.ones(50)/50, mode='valid'),\n",
        "        label='Reward moyenne (fenêtre=50)', color='orange'\n",
        "    )\n",
        "    plt.xlabel('Épisodes')\n",
        "    plt.ylabel('Récompense')\n",
        "    plt.title(title)\n",
        "    plt.legend()\n",
        "    plt.grid()\n",
        "    plt.show()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    env = LineWorld(length=5)\n",
        "    agent = DQNAgent(state_dim=1, n_actions=2)\n",
        "    rewards = train_dqn(agent, env, episodes=500)\n",
        "    plot_rewards(rewards, title=\"DQN sur LineWorld\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qOWhQUP-bw7C"
      },
      "source": [
        "Agent DoubleDeepQLearning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jdSBSFXWhiBL"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "class DoubleDQNNetwork(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(input_dim, 64), nn.ReLU(),\n",
        "            nn.Linear(64, 64), nn.ReLU(),\n",
        "            nn.Linear(64, output_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class DoubleDQNAgent:\n",
        "    def __init__(self, state_dim, n_actions, gamma=0.99, epsilon=1.0, epsilon_decay=0.995, epsilon_min=0.01, lr=1e-3):\n",
        "        self.state_dim = state_dim\n",
        "        self.n_actions = n_actions\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon\n",
        "        self.epsilon_decay = epsilon_decay\n",
        "        self.epsilon_min = epsilon_min\n",
        "\n",
        "        self.policy_net = DoubleDQNNetwork(state_dim, n_actions)\n",
        "        self.target_net = DoubleDQNNetwork(state_dim, n_actions)\n",
        "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
        "        self.target_net.eval()\n",
        "\n",
        "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=lr)\n",
        "        self.criterion = nn.MSELoss()\n",
        "        self.update_counter = 0\n",
        "\n",
        "    def select_action(self, state):\n",
        "        if random.random() < self.epsilon:\n",
        "            return random.randint(0, self.n_actions - 1)\n",
        "        with torch.no_grad():\n",
        "            state_tensor = torch.FloatTensor([state])\n",
        "            q_values = self.policy_net(state_tensor)\n",
        "            return torch.argmax(q_values).item()\n",
        "\n",
        "    def learn(self, state, action, reward, next_state, done):\n",
        "        state_tensor = torch.FloatTensor([state])\n",
        "        next_state_tensor = torch.FloatTensor([next_state])\n",
        "        reward_tensor = torch.tensor(reward)\n",
        "        done_tensor = torch.tensor(done, dtype=torch.float32)\n",
        "\n",
        "        current_q = self.policy_net(state_tensor)[0][action]\n",
        "\n",
        "        # Double DQN trick\n",
        "        with torch.no_grad():\n",
        "            next_action = torch.argmax(self.policy_net(next_state_tensor), dim=1)\n",
        "            next_q = self.target_net(next_state_tensor)[0][next_action]\n",
        "            target_q = reward_tensor if done else reward_tensor + self.gamma * next_q\n",
        "\n",
        "        loss = self.criterion(current_q, target_q)\n",
        "\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon *= self.epsilon_decay\n",
        "\n",
        "        self.update_counter += 1\n",
        "        if self.update_counter % 10 == 0:\n",
        "            self.target_net.load_state_dict(self.policy_net.state_dict())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-0RlY1Xvb4ET"
      },
      "source": [
        "Agent DoubleDeepQLearningWithExperienceReplay :"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pLzMxTieh_hj"
      },
      "source": [
        "Buffer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iMAQu0q-h3D1"
      },
      "outputs": [],
      "source": [
        "from collections import deque\n",
        "\n",
        "class ReplayBuffer:\n",
        "    def __init__(self, capacity=10000):\n",
        "        self.buffer = deque(maxlen=capacity)\n",
        "\n",
        "    def add(self, state, action, reward, next_state, done):\n",
        "        self.buffer.append((state, action, reward, next_state, done))\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        return random.sample(self.buffer, batch_size)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.buffer)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PpesRJesiBW4"
      },
      "source": [
        "DDQL ER"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bVN6142zh37D"
      },
      "outputs": [],
      "source": [
        "class DoubleDQNWithReplayAgent:\n",
        "    def __init__(self, state_dim, n_actions, gamma=0.99, epsilon=1.0, epsilon_decay=0.995, epsilon_min=0.01, lr=1e-3, buffer_size=10000, batch_size=64):\n",
        "        self.state_dim = state_dim\n",
        "        self.n_actions = n_actions\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon\n",
        "        self.epsilon_decay = epsilon_decay\n",
        "        self.epsilon_min = epsilon_min\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        self.policy_net = DoubleDQNNetwork(state_dim, n_actions)\n",
        "        self.target_net = DoubleDQNNetwork(state_dim, n_actions)\n",
        "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
        "        self.target_net.eval()\n",
        "\n",
        "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=lr)\n",
        "        self.criterion = nn.MSELoss()\n",
        "\n",
        "        self.memory = ReplayBuffer(capacity=buffer_size)\n",
        "        self.update_counter = 0\n",
        "\n",
        "    def select_action(self, state):\n",
        "        if random.random() < self.epsilon:\n",
        "            return random.randint(0, self.n_actions - 1)\n",
        "        with torch.no_grad():\n",
        "            state_tensor = torch.FloatTensor([state])\n",
        "            q_values = self.policy_net(state_tensor)\n",
        "            return torch.argmax(q_values).item()\n",
        "\n",
        "    def store_transition(self, state, action, reward, next_state, done):\n",
        "        self.memory.add(state, action, reward, next_state, done)\n",
        "\n",
        "    def learn(self):\n",
        "        if len(self.memory) < self.batch_size:\n",
        "            return\n",
        "\n",
        "        batch = self.memory.sample(self.batch_size)\n",
        "        states, actions, rewards, next_states, dones = zip(*batch)\n",
        "\n",
        "        states = torch.FloatTensor(states)\n",
        "        actions = torch.LongTensor(actions)\n",
        "        rewards = torch.FloatTensor(rewards)\n",
        "        next_states = torch.FloatTensor(next_states)\n",
        "        dones = torch.FloatTensor(dones)\n",
        "\n",
        "        current_q = self.policy_net(states).gather(1, actions.unsqueeze(1)).squeeze()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            next_actions = torch.argmax(self.policy_net(next_states), dim=1)\n",
        "            next_q = self.target_net(next_states).gather(1, next_actions.unsqueeze(1)).squeeze()\n",
        "            target_q = rewards + self.gamma * next_q * (1 - dones)\n",
        "\n",
        "        loss = self.criterion(current_q, target_q)\n",
        "\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon *= self.epsilon_decay\n",
        "\n",
        "        self.update_counter += 1\n",
        "        if self.update_counter % 10 == 0:\n",
        "            self.target_net.load_state_dict(self.policy_net.state_dict())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "be3Feyhlb81m"
      },
      "source": [
        "Agent DoubleDeepQLearningWithPrioritizedExperienceReplay"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Buffer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class PrioritizedReplayBuffer:\n",
        "    def __init__(self, capacity=10000, alpha=0.6):\n",
        "        self.capacity = capacity\n",
        "        self.buffer = []\n",
        "        self.priorities = []\n",
        "        self.alpha = alpha\n",
        "        self.pos = 0\n",
        "\n",
        "    def add(self, state, action, reward, next_state, done, td_error=1.0):\n",
        "        priority = (abs(td_error) + 1e-5) ** self.alpha\n",
        "        if len(self.buffer) < self.capacity:\n",
        "            self.buffer.append((state, action, reward, next_state, done))\n",
        "            self.priorities.append(priority)\n",
        "        else:\n",
        "            self.buffer[self.pos] = (state, action, reward, next_state, done)\n",
        "            self.priorities[self.pos] = priority\n",
        "            self.pos = (self.pos + 1) % self.capacity\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        probs = np.array(self.priorities) / sum(self.priorities)\n",
        "        indices = np.random.choice(len(self.buffer), batch_size, p=probs)\n",
        "        samples = [self.buffer[i] for i in indices]\n",
        "        return samples\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.buffer)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "DDQLPER"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class DoubleDQNWithPrioritizedReplayAgent:\n",
        "    def __init__(self, state_dim, n_actions, gamma=0.99, epsilon=1.0, epsilon_decay=0.995, epsilon_min=0.01, lr=1e-3, buffer_size=10000, batch_size=64):\n",
        "        self.state_dim = state_dim\n",
        "        self.n_actions = n_actions\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon\n",
        "        self.epsilon_decay = epsilon_decay\n",
        "        self.epsilon_min = epsilon_min\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        self.policy_net = DoubleDQNNetwork(state_dim, n_actions)\n",
        "        self.target_net = DoubleDQNNetwork(state_dim, n_actions)\n",
        "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
        "        self.target_net.eval()\n",
        "\n",
        "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=lr)\n",
        "        self.criterion = nn.MSELoss()\n",
        "\n",
        "        self.memory = PrioritizedReplayBuffer(capacity=buffer_size)\n",
        "        self.update_counter = 0\n",
        "\n",
        "    def select_action(self, state):\n",
        "        if random.random() < self.epsilon:\n",
        "            return random.randint(0, self.n_actions - 1)\n",
        "        with torch.no_grad():\n",
        "            state_tensor = torch.FloatTensor([state])\n",
        "            q_values = self.policy_net(state_tensor)\n",
        "            return torch.argmax(q_values).item()\n",
        "\n",
        "    def store_transition(self, state, action, reward, next_state, done):\n",
        "        with torch.no_grad():\n",
        "            s_tensor = torch.FloatTensor([state])\n",
        "            ns_tensor = torch.FloatTensor([next_state])\n",
        "            q_val = self.policy_net(s_tensor)[0][action]\n",
        "            next_action = torch.argmax(self.policy_net(ns_tensor)).item()\n",
        "            next_q = self.target_net(ns_tensor)[0][next_action]\n",
        "            target = reward + self.gamma * next_q * (1 - int(done))\n",
        "            td_error = abs(q_val.item() - target.item())\n",
        "\n",
        "        self.memory.add(state, action, reward, next_state, done, td_error)\n",
        "\n",
        "    def learn(self):\n",
        "        if len(self.memory) < self.batch_size:\n",
        "            return\n",
        "\n",
        "        batch = self.memory.sample(self.batch_size)\n",
        "        states, actions, rewards, next_states, dones = zip(*batch)\n",
        "\n",
        "        states = torch.FloatTensor(states)\n",
        "        actions = torch.LongTensor(actions)\n",
        "        rewards = torch.FloatTensor(rewards)\n",
        "        next_states = torch.FloatTensor(next_states)\n",
        "        dones = torch.FloatTensor(dones)\n",
        "\n",
        "        q_values = self.policy_net(states).gather(1, actions.unsqueeze(1)).squeeze()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            next_actions = torch.argmax(self.policy_net(next_states), dim=1)\n",
        "            next_q = self.target_net(next_states).gather(1, next_actions.unsqueeze(1)).squeeze()\n",
        "            targets = rewards + self.gamma * next_q * (1 - dones)\n",
        "\n",
        "        loss = self.criterion(q_values, targets)\n",
        "\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon *= self.epsilon_decay\n",
        "\n",
        "        self.update_counter += 1\n",
        "        if self.update_counter % 10 == 0:\n",
        "            self.target_net.load_state_dict(self.policy_net.state_dict())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "REINFORCE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class PolicyNetwork(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(state_dim, 32),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(32, action_dim),\n",
        "            nn.Softmax(dim=-1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "    \n",
        "class REINFORCEAgent:\n",
        "    def __init__(self, state_dim, action_dim, gamma=0.99, lr=1e-2):\n",
        "        self.policy = PolicyNetwork(state_dim, action_dim)\n",
        "        self.optimizer = optim.Adam(self.policy.parameters(), lr=lr)\n",
        "        self.gamma = gamma\n",
        "        self.trajectory = []\n",
        "\n",
        "    def select_action(self, state):\n",
        "        state_tensor = torch.FloatTensor([state])\n",
        "        probs = self.policy(state_tensor)\n",
        "        action_dist = torch.distributions.Categorical(probs)\n",
        "        action = action_dist.sample()\n",
        "        self.trajectory.append((state_tensor, action, action_dist.log_prob(action)))\n",
        "        return action.item()\n",
        "\n",
        "    def update_policy(self, rewards):\n",
        "        returns = []\n",
        "        G = 0\n",
        "        for r in reversed(rewards):\n",
        "            G = r + self.gamma * G\n",
        "            returns.insert(0, G)\n",
        "        returns = torch.tensor(returns)\n",
        "\n",
        "        loss = 0\n",
        "        for (_, _, log_prob), G in zip(self.trajectory, returns):\n",
        "            loss -= log_prob * G  # gradient ascent\n",
        "\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "        self.trajectory = []\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "REINFORCE with mean baseline\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class REINFORCEWithBaselineAgent:\n",
        "    def __init__(self, state_dim, action_dim, gamma=0.99, lr=1e-2):\n",
        "        self.policy = PolicyNetwork(state_dim, action_dim)\n",
        "        self.optimizer = optim.Adam(self.policy.parameters(), lr=lr)\n",
        "        self.gamma = gamma\n",
        "        self.trajectory = []\n",
        "        self.returns = []\n",
        "\n",
        "    def select_action(self, state):\n",
        "        state_tensor = torch.FloatTensor([state])\n",
        "        probs = self.policy(state_tensor)\n",
        "        dist = torch.distributions.Categorical(probs)\n",
        "        action = dist.sample()\n",
        "        self.trajectory.append((state_tensor, action, dist.log_prob(action)))\n",
        "        return action.item()\n",
        "\n",
        "    def update_policy(self, rewards):\n",
        "        returns = []\n",
        "        G = 0\n",
        "        for r in reversed(rewards):\n",
        "            G = r + self.gamma * G\n",
        "            returns.insert(0, G)\n",
        "\n",
        "        self.returns.extend(returns)\n",
        "        baseline = np.mean(self.returns)\n",
        "\n",
        "        loss = 0\n",
        "        for (_, _, log_prob), G in zip(self.trajectory, returns):\n",
        "            loss -= log_prob * (G - baseline)\n",
        "\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "        self.trajectory = []\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "REINFORCE with Baseline Learned by a Critic\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ValueNetwork(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super().__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(input_dim, 32),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(32, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "class REINFORCEWithCriticAgent:\n",
        "    def __init__(self, state_dim, action_dim, gamma=0.99, lr=1e-2):\n",
        "        self.policy = PolicyNetwork(state_dim, action_dim)\n",
        "        self.value = ValueNetwork(state_dim)\n",
        "        self.policy_opt = optim.Adam(self.policy.parameters(), lr=lr)\n",
        "        self.value_opt = optim.Adam(self.value.parameters(), lr=lr)\n",
        "        self.gamma = gamma\n",
        "        self.trajectory = []\n",
        "\n",
        "    def select_action(self, state):\n",
        "        state_tensor = torch.FloatTensor([state])\n",
        "        probs = self.policy(state_tensor)\n",
        "        dist = torch.distributions.Categorical(probs)\n",
        "        action = dist.sample()\n",
        "        self.trajectory.append((state_tensor, action, dist.log_prob(action)))\n",
        "        return action.item()\n",
        "\n",
        "    def update_policy(self, rewards):\n",
        "        returns = []\n",
        "        G = 0\n",
        "        for r in reversed(rewards):\n",
        "            G = r + self.gamma * G\n",
        "            returns.insert(0, G)\n",
        "\n",
        "        loss_policy = 0\n",
        "        loss_value = 0\n",
        "        for (state, _, log_prob), G in zip(self.trajectory, returns):\n",
        "            baseline = self.value(state).squeeze()\n",
        "            advantage = G - baseline\n",
        "            loss_policy -= log_prob * advantage.detach()\n",
        "            loss_value += (baseline - G) ** 2\n",
        "\n",
        "        self.policy_opt.zero_grad()\n",
        "        loss_policy.backward()\n",
        "        self.policy_opt.step()\n",
        "\n",
        "        self.value_opt.zero_grad()\n",
        "        loss_value.backward()\n",
        "        self.value_opt.step()\n",
        "\n",
        "        self.trajectory = []\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "PPO A2C style\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class A2CPolicyNetwork(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim):\n",
        "        super().__init__()\n",
        "        self.fc = nn.Linear(state_dim, 32)\n",
        "        self.action_head = nn.Linear(32, action_dim)\n",
        "        self.value_head = nn.Linear(32, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc(x))\n",
        "        return torch.softmax(self.action_head(x), dim=-1), self.value_head(x)\n",
        "\n",
        "class PPOA2CAgent:\n",
        "    def __init__(self, state_dim, action_dim, gamma=0.99, lr=1e-2):\n",
        "        self.model = A2CPolicyNetwork(state_dim, action_dim)\n",
        "        self.optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
        "        self.gamma = gamma\n",
        "        self.trajectory = []\n",
        "\n",
        "    def select_action(self, state):\n",
        "        state_tensor = torch.FloatTensor([state])\n",
        "        probs, _ = self.model(state_tensor)\n",
        "        dist = torch.distributions.Categorical(probs)\n",
        "        action = dist.sample()\n",
        "        self.trajectory.append((state_tensor, action, dist.log_prob(action)))\n",
        "        return action.item()\n",
        "\n",
        "    def update_policy(self, rewards):\n",
        "        returns = []\n",
        "        G = 0\n",
        "        for r in reversed(rewards):\n",
        "            G = r + self.gamma * G\n",
        "            returns.insert(0, G)\n",
        "        returns = torch.FloatTensor(returns)\n",
        "\n",
        "        loss = 0\n",
        "        for (state, _, log_prob), G in zip(self.trajectory, returns):\n",
        "            _, value = self.model(state)\n",
        "            advantage = G - value.item()\n",
        "            loss -= log_prob * advantage\n",
        "\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "        self.trajectory = []\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "RandomRollout"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class RandomRolloutAgent:\n",
        "    def __init__(self, n_actions):\n",
        "        self.n_actions = n_actions\n",
        "\n",
        "    def select_action(self, state):\n",
        "        return random.randint(0, self.n_actions - 1)\n",
        "\n",
        "    def update_policy(self, rewards):\n",
        "        pass  # sans training\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Monte Carlo Tree Search (UCT)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MCTSNode:\n",
        "    def __init__(self, state, parent=None):\n",
        "        self.state = state\n",
        "        self.parent = parent\n",
        "        self.children = {}\n",
        "        self.visits = 0\n",
        "        self.value = 0.0\n",
        "\n",
        "class MCTSAgent:\n",
        "    def __init__(self, env, n_actions, simulations=100, c=1.4):\n",
        "        self.env = env\n",
        "        self.n_actions = n_actions\n",
        "        self.simulations = simulations\n",
        "        self.c = c\n",
        "\n",
        "    def select_action(self, state):\n",
        "        root = MCTSNode(state)\n",
        "        for _ in range(self.simulations):\n",
        "            self.simulate(root)\n",
        "        return max(root.children.items(), key=lambda item: item[1].visits)[0]\n",
        "\n",
        "    def simulate(self, node):\n",
        "        env_copy = LineWorld(length=self.env.length)\n",
        "        env_copy.state = int(node.state)\n",
        "        path = []\n",
        "        while True:\n",
        "            if node.children == {}:\n",
        "                for a in range(self.n_actions):\n",
        "                    env_copy.state = int(node.state)\n",
        "                    s_, r, d, _ = env_copy.step(a)\n",
        "                    node.children[a] = MCTSNode(s_, node)\n",
        "                break\n",
        "            action, node = max(\n",
        "                node.children.items(),\n",
        "                key=lambda item: item[1].value / (1 + item[1].visits) +\n",
        "                self.c * np.sqrt(np.log(node.visits + 1) / (item[1].visits + 1e-5))\n",
        "            )\n",
        "            path.append(node)\n",
        "            env_copy.state = int(node.state)\n",
        "            _, r, done, _ = env_copy.step(action)\n",
        "            if done:\n",
        "                break\n",
        "        for n in path:\n",
        "            n.visits += 1\n",
        "            n.value += r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Expert Apprentice"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ExpertApprenticeAgent:\n",
        "    def __init__(self, env):\n",
        "        self.env = env\n",
        "\n",
        "    def select_action(self, state):\n",
        "        return 1 if state < self.env.end_state else 0  # Va toujours à droite\n",
        "\n",
        "    def update_policy(self, rewards):\n",
        "        pass  # sans training"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".conda",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
