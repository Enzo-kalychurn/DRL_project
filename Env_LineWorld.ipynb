{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "HNTb5A2iYRCk"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implémentation de environnement LineWorld"
      ],
      "metadata": {
        "id": "AwW9kuy_Y3xo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LineWorld:\n",
        "    def __init__(self, length=5):\n",
        "        self.length = length\n",
        "        self.start_state = 0\n",
        "        self.end_state = length - 1\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.state = self.start_state\n",
        "        return self.state\n",
        "\n",
        "    def step(self, action):\n",
        "        # 0 = gauche, 1 = droite\n",
        "        if action == 0:\n",
        "            self.state = max(0, self.state - 1)\n",
        "        elif action == 1:\n",
        "            self.state = min(self.length - 1, self.state + 1)\n",
        "\n",
        "        reward = 1 if self.state == self.end_state else 0\n",
        "        done = self.state == self.end_state\n",
        "        return self.state, reward, done, {}\n",
        "\n",
        "    def get_valid_actions(self):\n",
        "        return [0, 1]\n"
      ],
      "metadata": {
        "id": "NQ8hg8HOYsLJ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lancement des Agents test sur l'environnement LinWorld :"
      ],
      "metadata": {
        "id": "Y7jLpPEZZTfg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Agent Random"
      ],
      "metadata": {
        "id": "EH4RO15pZcH5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "def run_random_agent(env, num_episodes=1000):\n",
        "    total_rewards = []\n",
        "    total_steps = []\n",
        "\n",
        "    for episode in range(num_episodes):\n",
        "        state = env.reset()\n",
        "        done = False\n",
        "        episode_reward = 0\n",
        "        steps = 0\n",
        "\n",
        "        while not done:\n",
        "            action = random.choice(env.get_valid_actions())\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "            episode_reward += reward\n",
        "            steps += 1\n",
        "\n",
        "        total_rewards.append(episode_reward)\n",
        "        total_steps.append(steps)\n",
        "\n",
        "    avg_reward = sum(total_rewards) / num_episodes\n",
        "    avg_steps = sum(total_steps) / num_episodes\n",
        "    print(f\"Random Agent on LineWorld ({env.length} states)\")\n",
        "    print(f\"Average reward: {avg_reward:.2f}\")\n",
        "    print(f\"Average episode length: {avg_steps:.2f} steps\")\n",
        "\n",
        "# Exécution\n",
        "if __name__ == \"__main__\":\n",
        "    env = LineWorld(length=5)\n",
        "    run_random_agent(env)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uklAV2j6Zoit",
        "outputId": "e7e05e0d-4de2-4e0c-a20c-23085f682ede"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Agent on LineWorld (5 states)\n",
            "Average reward: 1.00\n",
            "Average episode length: 20.09 steps\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Agent TabularQLearning"
      ],
      "metadata": {
        "id": "rYw6iV_Wblqp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "from collections import defaultdict\n",
        "\n",
        "class TabularQLearningAgent:\n",
        "    def __init__(self, n_states, n_actions, alpha=0.1, gamma=0.99, epsilon=0.1):\n",
        "        self.q_table = defaultdict(lambda: np.zeros(n_actions))\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon\n",
        "        self.n_actions = n_actions\n",
        "\n",
        "    def select_action(self, state):\n",
        "        if random.random() < self.epsilon:\n",
        "            return random.randint(0, self.n_actions - 1)\n",
        "        return np.argmax(self.q_table[state])\n",
        "\n",
        "    def learn(self, state, action, reward, next_state, done):\n",
        "        q_predict = self.q_table[state][action]\n",
        "        q_target = reward if done else reward + self.gamma * np.max(self.q_table[next_state])\n",
        "        self.q_table[state][action] += self.alpha * (q_target - q_predict)"
      ],
      "metadata": {
        "id": "cEsWh728eaWQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Agent DeepQLearning"
      ],
      "metadata": {
        "id": "ZbeV2hXxbswb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "class DQNNetwork(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(input_dim, 64), nn.ReLU(),\n",
        "            nn.Linear(64, 64), nn.ReLU(),\n",
        "            nn.Linear(64, output_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class DQNAgent:\n",
        "    def __init__(self, state_dim, n_actions, gamma=0.99, epsilon=1.0, epsilon_decay=0.995, epsilon_min=0.01, lr=1e-3):\n",
        "        self.state_dim = state_dim\n",
        "        self.n_actions = n_actions\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon\n",
        "        self.epsilon_decay = epsilon_decay\n",
        "        self.epsilon_min = epsilon_min\n",
        "\n",
        "        self.model = DQNNetwork(state_dim, n_actions)\n",
        "        self.optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
        "        self.criterion = nn.MSELoss()\n",
        "\n",
        "    def select_action(self, state):\n",
        "        if random.random() < self.epsilon:\n",
        "            return random.randint(0, self.n_actions - 1)\n",
        "        with torch.no_grad():\n",
        "            state_tensor = torch.FloatTensor([state])\n",
        "            q_values = self.model(state_tensor)\n",
        "            return torch.argmax(q_values).item()\n",
        "\n",
        "    def learn(self, state, action, reward, next_state, done):\n",
        "        state_tensor = torch.FloatTensor([state])\n",
        "        next_state_tensor = torch.FloatTensor([next_state])\n",
        "        reward_tensor = torch.tensor(reward)\n",
        "        done_tensor = torch.tensor(done, dtype=torch.float32)\n",
        "\n",
        "        q_values = self.model(state_tensor)\n",
        "        next_q_values = self.model(next_state_tensor)\n",
        "\n",
        "        target = reward_tensor if done else reward_tensor + self.gamma * torch.max(next_q_values).detach()\n",
        "\n",
        "        loss = self.criterion(q_values[0][action], target)\n",
        "\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon *= self.epsilon_decay\n"
      ],
      "metadata": {
        "id": "yw8-nY2AhaJs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Agent DoubleDeepQLearning"
      ],
      "metadata": {
        "id": "qOWhQUP-bw7C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "class DoubleDQNNetwork(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(input_dim, 64), nn.ReLU(),\n",
        "            nn.Linear(64, 64), nn.ReLU(),\n",
        "            nn.Linear(64, output_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class DoubleDQNAgent:\n",
        "    def __init__(self, state_dim, n_actions, gamma=0.99, epsilon=1.0, epsilon_decay=0.995, epsilon_min=0.01, lr=1e-3):\n",
        "        self.state_dim = state_dim\n",
        "        self.n_actions = n_actions\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon\n",
        "        self.epsilon_decay = epsilon_decay\n",
        "        self.epsilon_min = epsilon_min\n",
        "\n",
        "        self.policy_net = DoubleDQNNetwork(state_dim, n_actions)\n",
        "        self.target_net = DoubleDQNNetwork(state_dim, n_actions)\n",
        "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
        "        self.target_net.eval()\n",
        "\n",
        "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=lr)\n",
        "        self.criterion = nn.MSELoss()\n",
        "        self.update_counter = 0\n",
        "\n",
        "    def select_action(self, state):\n",
        "        if random.random() < self.epsilon:\n",
        "            return random.randint(0, self.n_actions - 1)\n",
        "        with torch.no_grad():\n",
        "            state_tensor = torch.FloatTensor([state])\n",
        "            q_values = self.policy_net(state_tensor)\n",
        "            return torch.argmax(q_values).item()\n",
        "\n",
        "    def learn(self, state, action, reward, next_state, done):\n",
        "        state_tensor = torch.FloatTensor([state])\n",
        "        next_state_tensor = torch.FloatTensor([next_state])\n",
        "        reward_tensor = torch.tensor(reward)\n",
        "        done_tensor = torch.tensor(done, dtype=torch.float32)\n",
        "\n",
        "        current_q = self.policy_net(state_tensor)[0][action]\n",
        "\n",
        "        # Double DQN trick\n",
        "        with torch.no_grad():\n",
        "            next_action = torch.argmax(self.policy_net(next_state_tensor), dim=1)\n",
        "            next_q = self.target_net(next_state_tensor)[0][next_action]\n",
        "            target_q = reward_tensor if done else reward_tensor + self.gamma * next_q\n",
        "\n",
        "        loss = self.criterion(current_q, target_q)\n",
        "\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon *= self.epsilon_decay\n",
        "\n",
        "        self.update_counter += 1\n",
        "        if self.update_counter % 10 == 0:\n",
        "            self.target_net.load_state_dict(self.policy_net.state_dict())\n"
      ],
      "metadata": {
        "id": "jdSBSFXWhiBL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Agent DoubleDeepQLearningWithExperienceReplay :"
      ],
      "metadata": {
        "id": "-0RlY1Xvb4ET"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Buffer"
      ],
      "metadata": {
        "id": "pLzMxTieh_hj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import deque\n",
        "\n",
        "class ReplayBuffer:\n",
        "    def __init__(self, capacity=10000):\n",
        "        self.buffer = deque(maxlen=capacity)\n",
        "\n",
        "    def add(self, state, action, reward, next_state, done):\n",
        "        self.buffer.append((state, action, reward, next_state, done))\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        return random.sample(self.buffer, batch_size)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.buffer)\n"
      ],
      "metadata": {
        "id": "iMAQu0q-h3D1"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "DDQL ER"
      ],
      "metadata": {
        "id": "PpesRJesiBW4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DoubleDQNWithReplayAgent:\n",
        "    def __init__(self, state_dim, n_actions, gamma=0.99, epsilon=1.0, epsilon_decay=0.995, epsilon_min=0.01, lr=1e-3, buffer_size=10000, batch_size=64):\n",
        "        self.state_dim = state_dim\n",
        "        self.n_actions = n_actions\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon\n",
        "        self.epsilon_decay = epsilon_decay\n",
        "        self.epsilon_min = epsilon_min\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        self.policy_net = DoubleDQNNetwork(state_dim, n_actions)\n",
        "        self.target_net = DoubleDQNNetwork(state_dim, n_actions)\n",
        "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
        "        self.target_net.eval()\n",
        "\n",
        "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=lr)\n",
        "        self.criterion = nn.MSELoss()\n",
        "\n",
        "        self.memory = ReplayBuffer(capacity=buffer_size)\n",
        "        self.update_counter = 0\n",
        "\n",
        "    def select_action(self, state):\n",
        "        if random.random() < self.epsilon:\n",
        "            return random.randint(0, self.n_actions - 1)\n",
        "        with torch.no_grad():\n",
        "            state_tensor = torch.FloatTensor([state])\n",
        "            q_values = self.policy_net(state_tensor)\n",
        "            return torch.argmax(q_values).item()\n",
        "\n",
        "    def store_transition(self, state, action, reward, next_state, done):\n",
        "        self.memory.add(state, action, reward, next_state, done)\n",
        "\n",
        "    def learn(self):\n",
        "        if len(self.memory) < self.batch_size:\n",
        "            return\n",
        "\n",
        "        batch = self.memory.sample(self.batch_size)\n",
        "        states, actions, rewards, next_states, dones = zip(*batch)\n",
        "\n",
        "        states = torch.FloatTensor(states)\n",
        "        actions = torch.LongTensor(actions)\n",
        "        rewards = torch.FloatTensor(rewards)\n",
        "        next_states = torch.FloatTensor(next_states)\n",
        "        dones = torch.FloatTensor(dones)\n",
        "\n",
        "        current_q = self.policy_net(states).gather(1, actions.unsqueeze(1)).squeeze()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            next_actions = torch.argmax(self.policy_net(next_states), dim=1)\n",
        "            next_q = self.target_net(next_states).gather(1, next_actions.unsqueeze(1)).squeeze()\n",
        "            target_q = rewards + self.gamma * next_q * (1 - dones)\n",
        "\n",
        "        loss = self.criterion(current_q, target_q)\n",
        "\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon *= self.epsilon_decay\n",
        "\n",
        "        self.update_counter += 1\n",
        "        if self.update_counter % 10 == 0:\n",
        "            self.target_net.load_state_dict(self.policy_net.state_dict())\n"
      ],
      "metadata": {
        "id": "bVN6142zh37D"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Agent DoubleDeepQLearningWithPrioritizedExperienceReplay"
      ],
      "metadata": {
        "id": "be3Feyhlb81m"
      }
    }
  ]
}