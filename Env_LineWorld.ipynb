{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "HNTb5A2iYRCk"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AwW9kuy_Y3xo"
      },
      "source": [
        "Implémentation de environnement LineWorld"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "NQ8hg8HOYsLJ"
      },
      "outputs": [],
      "source": [
        "class LineWorld:\n",
        "    def __init__(self, length=5):\n",
        "        self.length = length\n",
        "        self.start_state = 0\n",
        "        self.end_state = length - 1\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.state = self.start_state\n",
        "        return self.state\n",
        "\n",
        "    def step(self, action):\n",
        "        # 0 = gauche, 1 = droite\n",
        "        if action == 0:\n",
        "            self.state = max(0, self.state - 1)\n",
        "        elif action == 1:\n",
        "            self.state = min(self.length - 1, self.state + 1)\n",
        "\n",
        "        reward = 1 if self.state == self.end_state else 0\n",
        "        done = self.state == self.end_state\n",
        "        return self.state, reward, done, {}\n",
        "\n",
        "    def get_valid_actions(self):\n",
        "        return [0, 1]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y7jLpPEZZTfg"
      },
      "source": [
        "Lancement des Agents test sur l'environnement LinWorld :"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EH4RO15pZcH5"
      },
      "source": [
        "Agent Random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uklAV2j6Zoit",
        "outputId": "e7e05e0d-4de2-4e0c-a20c-23085f682ede"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Random Agent on LineWorld (5 states)\n",
            "Average reward: 1.00\n",
            "Average episode length: 20.18 steps\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "\n",
        "def run_random_agent(env, num_episodes=1000):\n",
        "    total_rewards = []\n",
        "    total_steps = []\n",
        "\n",
        "    for episode in range(num_episodes):\n",
        "        state = env.reset()\n",
        "        done = False\n",
        "        episode_reward = 0\n",
        "        steps = 0\n",
        "\n",
        "        while not done:\n",
        "            action = random.choice(env.get_valid_actions())\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "            episode_reward += reward\n",
        "            steps += 1\n",
        "\n",
        "        total_rewards.append(episode_reward)\n",
        "        total_steps.append(steps)\n",
        "\n",
        "    avg_reward = sum(total_rewards) / num_episodes\n",
        "    avg_steps = sum(total_steps) / num_episodes\n",
        "    print(f\"Random Agent on LineWorld ({env.length} states)\")\n",
        "    print(f\"Average reward: {avg_reward:.2f}\")\n",
        "    print(f\"Average episode length: {avg_steps:.2f} steps\")\n",
        "\n",
        "# Exécution\n",
        "if __name__ == \"__main__\":\n",
        "    env = LineWorld(length=5)\n",
        "    run_random_agent(env)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rYw6iV_Wblqp"
      },
      "source": [
        "Agent TabularQLearning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "id": "cEsWh728eaWQ"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "from collections import defaultdict\n",
        "\n",
        "class TabularQLearningAgent:\n",
        "    def __init__(self, n_states, n_actions, alpha=0.1, gamma=0.99, epsilon=0.1):\n",
        "        self.q_table = defaultdict(lambda: np.zeros(n_actions))\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon\n",
        "        self.n_actions = n_actions\n",
        "\n",
        "    def select_action(self, state):\n",
        "        if random.random() < self.epsilon:\n",
        "            return random.randint(0, self.n_actions - 1)\n",
        "        return np.argmax(self.q_table[state])\n",
        "\n",
        "    def learn(self, state, action, reward, next_state, done):\n",
        "        q_predict = self.q_table[state][action]\n",
        "        q_target = reward if done else reward + self.gamma * np.max(self.q_table[next_state])\n",
        "        self.q_table[state][action] += self.alpha * (q_target - q_predict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZbeV2hXxbswb"
      },
      "source": [
        "Agent DeepQLearning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "yw8-nY2AhaJs"
      },
      "outputs": [],
      "source": [
        "class DQNNetwork(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(input_dim, 64), nn.ReLU(),\n",
        "            nn.Linear(64, 64), nn.ReLU(),\n",
        "            nn.Linear(64, output_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class DQNAgent:\n",
        "    def __init__(self, state_dim, n_actions, gamma=0.8, epsilon=0.99, epsilon_decay=0.995, epsilon_min=0.01, lr=1e-3):\n",
        "        self.state_dim = state_dim\n",
        "        self.n_actions = n_actions\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon\n",
        "        self.epsilon_decay = epsilon_decay\n",
        "        self.epsilon_min = epsilon_min\n",
        "\n",
        "        self.model = DQNNetwork(state_dim, n_actions)\n",
        "        self.optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
        "        self.criterion = nn.MSELoss()\n",
        "\n",
        "    def select_action(self, state):\n",
        "        if random.random() < self.epsilon:\n",
        "            return random.randint(0, self.n_actions - 1)\n",
        "        with torch.no_grad():\n",
        "            # state_tensor = torch.FloatTensor([state])\n",
        "            state_tensor = torch.FloatTensor(state).unsqueeze(1)  \n",
        "            q_values = self.model(state_tensor)\n",
        "            return torch.argmax(q_values).item()\n",
        "\n",
        "    def learn(self, state, action, reward, next_state, done):\n",
        "        state_tensor = torch.FloatTensor(state).unsqueeze(1)\n",
        "        next_state_tensor = torch.FloatTensor(next_state).unsqueeze(1)\n",
        "        reward_tensor = torch.tensor(reward, dtype=torch.float32)\n",
        "        done_tensor = torch.tensor(done, dtype=torch.float32)\n",
        "\n",
        "        q_values = self.model(state_tensor)\n",
        "        next_q_values = self.model(next_state_tensor)\n",
        "\n",
        "        target = reward_tensor if done else reward_tensor + self.gamma * torch.max(next_q_values).detach()\n",
        "\n",
        "        loss = self.criterion(q_values[0][action], target)\n",
        "\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon *= self.epsilon_decay\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_dqn(agent, env, episodes=1000):\n",
        "    rewards_per_episode = []\n",
        "\n",
        "    for episode in range(episodes):\n",
        "        state = env.reset()\n",
        "        total_reward = 0\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            action = agent.select_action([state])\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "            agent.learn([state], action, reward, [next_state], done)\n",
        "            state = next_state\n",
        "            total_reward += reward\n",
        "\n",
        "        rewards_per_episode.append(total_reward)\n",
        "\n",
        "        if (episode + 1) % 50 == 0:\n",
        "            avg_reward = np.mean(rewards_per_episode[-50:])\n",
        "            print(f\"Episode {episode+1}/{episodes} - Moyenne sur 50 derniers épisodes : {avg_reward:.3f}\")\n",
        "\n",
        "    return rewards_per_episode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 50/500 - Moyenne sur 50 derniers épisodes : 1.000\n",
            "Episode 100/500 - Moyenne sur 50 derniers épisodes : 1.000\n",
            "Episode 150/500 - Moyenne sur 50 derniers épisodes : 1.000\n",
            "Episode 200/500 - Moyenne sur 50 derniers épisodes : 1.000\n",
            "Episode 250/500 - Moyenne sur 50 derniers épisodes : 1.000\n",
            "Episode 300/500 - Moyenne sur 50 derniers épisodes : 1.000\n",
            "Episode 350/500 - Moyenne sur 50 derniers épisodes : 1.000\n",
            "Episode 400/500 - Moyenne sur 50 derniers épisodes : 1.000\n",
            "Episode 450/500 - Moyenne sur 50 derniers épisodes : 1.000\n",
            "Episode 500/500 - Moyenne sur 50 derniers épisodes : 1.000\n"
          ]
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA1cAAAHYCAYAAAChuxLUAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjMsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvZiW1igAAAAlwSFlzAAAPYQAAD2EBqD+naQAATxdJREFUeJzt3QmcjeX///HP2MmW7CJbkWyFCkVlS1qob6koSUqlLEVUtjZaFJVQihJZKlrs2UpE1hZRosieyja2Mef/eF+/x33+54wZZsY9zsyZ1/PxON+Zc5/7nPs651zf6X67rutzxwQCgYABAAAAAE5LltN7OgAAAABACFcAAAAA4APCFQAAAAD4gHAFAAAAAD4gXAEAAACADwhXAAAAAOADwhUAAAAA+IBwBQAAAAA+IFwBAAAAgA8IVwAApNIff/xhMTExNmbMGItmCxYscO9TP0/lqquucjcAyIwIVwCQiSgE6CTZu+XKlctKlixpzZo1s9dff93279+f5HO//fZba9WqlRUrVsxy5sxpZcuWtU6dOtmWLVtO2Ld///7u9bVvbGzsCY/ruddff71lhM9q+fLlEW3HddddZ2effbYFAoGw7atWrXLtO++88054zrx589xjb7/99hlsKQCAcAUAmdAzzzxjY8eOteHDh9sjjzzitnXt2tWqVatmP/zwwwn7v/HGG3bllVfajz/+6PZ/66237H//+59NmDDBqlevbt99912ix9m1a5c7RrRSsDl06JDdddddaXaMK664wv777z/76aefTgi72bJls82bN9tff/11wmPecwEAZw7hCgAyoebNm1vbtm2tffv21rt3b5s1a5Z99dVXLgzdeOONLjCEnqgreOlEXcHr6aeftg4dOtgrr7xiK1ascKNYt9xyiwsACdWsWdNefvnlsNdLTw4ePHhaz/dG/7JmzWppxQtIixYtCtuu70WjWnnz5j3hMd0/55xz7MILLzytYx8+fNji4+NP6zUAIDMhXAEAnGuuucb69Oljf/75p3344YfB7c8++6wLEe+//77lyZMn7DkVKlSwl156ybZt25boFLS+ffvazp07Uz16pSl5mrJYuHBhy507t5UrV87uvffeU64FSmwt1D333OOCyO+//+5CSb58+axNmzapaldyjrN161Zr2bKl+71IkSL2+OOP2/Hjx8Oer+AyZMgQu+iii1xI0zTKBx54wP7999/gPpdeeqnlyJEjOBrl0f0GDRq4x0Mf02tqJLFevXqubbJx40a79dZbrVChQu47vPzyy23atGlhr+d9lhqNVIAuVaqU23ffvn1Jvn995+oD+m7Ujm+++eY0Pk0AyPgIVwCAIG962+zZs91PrZeaO3eumxKoYJOY1q1bu9GrL7744oTH9DyFNgWwlI5eaRStadOmLsD06tXLTU1UGEpqCmJyxMXFubBWtGhRN/KmEbe0oBCl42j0SMdp2LChDR48+IQAqiDVo0cPq1+/vg0dOtSNJI4bN84999ixY24fha5atWqFjU5pnZtuClC6hYYrTd1UIPJGvBRutY9GJx966CF7/vnn3YiURiinTJlyQtsVphW8FAZfeOEFF+wS8+6777r2Fy9e3H2/eg96zcTW4AFAZpEt0g0AAKQf5557rhUoUMCN7shvv/3mAkmNGjWSfI6CVaVKlWzt2rWJPt6vXz8XLkaMGGHdunVLdlsWL17sRnAU9GrXrh3c/txzz1lqHTlyxI3gDBw40NKSwotCp0YCRYU/LrnkEhdIHnzwQbdNYWnUqFEuTN15553B51599dV27bXX2uTJk4PbFZQ0vVKjYRpRUpjyQpemY+r9qBiJRuO8EOaFq0GDBrmApVElb1vHjh3dWrnu3bvbTTfdZFmyZAlru0YMNRqVFAW/J5980k37nD9/fjCAValSxe6//34rXbp0GnyqAJD+MXIFAAijaWxe1UDvp07aT0aPJ1VpUFPXFBhSOnpVsGBB9/PLL78MjuL4wQs3aU2BKuEonqbneRSeFGSbNGlif//9d/CmwKTvQKHF44Uib9qdwpX2U6ipW7ducCqg95iClxdIp0+f7qbshRa30OsrBGlUMGEobteu3UmDlSh8aWRR7zF0ZEtTIvWeACCzIlwBAMIcOHAgGKa8nycr0e49rql2SVFp9h07drjRq+TSaJem7Q0YMMCtudIIy+jRo93oU2qpup5G59Kawo3WWYVSOfXQtVQaFdy7d6/73LRv6E3fgcKLR1PutB7Km/6nn9rmhVCNGIU+VqdOnWDo0Ro6jSwm5BW70OOhkpr+Gcp7zvnnnx+2PXv27Fa+fPlTPh8AohXTAgEAQSrprRP+ihUrBk+eFUgSK8/uUdhZv369Gx1JikavdGFZjV4lHNFJisLExx9/7EZktJ5La4ZUzEJrl7RNoy9ewYaEEhaOCJ3CGDoFLq0kp3qgRpsUrDQtMDGh4UxrtypXruym/Cl46fvQdEuP1lTpMX1/Ks1+OoU6TjVqBQBIGiNXAIAgXftKVFBBVC2uUaNG9vXXX58wwuGZNGlScC3TyXijVyNHjkxRm1TZTkUYNBVNQeTnn392Fe280SBJWAY+qbamJ6qyt2fPHjcC1bhx4xNuCde5aVqfilVoDZrCowKVR78vXbo0WDUxdAqgrsWl8JvQunXrgo+nlPccjb6F0vTNTZs2pfj1ACBaEK4AAM68efNcpThNCwsd+VBZ7kAg4NbTJFwzpRPpnj17ugIGp7qQrqb5afTqxRdfdEUTTkVT6HTcUCqgIN7UQJ3ka5RI4S+ULnKc3t12220uJOkzT0hFRBIGRgUm7a/qgxpRDB3ZUrjSiJbet0bmQoOXys4vW7bMlixZEnZ9L1UuLFu2rJtSmFJaz6Xja5rn0aNHg9tVkj6x650BQGbBtEAAyIRmzJjhRi50Eq9KcgpWc+bMcWHl888/d2uGQk/qX3vtNXchYVWYU8gqUaKEe/4777zjTuanTp0aLEBxMprKpuIWyaHraikstGrVyo3yaF2Xjpc/f34XGETFEzRipjLtmiKo/VQAI3S90ul67733bObMmSds79Kly2m9rsKmSpmr0t/q1atd2XmtWdJokIpdqDT7//73v+D+3miUQpK+g1AXXHCBW5emx6pVqxb2XaiM/UcffeQuHP3oo4+6a13ps1Uw/uSTT1I1TVLtVNVGtV+l9lUZUa+nNXGsuQKQmRGuACAT0sV9RUUPdLKtE3JdzFbXWUqsMqBOylVKXKMm2k/T2TSqpDVDa9ascdc6Sg6NXClULFy48JT7aj+NuGgKoAKggpTWdWlqYGjRBQUrTUfTKIrWVGlESGXLq1atan5I6gLICQNOaqjNqvqnqZIqba71bRpNatu2bbBghUehpWTJku6CzaEjUx5tUzAOnRIoujCxyto/8cQT7rPSqKFCstaxtWjRItVtV7VBjaTps9a1utSHdHyv/DwAZEYxgYRzLgAASAZNZ1NIe+qpp07r2lMAAEQLRq4AAKmiEQqNoqjYRJkyZdxIBgAAmRkjVwAAAADgA6oFAgAAAIAPCFcAAAAA4APCFQAAAAD4gHAFAAAAAD6gWmAS4uPjXRUsXe9FF6YEAAAAkDkFAgF3MXtdb/BkF18nXCVBwap06dKRbgYAAACAdGLLli127rnnJvk44SoJGrHyPsD8+fNHtC3Hjh2z2bNnW9OmTS179uwRbQsyBvoMUoo+g5SizyA16DfIqH1m3759buDFywhJIVwlwZsKqGCVHsJVnjx5XDv4Q4TkoM8gpegzSCn6DFKDfoOM3mdOtVyIghYAAAAA4APCFQAAAAD4gHAFAAAAAD5gzRUAAMhU5ZTj4uLs+PHjkW5Kpl0/ky1bNjt8+DDfAdJVn8maNas7zulegolwBQAAMoWjR4/a9u3bLTY2NtJNydThtnjx4q4aM9cRRXrrMyqcUaJECcuRI0eqX4NwBQAAol58fLxt2rTJ/eu0LgKqkydO7iPzPRw4cMDy5s170guxAmeyzyjA6R9fdu/e7f5OnH/++ak+FuEKAABEPZ046SRN16nRv04jMvQd6LvIlSsX4Qrpqs/kzp3blXr/888/g8dLDXo1AADINDihB5CWfx/4CwMAAAAAPiBcAQAAIFPS9K8XXnjBfvnll0g3BVGCcAUAAIA0c9VVV1nXrl0tPXrsscfsxx9/tMqVK6foef3797eaNWtaWlLBlalTp6bpMeA/whUAAEA6ds8997gTbd204L5cuXLWs2dPd90fpN6kSZPs559/tvfffz/FlSMff/xxmzt3bpq1DRkX1QIBAADSuWuvvdZGjx7tLqi6YsUKa9eunQsEL774oqUHKmWtC7zqIqzphT4rhdGk3Hbbbe6WGioLrhuQECNXAAAA6VzOnDndhVRVSr5ly5bWuHFjmzNnTli56oEDB7pRLZWUrlGjhn388cfBx2vXrm2vvPJK8L5eQ8FD1w+Sv/76y4W1DRs2uPtjx451z8mXL5877p133mm7du0KPn/BggVu/xkzZlitWrVc+xYtWmQHDx60u+++2wUPXYx18ODByZ5iN3LkyGCpfIWevXv3Bvf5/vvvrUmTJla4cGErUKCANWzY0FauXBn2OmrP8OHD7cYbb7SzzjrLnn/++USPd+TIETfyVKpUKbffZZdd5t6PZ8yYMVawYEE3JU/XO1JJ7mbNmrmL2CZsc+jncemll7rX03Pr16/vSnp71K4KFSq466tVqlTJfb6hfvvtN2vQoIE7VpUqVcK+W4+Or89Fr1+oUCG76aab7I8//jjl54szi3AFAAAyJY22xB6NO+M3Hfd0/PTTT7Z48WJ3ou5RsPrggw9sxIgRbqpbt27drG3btrZw4UL3uMKIFyB0/G+++cadpCsQifZT2KhYsWJw1OfZZ5+1NWvWuJChk3hNT0yoV69eNmjQIFcQonr16tajRw/3Wp999pnNnj3bHTNhCEqMQp2m6X3xxRc2c+ZMW7VqlT300EPBx/fv3+9G69Te7777zoWe6667zm0PpdDTqlUrt47q3nvvTfRYnTt3tiVLltiECRPshx9+sFtvvdWNDCrgeGJjY10402f67bff2n///We33357oq8XFxfnwqo+Y72eXvv+++8PTjWcMmWKdenSxa3v0nf3wAMPWPv27W3+/PnBYHzzzTe773Pp0qXuO3ziiSfCjqHvQwFPYVffndqkAKt2qygH0o/0M3YLAABwBh06dtyq9J11xo+79plmlidHyk7BvvzyS3cyrRN5jbzoejxvvvmme0z3VfHuq6++srp167pt5cuXd0FEo0E66VdRiXfffddN3dMJvk7kW7du7cKPTtD1U/t5QoOJXuv111+3OnXquJGu0OlwzzzzjBtREj2mY3z44YfWqFEjt03rmc4999xTvj+tH1OQUcCTN954w1q0aOFGvjRyds0114Tt//bbb7twqCB3/fXXB7drhE3BJSmbN2920yv1s2TJkm6bRrEU6LRdn6MXZvT5alTLex8XXnihLVu2zI1Qhdq3b58bZVM7NDol2tejEUMFUy8sdu/e3QVEbb/66qvd97Zu3TqbNWtWsE1qR/PmzYOvMXHiRBfCRo0aFQxtaq8+A313TZs2PeVnjDODkSsAAIB0Tifhq1evdiMbGsFRgLjllluCoz4aaVHI8dYC6aaw8vvvv7t9rrzySjfKoxEhBRIvcHmjWdqm+x6t67rhhhusTJkybrTEC14KJaE0ddCjY2kUxQskoulrmgZ3KjqOF6xEIVFhYv369e7+zp07rWPHjm7EStMC8+fP78LcydqTGI1oKWBecMEFYZ+V3r/3WYnWjilMelRNUEEmsZLteo8KTxpZ0mc2dOhQ2759e/BxPUfTBEPpvvda+qnpkF6w8t5/KI0g6nvWd+G1WcdVKA1tNyKPkSsAAJAp5c6e1Y0iReK4KaW1PN6Uvffee8+tqdIoUYcOHYLrpqZNmxYWUERroUTBQM9RmNK0NQUxrfHR6NWvv/7qpsR5AUrrphQUdBs3bpwVKVLEhRjdTzgFTe06ExQo9+zZ44LLeeed596XAkhK26PPKmvWrC486meo0ylQoVGkRx991I2AaZTp6aefduumLr/8cvOD2q21bfo+EtL3g/SDcAUAADIlTa9K6fS89EBTAp988kk3vUzT4FQAQWFDASh0al9CekzrfDS1TeuJNPKh6Wv6XcUnNJojmqKmIKO1VBpRkeXLl5+yXZoSpyIZGl3TSJT8+++/LrydrF2itm/bti04eqNpc3qf3qiX1hi99dZbbp2VV9zh77//tpS6+OKL3ciVinNoNC8pmn6p9+xNAdQImtZdhU73S+y1devdu7cLfuPHj3fhSs9R+xUQPbqv7030uN6PRrv0PXjvP9Qll1ziQlvRokXdqB3SL6YFAgAAZDAqwqCRl2HDhrmpYlo3pCIWWhukaWIqIqF1S7rv0bQ/revRlDfvornaptGQ0PCjYKQ1WXr+xo0b7fPPP3fFLU5FIz8aSVNRi3nz5rm1XZoup5B0KqqSp/Ch6W8q2KBRIFXG03or0XRAVdjTFDqFtzZt2riqiCmlAKnnqqLhp59+aps2bXJhUwVBNPLnUUh85JFH3LE0yqX3oaCUcL2V6DUUqDQiqAqBKuShkUAviOnzUAVCVQzU9ldffdUdW9+ZqPKj2hX6/p966qmwY6jNqpSoCoF6XMfUKKQ+J1V6RPpBuAIAAMhgFJBU9e6ll15y0/gUfvr06eNCgk7qVaRCYUGl2T0aqdE6ptAgpXClkZzQ9VaaZqYwMHnyZDe6ohGs0DLuJ/Pyyy+742jtkULDFVdc4aaznYqmPKpinkamVJxBlQc1UuXRFEiNgmkE56677nKhQqM4qZ3Cp3Cl6n0aGVOlP5V690bbROXgVbFPI4NaH6XgqJGjxGhfjfZpDZxCkioFPvzww64qoOj1NZ1Rn+FFF13kioyoDd5nrvCpioKHDh1y4e2+++47oYy8jvH111+7Nupz0nesIKs1V4xkpS8xgdOtBxqlVPlFCyZV/SXSnVYVa6ZPn+7+4JzsYniAhz6DlKLPINr7jE5C9a/9ChsaJUFkKNzpHEvnVt6Ilsqnq9y7CnakBwqWXbt2ddMAkT77TCT+TiQ3GzByBQAAAAA+IFwBAAAAgA8IVwAAAIgYTQtML1MCRcUrmBKI1CJcAQAAAIAPCFcAAAAA4APCFQAAAAD4gHAFAAAAAD4gXAEAAACADwhXAAAAAOADwhUAAAAA27lzpyuNv3v37kg3JcMiXAEAACDNXHXVVda1a9dINyNDuuuuu+yFF14I3o+NjbVbbrnF8ufPbzExMb5fj+uhhx6y77//3jp37mzRZsSIEXbDDTek+XEIVwAAAOmYLmqrE2ndsmfPbuXKlbOePXva4cOHI900pKE1a9bY9OnT7dFHHw1ue//99+2bb76xxYsX2/bt261AgQK+HW/ixImWI0cOmzZtmmXJksU+/fTT4GN//PGH63+Rutjz9ddfb1mzZg3+/0C3Tp06he2zefNma9GiheXJk8eKFi1qPXr0sLi4uODj9957r61cudJ9fmkpW5q+OgAAAE7btddea6NHj7Zjx47ZihUrrF27du4E88UXX7T0IBAI2PHjxy1bNk4t/fLGG2/Yrbfeannz5g1u+/333+3CCy+0qlWr+n681q1bu5t89NFHqXqNo0ePuoCWFu677z579tlng/cVojzqewpWxYsXDwbPu+++2/1jhDfyp3bdeeed9vrrr9uVV15paYWRKwAAkDkFAmZxB8/8TcdNoZw5c7oTx9KlS1vLli2tcePGNmfOnODj8fHxNnDgQDeqlTt3bqtRo4Z9/PHHwcdr165tr7zySvC+XkMnngcOHHD3//rrLxfWNmzY4O6PHTvWPSdfvnzuuDop3bVrV/D5CxYscPvPmDHDatWq5dq3aNEiO3jwoDupVSAoUaKEDR48+JTvTWt8atasae+9956VKVPGPVfT03TC/NJLL7njayTi+eefP2Gk4qabbnL7a5rcbbfd5tYMeSMtGn1Zvnx52HOGDBli5513nvu85KeffrLmzZu71yhWrJibhvf333+HTWnUyJFGCgsVKuTaovaG0ucwatQoa9WqlTvhP//88+3zzz8P2+dUx0lI713fX+g0NrVFn+fXX3/tjqn7cuTIEXv88cetVKlSdtZZZ9lll13mvh/PmDFjrGDBgjZr1iwXzNQGhXUFkFB6D3o8V65cVrlyZXvrrbeCj6lfycUXXxx2bI2qqi/puylZsqRVqlTJbd+yZYv7PnRcfW76nvSdnA59tvr8vZu+c8/s2bNt7dq19uGHH7q+pM9aQWzYsGEu8Hn0eeq7OXTokKUVwhUAAMicjseaTcp75m867mnQibr+dT50hEDB6oMPPnDrSn7++Wfr1q2btW3b1hYuXOgeb9iwYfCEW6NMmhqlE18FItF+OjmvWLGiu68RMp2camra1KlT3YmxTqQT6tWrlw0aNMh++eUXq169upuKpdf67LPP3AmvjqmpWKeiERkFtZkzZ7pRk3fffdeNRCj06fU0Qvf000/b0qVL3f4KRzph/+eff9zjCpobN24MjryULVvWBVCN9oXSfb0PBS+tV7rmmmtcYFAI07EVzhQKQmkqnkKLjq2w98wzz4QFWxkwYIB73g8//GDXXXedtWnTxrVNknucUHqdvXv3uoDr0TS9jh07Wt26dV0w8qbtaX3UkiVLbMKECe55Gu1SePrtt9/C1mopXCs0K5wpmCqQecaNG2d9+/Z1IUnfpUZ7+vTp4967LFu2zP386quvwo4tc+fOtfXr17vP5Msvv3R9p1mzZi6Yq599++23wUDnBR0dT9tOdks4fW/8+PFWuHBhN2rXu3dv9548ev/VqlVzwdWjNuzbt8/9/8Gjz1NTBb1+lCYCSNTevXv1z0ruZ6QdPXo0MHXqVPcTSA76DFKKPoNo7zOHDh0KrF271v0MOnYgEBhnZ/6m46ZAu3btAlmzZg2cddZZgZw5c7rzkyxZsgQ+/vhj9/jhw4cDefLkCSxevDjseR06dAjccccd7vfPP/88UKBAgUBcXFxg9erVgeLFiwe6dOkSeOKJJ9zj9913X+DOO+9Msg3ff/+9O+7+/fvd/fnz57v76gMePZYjR47ApEmTgtv27NkTyJ07tzuWHD9+PPDvv/+6n55+/fq59u/bty+4rVmzZoGyZcuG7VepUqXAwIED3e+zZ892n8nmzZuDj//888+uTcuWLXP3J06cGDj77LPd5yMrVqwIxMTEBDZt2uTuP/vss4GmTZuGvc8tW7a411i/fr2737Bhw8AVV1wRtk+dOnWCn5to/6effjp4/8CBA27bjBkzkn2chKZMmeLeX3x8fNh2fY5qk+fPP/90+23dujVsv0aNGgV69+7tfh89erQ71oYNG4KPDxs2LFCsWLHg/QoVKgTGjx8f9hpqd926dd3v+sz0GqtWrTqhb+p1jhw5Etw2duxY912Ftl2Pqx/MmjXL3dd3/dtvv530Fhsb6/ZVH3jttdcC06dPD/zwww+BDz/8MFCqVKlAq1atgq/fsWPHEz7jgwcPujbreaHUJ8aMGZP8vxMpzAZMjAUAAJlT1jxmtx2IzHFT6Oqrr7bhw4e7aXevvfaaW9ukqnGiqXz6V/wmTZqEPUejBBotEa0x2b9/v61atcqNemkkS1O7NOokGv3RqJNH67o0/U0jV//++29wGp1GPKpUqRLcL3RkRaNPOqampXk0JcybKnYyGmnSSIdHIxAqYKARptBt3tREja5oiqRuHrVLo3F6rE6dOm662sMPP2xTpkyx22+/3U2P0+eoY4ne2/z588PWNIW+lwsuuMD9rhG5UJruGDpFMuE+GuXSlDVvn+QeJ5SmrWmqpabgncyPP/7ophAmfA1NFTznnHPCptRVqFAh0fegPqV2dOjQwY2MeTTCk5yCGRoxCh1FXbNmjeuTod+nqACLjiN6LOHjJ6PRRn2m6g86ntrfqFEj93qh7ys5NG02dNTLb4QrAACQOenENdtZlhHohN2bsqe1SVpTpalzOiH21k2pypum9oXSCboodOg5mqanKVQKYg0aNHDT6H799Vc3hUyByzvZ1pQq3TR9q0iRIi5U6X7o+hWvXX7Q+q9QXmXEhNu8kJccOuHX+i9NBbz55pvdtLKhQ4cGH9fnpjU4iRUF0cn7ydqWsB0n2ye5xwml6W8KAKcqEKHXVghVGNbPUKFhLrH2/d+g2/+9hrzzzjthwVgSvmZiEvaBAwcOuHV46jsJqS+JHnvggQdO+rqaJppU4QmvnQpxCldag+VNXfR46+/0WChN1/TakRYIVwAAABmI/vX+ySeftO7du7tCExqxUYhSAPICUmL0mEZQdBKqtTUaVVIBA/2uk3xv9GPdunW2Z88eN6rljQwlLAyRGJ3k6iRe61lUmEI06qXwdrJ2pYbaraIJunltVEEDrW8KHVlThTmt0VFxBo3EKGR5LrnkEvvkk0/cSFZaVjlMzXFUlMF7T97vidHIpEauNAqV2gp4GhFUMQqtWdNascR4AU/HSs77nThxoitCElp0ItSNN954QpBLKOE/FITySsJ74VTr0NSP9TnouKI1YDp+aH/QSJdG0LwR3bRAQQsAAIAMRkULNKqgamiaXqXiBCpioQIEOoFUEQmV8vYKEoimAapinE7wVQ3O26ZRhNDwo2Ckk2k9Xyfcqq4WWgI7KRop0UiaphfOmzfPFd7wikf4TcUqND1MYUDvVYFRo1R6H6FTFRXCLr/8cnviiSfsjjvucFPCPJoyqFEMbdeFc/W56fNp3759skJEcqXmOBpZUUjxCo4kRYFYn4Heu4pMbNq0yX0WKnCikczkUkEOPUdlyhWGNd1QI36vvvqqe1yBRZ+dV4xDxTaS0qZNGzfypoIjKkqhNmnEVFUXVaBE1Gc1Enuym/dd6fN6+eWX3eicCquoP+r9auTVm47ZtGlTF6JUhVHTEvX5qgCKPntv9FbUnvLly6d4KmFKEK4AAAAyGAUkVYlT9TpN41P4UXU3nSArUKgym06uvRLaopENTVULDVIKVzrB90preyf2Wp80efJkd8KqEazQMu4no5NgHUfT4BSArrjiCjdFzG+a1qaKhGeffbY7ydaxdNKsEZOEFPg0vU4XkQ2l0RpVstP718m5wlrXrl3dFEo/A2Fqj6NRt8Sm1iWkEKSw8dhjj7n1bVprphDnjR4mh46lUux6LbVPfUR9wOs/6m8KXiNHjnTvR8EpKXny5HEVCXV8jRSqP+o70IhRUiNZJ6Ogr3CmPq1/FND71HrDL774IriP/qFBlQr1U6NYqpSpz0SVHUOpEmXourK0EKOqFml6hAxKpRu1iE/JPDUdwU8qaakrdKu0Z8I5s0Bi6DNIKfoMor3P6MRO/4Kuk0VdxweRoXCncyyvOMGZoOCpoKgy5RmJilooLCkwKjBkVvE+9RmVZFdJfI3MJVWo42R/J5KbDRi5AgAAQNRRYQVNTXzzzTftkUcesYxG0+J07bKTXWwYyafrc+nzTE4FxNMR8XClYUMNHWuIUUO8ulDdqWhoUPNQNYdSczI1bJkUDWXrdTX8CgAAgMxB0yY1JVFTHhNOCcwo1HadJ+P0aeqoKl6mtYiHK80TVmlQLchMDg3V6Yrduk6BKoUoNGmeqBauJaT5ppobmvD6BAAAAIhu+sd3Xe9J0+qSU1IciIpS7M2bN3e35BoxYoSbBzl48GB3X4vkVElFF9QLTaMaCla1EtXsf+6559Kk7QAAAACQbsJVSunCdxrWC6VQlXDan0ovaoRL+yYnXOlfNnQLXbTmLdjVLZK840e6Hcg46DNIKfoMor3P6BpHquGlim0puRAt/OXVUdNPvgektz6jvw86jv5eJPzblty/dRkuXO3YscNd7CyU7isMqaqKFv9NmDDBXfNA0wKTS6VLVeM/odmzZ7uSkumBLoYGpAR9BilFn0G09hmtv9YFR3W9IV1jB5G1f//+SDcBGcz+M9BndAwtWdJ12hIWVI+NjY3OcHUqulJ3ly5d3B/7lJRa7d27t7vSuUdhTVf81vUI0kMpdr2fJk2aZIhyt4g8+gxSij6DzNBndPFT/fdd5wf6h1MFLpxZOmHVyetZZ53F549002d0DIUnhSv9I0zNmjVP2Meb1RZ14ap48eLuj2Mo3VcA0qiVrt68a9cuV00wdIhPVQlVilNT/xJb1KjKg6FXcPboPxjp5T8a6aktyBjoM0gp+gyiuc+UKlXKnQNQ2jpydBLrzTQiXCG99RldlFpZI7HjJPfvXIYLV7qImi5aGEr/cuZdXK1Ro0b2448/hj3evn17d0XnJ554gmoxAABkUt7UwKJFi2aYtWLRRp+7/sG7QYMGGSaUI3P0mezZs/uSEyIerlTVb8OGDWGl1lVivVChQlamTBk3XW/r1q3uol/SqVMnNwLVs2dPd80CzYmcNGmSTZs2zT2uedRVq1YNO4aGEc8555wTtgMAgMxHJ1D8Y2tk6HNXsQBNzSRcIRr7TMSvc7V8+XK7+OKL3U207km/9+3bN3g15c2bNwf3Vxl2BSmNVun6WCrJPmrUqDNyUTAAAAAASLcjV7rydMJqHAkvAJfYc1atWpXsYyxYsCDV7QMAAACADDFyBQAAAADRgHAFAAAAAD4gXAEAAACADwhXAAAAAOADwhUAAAAA+IBwBQAAAAA+IFwBAAAAgA8IVwAAAADgA8IVAAAAAPiAcAUAAAAAPiBcAQAAAIAPCFcAAAAA4APCFQAAAAD4gHAFAAAAAD4gXAEAAACADwhXAAAAAOADwhUAAAAA+IBwBQAAAAA+IFwBAAAAgA8IVwAAAADgA8IVAAAAAPiAcAUAAAAAPiBcAQAAAIAPCFcAAAAA4APCFQAAAAD4gHAFAAAAAD4gXAEAAACADwhXAAAAAOADwhUAAAAA+IBwBQAAAAA+IFwBAAAAgA8IVwAAAADgA8IVAAAAAPiAcAUAAAAAPiBcAQAAAIAPCFcAAAAA4APCFQAAAAD4gHAFAAAAAD4gXAEAAACADwhXAAAAAOADwhUAAAAA+IBwBQAAAAA+IFwBAAAAgA8IVwAAAADgA8IVAAAAAPiAcAUAAAAAPiBcAQAAAIAPCFcAAAAA4APCFQAAAAD4gHAFAAAAAD4gXAEAAACADwhXAAAAAOADwhUAAAAA+IBwBQAAAAA+IFwBAAAAgA8IVwAAAADgA8IVAAAAAPiAcAUAAAAAPiBcAQAAAEA0hKuvv/7abrjhBitZsqTFxMTY1KlTT/mcBQsW2CWXXGI5c+a0ihUr2pgxY8IeHzhwoNWpU8fy5ctnRYsWtZYtW9r69evT8F0AAAAAyOwiHq4OHjxoNWrUsGHDhiVr/02bNlmLFi3s6quvttWrV1vXrl3tvvvus1mzZgX3WbhwoT388MP23Xff2Zw5c+zYsWPWtGlTdywAAAAASAvZLMKaN2/ubsk1YsQIK1eunA0ePNjdv/DCC23RokX22muvWbNmzdy2mTNnhj1HI1sawVqxYoU1aNDA53cAAAAAAOkgXKXUkiVLrHHjxmHbFKo0gpWUvXv3up+FChVKcp8jR464m2ffvn3up0a9dIsk7/iRbgcyDvoMUoo+g5SizyA16DfIqH0mucfPcOFqx44dVqxYsbBtuq8wdOjQIcudO3fYY/Hx8S541a9f36pWrZrk62qd1oABA07YPnv2bMuTJ4+lB5riCKQEfQYpRZ9BStFnkBr0G2S0PhMbGxud4SqltPbqp59+clMHT6Z3797WvXv34H2FtdKlS7u1Wvnz57dIJ2V1qCZNmlj27Nkj2hZkDPQZpBR9BilFn0Fq0G+QUfuMN6st6sJV8eLFbefOnWHbdF8BKOGoVefOne3LL790FQnPPffck76uKg/qlpC+xPTyf/701BZkDPQZpBR9BilFn0Fq0G+Q0fpMco8d8WqBKVW3bl2bO3du2DalWW33BAIBF6ymTJli8+bNcwUwAAAAACAtRTxcHThwwJVU180rta7fN2/eHJyud/fddwf379Spk23cuNF69uxp69ats7feessmTZpk3bp1C5sK+OGHH9r48ePdta60Tks3rckCAAAAgKgMV8uXL7eLL77Y3UTrnvR737593f3t27cHg5ZoFGratGlutErXx1JJ9lGjRgXLsMvw4cNdhcCrrrrKSpQoEbxNnDgxAu8QAAAAQGYQ8TVXCkCaxpcUXaMqseesWrUqyeec7PUAAAAAICpHrgAAAAAgGhCuAAAAAMAHhCsAAAAA8AHhCgAAAAB8QLgCAAAAAB8QrgAAAADAB4QrAAAAAPAB4QoAAAAAfEC4AgAAAAAfEK4AAAAAwAeEKwAAAADwAeEKAAAAAHxAuAIAAAAAHxCuAAAAAMAHhCsAAAAA8AHhCgAAAAB8QLgCAAAAAB8QrgAAAADAB4QrAAAAAPAB4QoAAAAAfEC4AgAAAAAfEK4AAAAAID2Eq6NHj9r69estLi7Oj/YAAAAAQOYKV7GxsdahQwfLkyePXXTRRbZ582a3/ZFHHrFBgwb52UYAAAAAiN5w1bt3b1uzZo0tWLDAcuXKFdzeuHFjmzhxol/tAwAAAIAMIVtqnzh16lQXoi6//HKLiYkJbtco1u+//+5X+wAAAAAgukeudu/ebUWLFj1h+8GDB8PCFgAAAABkBqkOV7Vr17Zp06YF73uBatSoUVa3bl1/WgcAAAAA0T4t8IUXXrDmzZvb2rVrXaXAoUOHut8XL15sCxcu9LeVAAAAABCtI1dXXHGFrV692gWratWq2ezZs900wSVLllitWrX8bSUAAAAAROvIlVSoUMHeeecd/1oDAAAAAJlt5GrlypX2448/Bu9/9tln1rJlS3vyySfdhYUBAAAAIDNJdbh64IEH7Ndff3W/b9y40Vq3bu0uKDx58mTr2bOnn20EAAAAgOgNVwpWNWvWdL8rUDVs2NDGjx9vY8aMsU8++cTPNgIAAABA9IarQCBg8fHx7vevvvrKrrvuOvd76dKl7e+///avhQAAAAAQ7de5eu6552zs2LGu9HqLFi3c9k2bNlmxYsX8bCMAAAAARG+4GjJkiCtq0blzZ3vqqaesYsWKbvvHH39s9erV87ONAAAAABC9pdirV68eVi3Q8/LLL1vWrFlPt10AAAAAkHmucyUqu75r167g+itPmTJlTvelAQAAACD6w5WqBXbo0MEWL158QqGLmJgYO378uB/tAwAAAIDoDlft27e3bNmy2ZdffmklSpRwgQoAAAAAMqtUh6vVq1fbihUrrHLlyv62CAAAAAAyU7XAKlWqcD0rAAAAADjdcPXiiy9az549bcGCBbZnzx7bt29f2A0AAAAAMpNUTwts3Lix+9moUaOw7RS0AAAAAJAZpTpczZ8/39+WAAAAAEBmDFcNGzb0tyUAAAAAkBnXXMk333xjbdu2tXr16tnWrVvdtrFjx9qiRYv8ah8AAAAARHe4+uSTT6xZs2aWO3duW7lypR05csRt37t3r73wwgt+thEAAAAAojdcPffcczZixAh75513LHv27MHt9evXd2ELAAAAADKTVIer9evXW4MGDU7YXqBAAfvvv/9Ot10AAAAAkDnCVfHixW3Dhg0nbNd6q/Lly59uuwAAAAAgc4Srjh07WpcuXWzp0qXuulbbtm2zcePG2eOPP24PPvigv60EAAAAgGgtxd6rVy+Lj493FxGOjY11UwRz5szpwtUjjzzibysBAAAAIFrDlUarnnrqKevRo4ebHnjgwAGrUqWK5c2b198WAgAAAEA0hytPjhw5LF++fO5GsAIAAACQWaV6zVVcXJz16dPHVQcsW7asu+n3p59+2o4dO+ZvKwEAAAAgWkeutK7q008/tZdeesnq1q3rti1ZssT69+9ve/bsseHDh/vZTgAAAACIznA1fvx4mzBhgjVv3jy4rXr16la6dGm74447CFcAAAAAMpVUTwtUZUBNBUyoXLlybh1Wcn399dd2ww03WMmSJV2RjKlTp57yOQsWLLBLLrnEtaFixYo2ZsyYE/YZNmyYa1+uXLnssssus2XLliW7TQAAAABwxsJV586d7dlnn7UjR44Et+n3559/3j2WXAcPHrQaNWq4MJQcmzZtshYtWtjVV19tq1evtq5du9p9991ns2bNCu4zceJE6969u/Xr189WrlzpXr9Zs2a2a9euFL5LAAAAAEjjaYGrVq2yuXPn2rnnnuvCi6xZs8aOHj3qrn118803B/fV2qykaFph6NTCUxkxYoQbHRs8eLC7f+GFF9qiRYvstddecwFKXn31VXeR4/bt2wefM23aNHvvvffc9bkykkB8vMXG7rW4uMPuZ/bs2SPdJGQAKipDn0FK0GeQUvQZpAb95hSy5tH1jiLdinQlmwUsU4SrggUL2i233BK2Teut0pqKZjRu3Dhsm0KVRrBE4W7FihXWu3fv4ONZsmRxz9Fzk6JRt9BRuH379gX/CESy+qH++BSYVsTcJz0tYs1ABkSfQUrRZ5BS9BmkBv0maRf++LEdCuSKdDPSleW9Grifka5GntzjpzpcjR492iJhx44dVqxYsbBtuq8wdOjQIfv333/t+PHjie6zbt26JF934MCBNmDAgBO2z5492/LkyWORon/dCY+wAAAAQOYwb948y5nVbM6cORFtR2xs7Jm5iHC00EiX1ml5FNY0Ete0aVPLnz9/RKcF/r1/uy1cuNAaNmxo2bPzleHUjh2Lo88gRegzSCn6DFKDfnNy393ItMCEslm8ffXVV9akSZOITiX1ZrWdSqp7ta5l1bdvX5s/f74rFBEfHx/2+D///GNpoXjx4rZz586wbbqvAJQ7d27LmjWruyW2j56bFFUe1C0hfYmRnhNcIMs5li1bLitQ4JyItwUZg4au6TNICfoMUoo+g9Sg3yC10/EifU6e3GOnOlzdddddtmHDBuvQoYObcqcy6meCLlg8ffr0sG0aJvQuZKwy8LVq1XLFNlq2bOm2KfjpfkqqGAIAAABASqQ6XH3zzTeuSp9XKTC1Dhw44EJaaKl1lVgvVKiQlSlTxk3X27p1q33wwQfu8U6dOtmbb75pPXv2tHvvvdfNw5w0aZKrBujR9L527dpZ7dq17dJLL7UhQ4a4ku9e9UAAAAAASDfhqnLlyq6AxOlavny5u2aVx1v3pHCkiwNv377dNm/eHHxcZdgVpLp162ZDhw51peBHjRoVLMMurVu3tt27d7tpiyqAUbNmTZs5c+YJRS4AAAAAIOLh6q233nLXjFKAqVq16gnzEJNbBOKqq66yQCDp+vUKWIk9R9fZOhlNAWQaIAAAAIAMcZ0rVc245pprwrYrKGn9lcqhAwAAAEBmkepw1aZNGzdaNX78+DNa0AIAAAAAoipc/fTTT25qXqVKlfxtEQAAAABkQFlS+0RV4tuyZYu/rQEAAACAzDZy9cgjj1iXLl2sR48eVq1atRMKWlSvXt2P9gEAAABAdIcrlTsXXWvKo3VXFLQAAAAAkBmlOlzpYr8AAAAAgNMMV+edd15qnwoAAAAAUSfV4Up+//13GzJkiP3yyy/ufpUqVdw6rAoVKvjVPgAAAACI7mqBs2bNcmFq2bJlrniFbkuXLrWLLrrI5syZ428rAQAAACBaR6569epl3bp1s0GDBp2w/YknnrAmTZr40T4AAAAAiO6RK00F7NChwwnbVT1w7dq1p9suAAAAAMgc4apIkSK2evXqE7ZrW9GiRU+3XQAAAACQOaYFduzY0e6//37buHGj1atXz2379ttv7cUXX7Tu3bv72UYAAAAAiN5w1adPH8uXL58NHjzYevfu7baVLFnS+vfvb48++qifbQQAAACA6A1XMTExrqCFbvv373fbFLYAAAAAIDNKdbjatGmTxcXF2fnnnx8Wqn777TfLnj27lS1b1q82AgAAAED0FrS45557bPHixSds17Wu9BgAAAAAZCapDlerVq2y+vXrn7D98ssvT7SKIAAAAABEsyyns+bKW2sVau/evXb8+PHTbRcAAAAAZI5w1aBBAxs4cGBYkNLv2nbFFVf41T4AAAAAiO6CFrqelQJWpUqV7Morr3TbvvnmG9u3b5/NmzfPzzYCAAAAQPSOXFWpUsV++OEHu+2222zXrl1uiuDdd99t69ats6pVq/rbSgAAAACI1pEr76LBL7zwgn+tAQAAAIDMNnLlTQNs27at1atXz7Zu3eq2jR071hYtWuRX+wAAAAAgusKVrl917Nix4P1PPvnEmjVrZrlz57aVK1fakSNHgtUCGc0CAAAAkNmkKFw1bdo0WH79ueeesxEjRtg777xj2bNnD+6na18pbAEAAABAZpLsNVePPvqoG7lq2LChC0/r16931QITKlCggP33339+txMAAAAAoqegxWOPPWZ169Z1vxcvXtw2bNhgZcuWDdtH663Kly/vbysBAAAAINoKWqh4hXTs2NG6dOnipgvGxMTYtm3bbNy4cS6APfjgg2nRVgAAAACIvlLsvXr1svj4eGvUqJHFxsa6KYI5c+a0Hj162H333edvKwEAAAAgWkuxa7Tqqaeesn/++cd++ukn++6772z37t1uzVW5cuX8bSUAAAAARFu4Usn13r17W+3atV1lwOnTp1uVKlXs559/tkqVKtnQoUOtW7duadNaAAAAAIiWaYF9+/a1kSNHWuPGjW3x4sV26623Wvv27d3I1eDBg939rFmzpk1rAQAAACBawtXkyZPtgw8+sBtvvNFNB6xevbrFxcXZmjVr3FRBAAAAAMiMUjwt8K+//rJatWq536tWreqKWGgaIMEKAAAAQGaW4nB1/Phxy5EjR/B+tmzZLG/evH63CwAAAACie1pgIBCwe+65x41YyeHDh61Tp0521llnhe336aef+tdKAAAAAIi2cNWuXbuw+23btvWzPQAAAACQOcLV6NGj06YlAAAAAJAZLyIMAAAAAPj/CFcAAAAA4APCFQAAAAD4gHAFAAAAAD4gXAEAAACADwhXAAAAAOADwhUAAAAA+IBwBQAAAAA+IFwBAAAAgA8IVwAAAADgA8IVAAAAAPiAcAUAAAAAPiBcAQAAAIAPCFcAAAAA4APCFQAAAAD4gHAFAAAAAD4gXAEAAACADwhXAAAAAOADwhUAAAAAREu4GjZsmJUtW9Zy5cpll112mS1btizJfY8dO2bPPPOMVahQwe1fo0YNmzlzZtg+x48ftz59+li5cuUsd+7cbt9nn33WAoHAGXg3AAAAADKjiIeriRMnWvfu3a1fv362cuVKF5aaNWtmu3btSnT/p59+2kaOHGlvvPGGrV271jp16mStWrWyVatWBfd58cUXbfjw4fbmm2/aL7/84u6/9NJL7jkAAAAAEJXh6tVXX7WOHTta+/btrUqVKjZixAjLkyePvffee4nuP3bsWHvyySftuuuus/Lly9uDDz7ofh88eHBwn8WLF9tNN91kLVq0cCNi//vf/6xp06YnHREDAAAAgNORzSLo6NGjtmLFCuvdu3dwW5YsWaxx48a2ZMmSRJ9z5MgRNx0wlKb+LVq0KHi/Xr169vbbb9uvv/5qF1xwga1Zs8Y9riCXFL2ubp59+/YFpyHqFkne8SPdDmQc9BmkFH0GKUWfQWrQb5BR+0xyjx/RcPX333+79VHFihUL267769atS/Q5mjKokNSgQQO3lmru3Ln26aefutfx9OrVy4WjypUrW9asWd1jzz//vLVp0ybJtgwcONAGDBhwwvbZs2e7kbT0YM6cOZFuAjIY+gxSij6DlKLPIDXoN8hofSY2Njb9h6vUGDp0qJtGqOAUExPjApamFIZOI5w0aZKNGzfOxo8fbxdddJGtXr3aunbtaiVLlrR27dol+roaPdPaL4/CWenSpd10wvz581ukk7I6VJMmTSx79uwRbQsyBvoMUoo+g5SizyA16DfIqH3Gm9WWrsNV4cKF3cjSzp07w7brfvHixRN9TpEiRWzq1Kl2+PBh27NnjwtMGqnS+itPjx493Lbbb7/d3a9WrZr9+eefbnQqqXCVM2dOd0tIX2J6+T9/emoLMgb6DFKKPoOUos8gNeg3yGh9JrnHjmhBixw5clitWrXc1D5PfHy8u1+3bt2TPlfrrkqVKmVxcXH2ySefuAIWocN2WrsVSiFOrw0AAAAAaSHi0wI1FU+jSbVr17ZLL73UhgwZYgcPHnRT/eTuu+92IUqjTrJ06VLbunWr1axZ0/3s37+/C009e/YMvuYNN9zg1liVKVPGTQtUmXat07r33nsj9j4BAAAARLeIh6vWrVvb7t27rW/fvrZjxw4XmnRRYK/IxebNm8NGoTQdUNe62rhxo+XNm9eVYVd59oIFCwb30fWsdBHhhx56yF0vS1MHH3jgAXcMAAAAAIjKcCWdO3d2t8QsWLAg7H7Dhg3dxYNPJl++fG4ETDcAAAAAyBQXEQYAAACAaEC4AgAAAAAfEK4AAAAAwAeEKwAAAADwAeEKAAAAAHxAuAIAAAAAHxCuAAAAAMAHhCsAAAAA8AHhCgAAAAB8QLgCAAAAAB8QrgAAAADAB4QrAAAAAPAB4QoAAAAAfEC4AgAAAAAfEK4AAAAAwAeEKwAAAADwAeEKAAAAAHxAuAIAAAAAHxCuAAAAAMAHhCsAAAAA8AHhCgAAAAB8QLgCAAAAAB8QrgAAAADAB4QrAAAAAPAB4QoAAAAAfEC4AgAAAAAfEK4AAAAAwAeEKwAAAADwAeEKAAAAAHxAuAIAAAAAHxCuAAAAAMAHhCsAAAAA8AHhCgAAAAB8QLgCAAAAAB8QrgAAAADAB4QrAAAAAPAB4QoAAAAAfEC4AgAAAAAfEK4AAAAAwAeEKwAAAADwAeEKAAAAAHxAuAIAAAAAHxCuAAAAAMAHhCsAAAAA8AHhCgAAAAB8QLgCAAAAAB8QrgAAAADAB4QrAAAAAPAB4QoAAAAAfEC4AgAAAAAfEK4AAAAAwAeEKwAAAADwAeEKAAAAAHxAuAIAAAAAHxCuAAAAAMAHhCsAAAAA8AHhCgAAAAB8QLgCAAAAAB8QrgAAAAAgWsLVsGHDrGzZspYrVy677LLLbNmyZUnue+zYMXvmmWesQoUKbv8aNWrYzJkzT9hv69at1rZtWzvnnHMsd+7cVq1aNVu+fHkavxMAAAAAmVXEw9XEiROte/fu1q9fP1u5cqULS82aNbNdu3Yluv/TTz9tI0eOtDfeeMPWrl1rnTp1slatWtmqVauC+/z7779Wv359y549u82YMcPtN3jwYDv77LPP4DsDAAAAkJlEPFy9+uqr1rFjR2vfvr1VqVLFRowYYXny5LH33nsv0f3Hjh1rTz75pF133XVWvnx5e/DBB93vCk+eF1980UqXLm2jR4+2Sy+91MqVK2dNmzZ1o10AAAAAkBayWQQdPXrUVqxYYb179w5uy5IlizVu3NiWLFmS6HOOHDnipgOG0rS/RYsWBe9//vnnbvTr1ltvtYULF1qpUqXsoYceciEuKXpd3Tz79u0LTkPULZK840e6Hcg46DNIKfoMUoo+g9Sg3yCj9pnkHj8mEAgELEK2bdvmgs/ixYutbt26we09e/Z0oWjp0qUnPOfOO++0NWvW2NSpU91I1Ny5c+2mm26y48ePB8ORF7403VAB6/vvv7cuXbq4UbF27dol2pb+/fvbgAEDTtg+fvx4N5IGAAAAIHOKjY11OWTv3r2WP3/+6AlXu3fvdiNQX3zxhcXExLiApZEuTSM8dOiQ2ydHjhxWu3Zt97qeRx991IWsk42IJRy50tTCv//++6Qf4JlKynPmzLEmTZq4dWTAqdBnkFL0GaQUfQapQb9BRu0zygaFCxc+ZbiK6LRANTBr1qy2c+fOsO26X7x48USfU6RIETdqdfjwYduzZ4+VLFnSevXq5dZfeUqUKOHWb4W68MIL7ZNPPkmyLTlz5nS3hPQlppf/86entiBjoM8gpegzSCn6DFKDfoOM1meSe+yIFrTQCFOtWrXc1D5PfHy8ux86kpUYTf3TqFdcXJwLTZoa6FGlwPXr14ft/+uvv9p5552XBu8CAAAAACI8cuWti9I6KE3jU2W/IUOG2MGDB131QLn77rtdiBo4cKC7r6mCuoZVzZo13U+tlVIg01RCT7du3axevXr2wgsv2G233eaum/X222+7GwAAAABEZbhq3bq1W0fVt29f27FjhwtNuihwsWLF3OObN292FQQ9mg6oa11t3LjR8ubN68qwqzx7wYIFg/vUqVPHpkyZ4qoQ6oLDKsWu0NamTZuIvEcAAAAA0S/i4Uo6d+7sbolZsGBB2P2GDRu6iwKfyvXXX+9uAAAAAJApLiIMAAAAANGAcAUAAAAAPiBcAQAAAIAPCFcAAAAA4APCFQAAAAD4gHAFAAAAAD4gXAEAAACADwhXAAAAAOADwhUAAAAA+IBwBQAAAAA+IFwBAAAAgA8IVwAAAADgA8IVAAAAAPiAcAUAAAAAPiBcAQAAAIAPCFcAAAAA4APCFQAAAAD4gHAFAAAAAD4gXAEAAACADwhXAAAAAOADwhUAAAAA+IBwBQAAAAA+IFwBAAAAgA8IVwAAAADgA8IVAAAAAPiAcAUAAAAAPiBcAQAAAIAPCFcAAAAA4APCFQAAAAD4gHAFAAAAAD4gXAEAAACADwhXAAAAAOADwhUAAAAA+IBwBQAAAAA+IFwBAAAAgA8IVwAAAADgA8IVAAAAAPiAcAUAAAAAPiBcAQAAAIAPCFcAAAAA4APCFQAAAAD4gHAFAAAAAD4gXAEAAACADwhXAAAAAOADwhUAAAAA+CCbHy8SjQKBgPu5b9++SDfFjh07ZrGxsa4t2bNnj3RzkAHQZ5BS9BmkFH0GqUG/QUbtM14m8DJCUghXSdi/f7/7Wbp06Ug3BQAAAEA6yQgFChRI8vGYwKniVyYVHx9v27Zts3z58llMTExE26KkrJC3ZcsWy58/f0TbgoyBPoOUos8gpegzSA36DTJqn1FkUrAqWbKkZcmS9MoqRq6SoA/t3HPPtfREHYo/REgJ+gxSij6DlKLPIDXoN8iIfeZkI1YeCloAAAAAgA8IVwAAAADgA8JVBpAzZ07r16+f+wkkB30GKUWfQUrRZ5Aa9BtEe5+hoAUAAAAA+ICRKwAAAADwAeEKAAAAAHxAuAIAAAAAHxCuAAAAAMAHhKt0btiwYVa2bFnLlSuXXXbZZbZs2bJINwkR8vXXX9sNN9zgrgweExNjU6dODXtctWn69u1rJUqUsNy5c1vjxo3tt99+C9vnn3/+sTZt2riL8BUsWNA6dOhgBw4cOMPvBGfKwIEDrU6dOpYvXz4rWrSotWzZ0tavXx+2z+HDh+3hhx+2c845x/LmzWu33HKL7dy5M2yfzZs3W4sWLSxPnjzudXr06GFxcXFn+N3gTBg+fLhVr149eLHOunXr2owZM4KP019wKoMGDXL/jeratWtwG/0GCfXv39/1k9Bb5cqVo6LPEK7SsYkTJ1r37t1d+cmVK1dajRo1rFmzZrZr165INw0RcPDgQdcHFLgT89JLL9nrr79uI0aMsKVLl9pZZ53l+ov+QHkUrH7++WebM2eOffnlly6w3X///WfwXeBMWrhwofuP03fffee+82PHjlnTpk1dX/J069bNvvjiC5s8ebLbf9u2bXbzzTcHHz9+/Lj7j9fRo0dt8eLF9v7779uYMWNckEf0Offcc93J8YoVK2z58uV2zTXX2E033eT+bgj9BSfz/fff28iRI11AD0W/QWIuuugi2759e/C2aNGi6OgzKsWO9OnSSy8NPPzww8H7x48fD5QsWTIwcODAiLYLkaf/606ZMiV4Pz4+PlC8ePHAyy+/HNz233//BXLmzBn46KOP3P21a9e6533//ffBfWbMmBGIiYkJbN269Qy/A0TCrl27XB9YuHBhsI9kz549MHny5OA+v/zyi9tnyZIl7v706dMDWbJkCezYsSO4z/DhwwP58+cPHDlyJALvAmfa2WefHRg1ahT9BSe1f//+wPnnnx+YM2dOoGHDhoEuXbq47fQbJKZfv36BGjVqJPpYRu8zjFylU0ri+pdDTe3yZMmSxd1fsmRJRNuG9GfTpk22Y8eOsP5SoEABN5XU6y/6qamAtWvXDu6j/dWvNNKF6Ld37173s1ChQu6n/sZoNCu032haRpkyZcL6TbVq1axYsWLBfTQium/fvuBoBqKT/mV4woQJbqRT0wPpLzgZjZJrJCG0fwj9BknR0gUtdShfvrybWaNpftHQZ7JF9OhI0t9//+3+wxbaaUT3161bF7F2IX1SsJLE+ov3mH5qTnKobNmyuRNtbx9Er/j4eLcGon79+la1alW3Td97jhw5XOg+Wb9JrF95jyH6/Pjjjy5MaUqx1jpMmTLFqlSpYqtXr6a/IFEK4Vq+oGmBCfF3BonRP/5qGl+lSpXclMABAwbYlVdeaT/99FOG7zOEKwDIJP+qrP9ohc5pBxKjkx0FKY10fvzxx9auXTu35gFIzJYtW6xLly5uXaeKbwHJ0bx58+DvWqOnsHXeeefZpEmTXFGujIxpgelU4cKFLWvWrCdURtH94sWLR6xdSJ+8PnGy/qKfCYuhqKqOKgjSp6Jb586dXQGT+fPnu4IFHn3vmoL833//nbTfJNavvMcQffQvxhUrVrRatWq5ipMqpDN06FD6CxKlKVz6b8sll1ziZkPopjCuAkv6XaMJ9BucikapLrjgAtuwYUOG/1tDuErH/3HTf9jmzp0bNq1H9zVdAwhVrlw598cktL9o3rHWUnn9RT/1h0r/IfTMmzfP9Sv9ixGij2qfKFhpWpe+a/WTUPobkz179rB+o1Ltmvce2m80TSw0mOtfqFWmW1PFEP30N+LIkSP0FySqUaNG7jvXaKd309peraHxfqff4FR0WZjff//dXU4mw/+tiWg5DZzUhAkTXLW3MWPGuEpv999/f6BgwYJhlVGQuSoxrVq1yt30f91XX33V/f7nn3+6xwcNGuT6x2effRb44YcfAjfddFOgXLlygUOHDgVf49prrw1cfPHFgaVLlwYWLVrkKjvdcccdEXxXSEsPPvhgoECBAoEFCxYEtm/fHrzFxsYG9+nUqVOgTJkygXnz5gWWL18eqFu3rrt54uLiAlWrVg00bdo0sHr16sDMmTMDRYoUCfTu3TtC7wppqVevXq6a5KZNm9zfEd1XRdHZs2e7x+kvSI7QaoFCv0FCjz32mPtvk/7WfPvtt4HGjRsHChcu7KraZvQ+Q7hK59544w3XuXLkyOFKs3/33XeRbhIiZP78+S5UJby1a9cuWI69T58+gWLFirlQ3qhRo8D69evDXmPPnj0uTOXNm9eVK23fvr0LbYhOifUX3UaPHh3cR+H7oYcecuW28+TJE2jVqpULYKH++OOPQPPmzQO5c+d2//HTfxSPHTsWgXeEtHbvvfcGzjvvPPffHJ2o6O+IF6yE/oLUhCv6DRJq3bp1oESJEu5vTalSpdz9DRs2REWfidH/RHbsDAAAAAAyPtZcAQAAAIAPCFcAAAAA4APCFQAAAAD4gHAFAAAAAD4gXAEAAACADwhXAAAAAOADwhUAAAAA+IBwBQAAAAA+IFwBADKkLl262P3332/x8fGRbgoAAA7hCgCQ4WzZssUqVapkI0eOtCxZ+E8ZACB9iAkEAoFINwIAgDPhjz/+sHLlytmqVausZs2aaXKMe+65x/777z+bOnVqmrw+ACD94p/7AAAZhoJLTEzMCbdrr702Wc8vXbq0bd++3apWrZrmbQUAZD7ZIt0AAABSQkFq9OjRYdty5syZrOdmzZrVihcvnkYtAwBkdoxcAQAyFAUpBaTQ29lnn+0e0yjW8OHDrXnz5pY7d24rX768ffzxx2HTArXP6tWr3f1///3X2rRpY0WKFHH7n3/++WHB7ccff7RrrrnGPXbOOee4AhoHDhwIPn78+HHr3r27FSxY0D3es2dPSzjbXgU3Bg4c6KYj6nVq1KgR1qZTtQEAkHEQrgAAUaVPnz52yy232Jo1a1xouf322+2XX35Jct+1a9fajBkz3D4KZoULF3aPHTx40Jo1a+aC2/fff2+TJ0+2r776yjp37hx8/uDBg23MmDH23nvv2aJFi+yff/6xKVOmhB1DweqDDz6wESNG2M8//2zdunWztm3b2sKFC0/ZBgBAxkJBCwBAhlpz9eGHH1quXLnCtj/55JPuplGpTp06uYDiufzyy+2SSy6xt95664SCFjfeeKMLMgpHCb3zzjv2xBNPuMqEZ511lts2ffp0u+GGG2zbtm1WrFgxK1mypAtLPXr0cI/HxcW5169Vq5YraHHkyBErVKiQC2V169YNvvZ9991nsbGxNn78+JO2AQCQsbDmCgCQoVx99dVh4UkUYDyhIca7700DTOjBBx90o1wrV660pk2bWsuWLa1evXruMY0iaQqfF6ykfv36bprf+vXrXcBTcYzLLrss+Hi2bNmsdu3awamBGzZscCGqSZMmYcc9evSoXXzxxadsAwAgYyFcAQAyFIWdihUr+vJaWpv1559/uhGpOXPmWKNGjezhhx+2V155xZfX99ZnTZs2zUqVKpVoEY60bgMA4MxhzRUAIKp89913J9y/8MILk9xfhSTatWvnphsOGTLE3n77bbddz9G6La298nz77bfuosW6gHGBAgWsRIkStnTp0uDjmha4YsWK4P0qVaq4ELV582YXCENvKgt/qjYAADIWRq4AABmK1jHt2LEjbJum43lFIFR4QlPzrrjiChs3bpwtW7bM3n333URfq2/fvm591EUXXeRe98svvwwGMRXD6Nevnws9/fv3t927d9sjjzxid911l1tvJV26dLFBgwa5Cn+VK1e2V1991V1A2JMvXz57/PHH3bosTSdUm/bu3etCWv78+d1rn6wNAICMhXAFAMhQZs6c6UaMQmkkad26de73AQMG2IQJE+yhhx5y+3300UduBCkxOXLksN69e7tCFyqDfuWVV7rnSp48eWzWrFkuQNWpU8fd19ooBSjPY4895tZdKSRpROvee++1Vq1auQDlefbZZ93IlKoGbty40ZVtV4ENFeA4VRsAABkL1QIBAFFD1QJVCl1FIQAAONNYcwUAAAAAPiBcAQAAAIAPWHMFAIgazHQHAEQSI1cAAAAA4APCFQAAAAD4gHAFAAAAAD4gXAEAAACADwhXAAAAAOADwhUAAAAA+IBwBQAAAAA+IFwBAAAAgJ2+/wc1DwGNIcKH/QAAAABJRU5ErkJggg==",
            "text/plain": [
              "<Figure size 1000x500 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "def plot_rewards(rewards, title=\"Récompense par épisode\"):\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.plot(rewards, label='Reward par épisode')\n",
        "    plt.plot(\n",
        "        np.convolve(rewards, np.ones(50)/50, mode='valid'),\n",
        "        label='Reward moyenne (fenêtre=50)', color='orange'\n",
        "    )\n",
        "    plt.xlabel('Épisodes')\n",
        "    plt.ylabel('Récompense')\n",
        "    plt.title(title)\n",
        "    plt.legend()\n",
        "    plt.grid()\n",
        "    plt.show()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    env = LineWorld(length=5)\n",
        "    agent = DQNAgent(state_dim=1, n_actions=2)\n",
        "    rewards = train_dqn(agent, env, episodes=500)\n",
        "    plot_rewards(rewards, title=\"DQN sur LineWorld\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qOWhQUP-bw7C"
      },
      "source": [
        "Agent DoubleDeepQLearning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "id": "jdSBSFXWhiBL"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "class DoubleDQNNetwork(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(input_dim, 64), nn.ReLU(),\n",
        "            nn.Linear(64, 64), nn.ReLU(),\n",
        "            nn.Linear(64, output_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class DoubleDQNAgent:\n",
        "    def __init__(self, state_dim, n_actions, gamma=0.99, epsilon=1.0, epsilon_decay=0.995, epsilon_min=0.01, lr=1e-3):\n",
        "        self.state_dim = state_dim\n",
        "        self.n_actions = n_actions\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon\n",
        "        self.epsilon_decay = epsilon_decay\n",
        "        self.epsilon_min = epsilon_min\n",
        "\n",
        "        self.policy_net = DoubleDQNNetwork(state_dim, n_actions)\n",
        "        self.target_net = DoubleDQNNetwork(state_dim, n_actions)\n",
        "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
        "        self.target_net.eval()\n",
        "\n",
        "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=lr)\n",
        "        self.criterion = nn.MSELoss()\n",
        "        self.update_counter = 0\n",
        "\n",
        "    def select_action(self, state):\n",
        "        if random.random() < self.epsilon:\n",
        "            return random.randint(0, self.n_actions - 1)\n",
        "        with torch.no_grad():\n",
        "            state_tensor = torch.FloatTensor([state])\n",
        "            q_values = self.policy_net(state_tensor)\n",
        "            return torch.argmax(q_values).item()\n",
        "\n",
        "    def learn(self, state, action, reward, next_state, done):\n",
        "        state_tensor = torch.FloatTensor([state])\n",
        "        next_state_tensor = torch.FloatTensor([next_state])\n",
        "        reward_tensor = torch.tensor(reward)\n",
        "        done_tensor = torch.tensor(done, dtype=torch.float32)\n",
        "\n",
        "        current_q = self.policy_net(state_tensor)[0][action]\n",
        "\n",
        "        # Double DQN trick\n",
        "        with torch.no_grad():\n",
        "            next_action = torch.argmax(self.policy_net(next_state_tensor), dim=1)\n",
        "            next_q = self.target_net(next_state_tensor)[0][next_action]\n",
        "            target_q = reward_tensor if done else reward_tensor + self.gamma * next_q\n",
        "\n",
        "        loss = self.criterion(current_q, target_q)\n",
        "\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon *= self.epsilon_decay\n",
        "\n",
        "        self.update_counter += 1\n",
        "        if self.update_counter % 10 == 0:\n",
        "            self.target_net.load_state_dict(self.policy_net.state_dict())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-0RlY1Xvb4ET"
      },
      "source": [
        "Agent DoubleDeepQLearningWithExperienceReplay :"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pLzMxTieh_hj"
      },
      "source": [
        "Buffer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "id": "iMAQu0q-h3D1"
      },
      "outputs": [],
      "source": [
        "from collections import deque\n",
        "\n",
        "class ReplayBuffer:\n",
        "    def __init__(self, capacity=10000):\n",
        "        self.buffer = deque(maxlen=capacity)\n",
        "\n",
        "    def add(self, state, action, reward, next_state, done):\n",
        "        self.buffer.append((state, action, reward, next_state, done))\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        return random.sample(self.buffer, batch_size)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.buffer)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PpesRJesiBW4"
      },
      "source": [
        "DDQL ER"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "id": "bVN6142zh37D"
      },
      "outputs": [],
      "source": [
        "class DoubleDQNWithReplayAgent:\n",
        "    def __init__(self, state_dim, n_actions, gamma=0.99, epsilon=1.0, epsilon_decay=0.995, epsilon_min=0.01, lr=1e-3, buffer_size=10000, batch_size=64):\n",
        "        self.state_dim = state_dim\n",
        "        self.n_actions = n_actions\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon\n",
        "        self.epsilon_decay = epsilon_decay\n",
        "        self.epsilon_min = epsilon_min\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        self.policy_net = DoubleDQNNetwork(state_dim, n_actions)\n",
        "        self.target_net = DoubleDQNNetwork(state_dim, n_actions)\n",
        "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
        "        self.target_net.eval()\n",
        "\n",
        "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=lr)\n",
        "        self.criterion = nn.MSELoss()\n",
        "\n",
        "        self.memory = ReplayBuffer(capacity=buffer_size)\n",
        "        self.update_counter = 0\n",
        "\n",
        "    def select_action(self, state):\n",
        "        if random.random() < self.epsilon:\n",
        "            return random.randint(0, self.n_actions - 1)\n",
        "        with torch.no_grad():\n",
        "            state_tensor = torch.FloatTensor([state])\n",
        "            q_values = self.policy_net(state_tensor)\n",
        "            return torch.argmax(q_values).item()\n",
        "\n",
        "    def store_transition(self, state, action, reward, next_state, done):\n",
        "        self.memory.add(state, action, reward, next_state, done)\n",
        "\n",
        "    def learn(self):\n",
        "        if len(self.memory) < self.batch_size:\n",
        "            return\n",
        "\n",
        "        batch = self.memory.sample(self.batch_size)\n",
        "        states, actions, rewards, next_states, dones = zip(*batch)\n",
        "\n",
        "        states = torch.FloatTensor(states)\n",
        "        actions = torch.LongTensor(actions)\n",
        "        rewards = torch.FloatTensor(rewards)\n",
        "        next_states = torch.FloatTensor(next_states)\n",
        "        dones = torch.FloatTensor(dones)\n",
        "\n",
        "        current_q = self.policy_net(states).gather(1, actions.unsqueeze(1)).squeeze()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            next_actions = torch.argmax(self.policy_net(next_states), dim=1)\n",
        "            next_q = self.target_net(next_states).gather(1, next_actions.unsqueeze(1)).squeeze()\n",
        "            target_q = rewards + self.gamma * next_q * (1 - dones)\n",
        "\n",
        "        loss = self.criterion(current_q, target_q)\n",
        "\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon *= self.epsilon_decay\n",
        "\n",
        "        self.update_counter += 1\n",
        "        if self.update_counter % 10 == 0:\n",
        "            self.target_net.load_state_dict(self.policy_net.state_dict())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "be3Feyhlb81m"
      },
      "source": [
        "Agent DoubleDeepQLearningWithPrioritizedExperienceReplay"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Buffer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {},
      "outputs": [],
      "source": [
        "class PrioritizedReplayBuffer:\n",
        "    def __init__(self, capacity=10000, alpha=0.6):\n",
        "        self.capacity = capacity\n",
        "        self.buffer = []\n",
        "        self.priorities = []\n",
        "        self.alpha = alpha\n",
        "        self.pos = 0\n",
        "\n",
        "    def add(self, state, action, reward, next_state, done, td_error=1.0):\n",
        "        priority = (abs(td_error) + 1e-5) ** self.alpha\n",
        "        if len(self.buffer) < self.capacity:\n",
        "            self.buffer.append((state, action, reward, next_state, done))\n",
        "            self.priorities.append(priority)\n",
        "        else:\n",
        "            self.buffer[self.pos] = (state, action, reward, next_state, done)\n",
        "            self.priorities[self.pos] = priority\n",
        "            self.pos = (self.pos + 1) % self.capacity\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        probs = np.array(self.priorities) / sum(self.priorities)\n",
        "        indices = np.random.choice(len(self.buffer), batch_size, p=probs)\n",
        "        samples = [self.buffer[i] for i in indices]\n",
        "        return samples\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.buffer)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "DDQLPER"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {},
      "outputs": [],
      "source": [
        "class DoubleDQNWithPrioritizedReplayAgent:\n",
        "    def __init__(self, state_dim, n_actions, gamma=0.99, epsilon=1.0, epsilon_decay=0.995, epsilon_min=0.01, lr=1e-3, buffer_size=10000, batch_size=64):\n",
        "        self.state_dim = state_dim\n",
        "        self.n_actions = n_actions\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon\n",
        "        self.epsilon_decay = epsilon_decay\n",
        "        self.epsilon_min = epsilon_min\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        self.policy_net = DoubleDQNNetwork(state_dim, n_actions)\n",
        "        self.target_net = DoubleDQNNetwork(state_dim, n_actions)\n",
        "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
        "        self.target_net.eval()\n",
        "\n",
        "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=lr)\n",
        "        self.criterion = nn.MSELoss()\n",
        "\n",
        "        self.memory = PrioritizedReplayBuffer(capacity=buffer_size)\n",
        "        self.update_counter = 0\n",
        "\n",
        "    def select_action(self, state):\n",
        "        if random.random() < self.epsilon:\n",
        "            return random.randint(0, self.n_actions - 1)\n",
        "        with torch.no_grad():\n",
        "            state_tensor = torch.FloatTensor([state])\n",
        "            q_values = self.policy_net(state_tensor)\n",
        "            return torch.argmax(q_values).item()\n",
        "\n",
        "    def store_transition(self, state, action, reward, next_state, done):\n",
        "        with torch.no_grad():\n",
        "            s_tensor = torch.FloatTensor([state])\n",
        "            ns_tensor = torch.FloatTensor([next_state])\n",
        "            q_val = self.policy_net(s_tensor)[0][action]\n",
        "            next_action = torch.argmax(self.policy_net(ns_tensor)).item()\n",
        "            next_q = self.target_net(ns_tensor)[0][next_action]\n",
        "            target = reward + self.gamma * next_q * (1 - int(done))\n",
        "            td_error = abs(q_val.item() - target.item())\n",
        "\n",
        "        self.memory.add(state, action, reward, next_state, done, td_error)\n",
        "\n",
        "    def learn(self):\n",
        "        if len(self.memory) < self.batch_size:\n",
        "            return\n",
        "\n",
        "        batch = self.memory.sample(self.batch_size)\n",
        "        states, actions, rewards, next_states, dones = zip(*batch)\n",
        "\n",
        "        states = torch.FloatTensor(states)\n",
        "        actions = torch.LongTensor(actions)\n",
        "        rewards = torch.FloatTensor(rewards)\n",
        "        next_states = torch.FloatTensor(next_states)\n",
        "        dones = torch.FloatTensor(dones)\n",
        "\n",
        "        q_values = self.policy_net(states).gather(1, actions.unsqueeze(1)).squeeze()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            next_actions = torch.argmax(self.policy_net(next_states), dim=1)\n",
        "            next_q = self.target_net(next_states).gather(1, next_actions.unsqueeze(1)).squeeze()\n",
        "            targets = rewards + self.gamma * next_q * (1 - dones)\n",
        "\n",
        "        loss = self.criterion(q_values, targets)\n",
        "\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon *= self.epsilon_decay\n",
        "\n",
        "        self.update_counter += 1\n",
        "        if self.update_counter % 10 == 0:\n",
        "            self.target_net.load_state_dict(self.policy_net.state_dict())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "REINFORCE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {},
      "outputs": [],
      "source": [
        "class PolicyNetwork(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(state_dim, 32),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(32, action_dim),\n",
        "            nn.Softmax(dim=-1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "    \n",
        "class REINFORCEAgent:\n",
        "    def __init__(self, state_dim, action_dim, gamma=0.99, lr=1e-2):\n",
        "        self.policy = PolicyNetwork(state_dim, action_dim)\n",
        "        self.optimizer = optim.Adam(self.policy.parameters(), lr=lr)\n",
        "        self.gamma = gamma\n",
        "        self.trajectory = []\n",
        "\n",
        "    def select_action(self, state):\n",
        "        state_tensor = torch.FloatTensor([state])\n",
        "        probs = self.policy(state_tensor)\n",
        "        action_dist = torch.distributions.Categorical(probs)\n",
        "        action = action_dist.sample()\n",
        "        self.trajectory.append((state_tensor, action, action_dist.log_prob(action)))\n",
        "        return action.item()\n",
        "\n",
        "    def update_policy(self, rewards):\n",
        "        returns = []\n",
        "        G = 0\n",
        "        for r in reversed(rewards):\n",
        "            G = r + self.gamma * G\n",
        "            returns.insert(0, G)\n",
        "        returns = torch.tensor(returns)\n",
        "\n",
        "        loss = 0\n",
        "        for (_, _, log_prob), G in zip(self.trajectory, returns):\n",
        "            loss -= log_prob * G  # gradient ascent\n",
        "\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "        self.trajectory = []\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "REINFORCE with mean baseline\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {},
      "outputs": [],
      "source": [
        "class REINFORCEWithBaselineAgent:\n",
        "    def __init__(self, state_dim, action_dim, gamma=0.99, lr=1e-2):\n",
        "        self.policy = PolicyNetwork(state_dim, action_dim)\n",
        "        self.optimizer = optim.Adam(self.policy.parameters(), lr=lr)\n",
        "        self.gamma = gamma\n",
        "        self.trajectory = []\n",
        "        self.returns = []\n",
        "\n",
        "    def select_action(self, state):\n",
        "        state_tensor = torch.FloatTensor([state])\n",
        "        probs = self.policy(state_tensor)\n",
        "        dist = torch.distributions.Categorical(probs)\n",
        "        action = dist.sample()\n",
        "        self.trajectory.append((state_tensor, action, dist.log_prob(action)))\n",
        "        return action.item()\n",
        "\n",
        "    def update_policy(self, rewards):\n",
        "        returns = []\n",
        "        G = 0\n",
        "        for r in reversed(rewards):\n",
        "            G = r + self.gamma * G\n",
        "            returns.insert(0, G)\n",
        "\n",
        "        self.returns.extend(returns)\n",
        "        baseline = np.mean(self.returns)\n",
        "\n",
        "        loss = 0\n",
        "        for (_, _, log_prob), G in zip(self.trajectory, returns):\n",
        "            loss -= log_prob * (G - baseline)\n",
        "\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "        self.trajectory = []\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "REINFORCE with Baseline Learned by a Critic\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ValueNetwork(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super().__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(input_dim, 32),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(32, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "class REINFORCEWithCriticAgent:\n",
        "    def __init__(self, state_dim, action_dim, gamma=0.99, lr=1e-2):\n",
        "        self.policy = PolicyNetwork(state_dim, action_dim)\n",
        "        self.value = ValueNetwork(state_dim)\n",
        "        self.policy_opt = optim.Adam(self.policy.parameters(), lr=lr)\n",
        "        self.value_opt = optim.Adam(self.value.parameters(), lr=lr)\n",
        "        self.gamma = gamma\n",
        "        self.trajectory = []\n",
        "\n",
        "    def select_action(self, state):\n",
        "        state_tensor = torch.FloatTensor([state])\n",
        "        probs = self.policy(state_tensor)\n",
        "        dist = torch.distributions.Categorical(probs)\n",
        "        action = dist.sample()\n",
        "        self.trajectory.append((state_tensor, action, dist.log_prob(action)))\n",
        "        return action.item()\n",
        "\n",
        "    def update_policy(self, rewards):\n",
        "        returns = []\n",
        "        G = 0\n",
        "        for r in reversed(rewards):\n",
        "            G = r + self.gamma * G\n",
        "            returns.insert(0, G)\n",
        "\n",
        "        loss_policy = 0\n",
        "        loss_value = 0\n",
        "        for (state, _, log_prob), G in zip(self.trajectory, returns):\n",
        "            baseline = self.value(state).squeeze()\n",
        "            advantage = G - baseline\n",
        "            loss_policy -= log_prob * advantage.detach()\n",
        "            loss_value += (baseline - G) ** 2\n",
        "\n",
        "        self.policy_opt.zero_grad()\n",
        "        loss_policy.backward()\n",
        "        self.policy_opt.step()\n",
        "\n",
        "        self.value_opt.zero_grad()\n",
        "        loss_value.backward()\n",
        "        self.value_opt.step()\n",
        "\n",
        "        self.trajectory = []\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "PPO A2C style\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {},
      "outputs": [],
      "source": [
        "class A2CPolicyNetwork(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim):\n",
        "        super().__init__()\n",
        "        self.fc = nn.Linear(state_dim, 32)\n",
        "        self.action_head = nn.Linear(32, action_dim)\n",
        "        self.value_head = nn.Linear(32, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc(x))\n",
        "        return torch.softmax(self.action_head(x), dim=-1), self.value_head(x)\n",
        "\n",
        "class PPOA2CAgent:\n",
        "    def __init__(self, state_dim, action_dim, gamma=0.99, lr=1e-2):\n",
        "        self.model = A2CPolicyNetwork(state_dim, action_dim)\n",
        "        self.optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
        "        self.gamma = gamma\n",
        "        self.trajectory = []\n",
        "\n",
        "    def select_action(self, state):\n",
        "        state_tensor = torch.FloatTensor([state])\n",
        "        probs, _ = self.model(state_tensor)\n",
        "        dist = torch.distributions.Categorical(probs)\n",
        "        action = dist.sample()\n",
        "        self.trajectory.append((state_tensor, action, dist.log_prob(action)))\n",
        "        return action.item()\n",
        "\n",
        "    def update_policy(self, rewards):\n",
        "        returns = []\n",
        "        G = 0\n",
        "        for r in reversed(rewards):\n",
        "            G = r + self.gamma * G\n",
        "            returns.insert(0, G)\n",
        "        returns = torch.FloatTensor(returns)\n",
        "\n",
        "        loss = 0\n",
        "        for (state, _, log_prob), G in zip(self.trajectory, returns):\n",
        "            _, value = self.model(state)\n",
        "            advantage = G - value.item()\n",
        "            loss -= log_prob * advantage\n",
        "\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "        self.trajectory = []\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "RandomRollout"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {},
      "outputs": [],
      "source": [
        "class RandomRolloutAgent:\n",
        "    def __init__(self, n_actions):\n",
        "        self.n_actions = n_actions\n",
        "\n",
        "    def select_action(self, state):\n",
        "        return random.randint(0, self.n_actions - 1)\n",
        "\n",
        "    def update_policy(self, rewards):\n",
        "        pass  # sans training\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Monte Carlo Tree Search (UCT)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MCTSNode:\n",
        "    def __init__(self, state, parent=None):\n",
        "        self.state = state\n",
        "        self.parent = parent\n",
        "        self.children = {}\n",
        "        self.visits = 0\n",
        "        self.value = 0.0\n",
        "\n",
        "class MCTSAgent:\n",
        "    def __init__(self, env, n_actions, simulations=100, c=1.4):\n",
        "        self.env = env\n",
        "        self.n_actions = n_actions\n",
        "        self.simulations = simulations\n",
        "        self.c = c\n",
        "\n",
        "    def select_action(self, state):\n",
        "        root = MCTSNode(state)\n",
        "        for _ in range(self.simulations):\n",
        "            self.simulate(root)\n",
        "        return max(root.children.items(), key=lambda item: item[1].visits)[0]\n",
        "\n",
        "    def simulate(self, node):\n",
        "        env_copy = LineWorld(length=self.env.length)\n",
        "        env_copy.state = int(node.state)\n",
        "        path = []\n",
        "        while True:\n",
        "            if node.children == {}:\n",
        "                for a in range(self.n_actions):\n",
        "                    env_copy.state = int(node.state)\n",
        "                    s_, r, d, _ = env_copy.step(a)\n",
        "                    node.children[a] = MCTSNode(s_, node)\n",
        "                break\n",
        "            action, node = max(\n",
        "                node.children.items(),\n",
        "                key=lambda item: item[1].value / (1 + item[1].visits) +\n",
        "                self.c * np.sqrt(np.log(node.visits + 1) / (item[1].visits + 1e-5))\n",
        "            )\n",
        "            path.append(node)\n",
        "            env_copy.state = int(node.state)\n",
        "            _, r, done, _ = env_copy.step(action)\n",
        "            if done:\n",
        "                break\n",
        "        for n in path:\n",
        "            n.visits += 1\n",
        "            n.value += r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Expert Apprentice"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ExpertApprenticeAgent:\n",
        "    def __init__(self, env):\n",
        "        self.env = env\n",
        "\n",
        "    def select_action(self, state):\n",
        "        return 1 if state < self.env.end_state else 0  # Va toujours à droite\n",
        "\n",
        "    def update_policy(self, rewards):\n",
        "        pass  # sans training"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".conda",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
