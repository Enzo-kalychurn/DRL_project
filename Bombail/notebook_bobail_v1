{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "ENVIRONNEMENT"
      ],
      "metadata": {
        "id": "4wws7l--uk63"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "7X9LLyFlt_S-"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Codes couleur ANSI\n",
        "GREEN = \"\\033[92m\"   # Vert pour le joueur 1\n",
        "RED = \"\\033[91m\"     # Rouge pour le joueur 2\n",
        "YELLOW = \"\\033[93m\"  # Jaune pour le Bobail\n",
        "RESET = \"\\033[0m\"    # Réinitialisation des couleurs\n",
        "\n",
        "class BobailEnv:\n",
        "    def __init__(self):\n",
        "        self.board_size = 5\n",
        "        self.directions = [(-1, -1), (-1, 0), (-1, 1),\n",
        "                           (0, -1),           (0, 1),\n",
        "                           (1, -1),  (1, 0),  (1, 1)]\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        # Création d'un plateau vide\n",
        "        self.board = np.zeros((self.board_size, self.board_size), dtype=int)\n",
        "\n",
        "        # Placement des pions\n",
        "        # Joueur 1 sur la première ligne (ligne 0)\n",
        "        for j in range(self.board_size):\n",
        "            self.board[0, j] = 1\n",
        "        # Joueur 2 sur la dernière ligne (ligne 4)\n",
        "        for j in range(self.board_size):\n",
        "            self.board[self.board_size - 1, j] = 2\n",
        "\n",
        "        # Placement du Bobail au centre\n",
        "        self.bobail_pos = (self.board_size // 2, self.board_size // 2)\n",
        "        self.board[self.bobail_pos] = 3  # 3 représente le Bobail\n",
        "\n",
        "        # Sauvegarde des positions initiales pour vérifier l'objectif\n",
        "        self.initial_positions = {\n",
        "            1: {(0, j) for j in range(self.board_size)},\n",
        "            2: {(self.board_size - 1, j) for j in range(self.board_size)}\n",
        "        }\n",
        "\n",
        "        # Variables de gestion de partie\n",
        "        self.current_player = 1  # Le joueur 1 commence\n",
        "        self.first_turn = True   # Premier tour : seul un pion est déplacé\n",
        "        self.done = False\n",
        "        self.winner = None\n",
        "        return self.board.copy()\n",
        "\n",
        "    def in_bounds(self, x, y):\n",
        "        return 0 <= x < self.board_size and 0 <= y < self.board_size\n",
        "\n",
        "    def get_valid_bobail_moves(self):\n",
        "        \"\"\"Retourne la liste des directions valides pour déplacer le Bobail.\"\"\"\n",
        "        valid_moves = []\n",
        "        x, y = self.bobail_pos\n",
        "        for d in self.directions:\n",
        "            new_x, new_y = x + d[0], y + d[1]\n",
        "            if self.in_bounds(new_x, new_y) and self.board[new_x, new_y] == 0:\n",
        "                valid_moves.append(d)\n",
        "        return valid_moves\n",
        "\n",
        "    def move_bobail(self, direction):\n",
        "        \"\"\"Déplace le Bobail d'une case dans la direction donnée.\"\"\"\n",
        "        if self.first_turn:\n",
        "            raise ValueError(\"Le Bobail ne doit pas être déplacé lors du premier tour.\")\n",
        "        x, y = self.bobail_pos\n",
        "        new_x, new_y = x + direction[0], y + direction[1]\n",
        "        if not self.in_bounds(new_x, new_y) or self.board[new_x, new_y] != 0:\n",
        "            raise ValueError(\"Mouvement du Bobail invalide.\")\n",
        "        # Mise à jour du plateau\n",
        "        self.board[x, y] = 0\n",
        "        self.bobail_pos = (new_x, new_y)\n",
        "        self.board[new_x, new_y] = 3\n",
        "\n",
        "        # Condition de victoire : le Bobail arrive sur une case initiale du joueur courant\n",
        "        if (new_x, new_y) in self.initial_positions[self.current_player]:\n",
        "            self.done = True\n",
        "            self.winner = self.current_player\n",
        "\n",
        "    def get_valid_pawn_moves(self, pawn_pos):\n",
        "        \"\"\"\n",
        "        Pour un pion situé en pawn_pos, retourne la liste des mouvements valides\n",
        "        sous forme d'une liste de tuples (direction, position_finale).\n",
        "        \"\"\"\n",
        "        valid_moves = []\n",
        "        x, y = pawn_pos\n",
        "        for d in self.directions:\n",
        "            new_x, new_y = x, y\n",
        "            # Vérifier que la case voisine dans la direction d est libre\n",
        "            next_x, next_y = new_x + d[0], new_y + d[1]\n",
        "            if not self.in_bounds(next_x, next_y) or self.board[next_x, next_y] != 0:\n",
        "                continue\n",
        "            # Avancer dans la direction d jusqu'au bord ou jusqu'à rencontrer un obstacle\n",
        "            while self.in_bounds(new_x + d[0], new_y + d[1]) and self.board[new_x + d[0], new_y + d[1]] == 0:\n",
        "                new_x += d[0]\n",
        "                new_y += d[1]\n",
        "            valid_moves.append((d, (new_x, new_y)))\n",
        "        return valid_moves\n",
        "\n",
        "    def move_pawn(self, pawn_pos, direction):\n",
        "        \"\"\"\n",
        "        Déplace le pion situé en pawn_pos dans la direction donnée.\n",
        "        Le pion glisse jusqu'au bout de son déplacement.\n",
        "        \"\"\"\n",
        "        if self.board[pawn_pos] != self.current_player:\n",
        "            raise ValueError(\"Ce pion n'appartient pas au joueur courant.\")\n",
        "        valid_moves = self.get_valid_pawn_moves(pawn_pos)\n",
        "        move_final = None\n",
        "        for d, pos_finale in valid_moves:\n",
        "            if d == direction:\n",
        "                move_final = pos_finale\n",
        "                break\n",
        "        if move_final is None:\n",
        "            raise ValueError(\"Mouvement du pion invalide.\")\n",
        "        # Mise à jour du plateau\n",
        "        self.board[pawn_pos] = 0\n",
        "        self.board[move_final] = self.current_player\n",
        "\n",
        "    def step(self, bobail_direction, pawn_pos, pawn_direction):\n",
        "        \"\"\"\n",
        "        Exécute un tour complet pour le joueur courant.\n",
        "        - Sauf au premier tour, le joueur déplace d'abord le Bobail.\n",
        "        - Puis, le joueur déplace un de ses pions.\n",
        "\n",
        "        Retourne : (plateau, done, winner)\n",
        "        \"\"\"\n",
        "        if self.done:\n",
        "            raise Exception(\"La partie est terminée.\")\n",
        "\n",
        "        # Déplacement du Bobail si ce n'est pas le premier tour\n",
        "        if not self.first_turn:\n",
        "            if not self.get_valid_bobail_moves():\n",
        "                self.done = True\n",
        "                self.winner = 3 - self.current_player  # L'adversaire gagne\n",
        "                return self.board.copy(), self.done, self.winner\n",
        "            self.move_bobail(bobail_direction)\n",
        "            if self.done:\n",
        "                return self.board.copy(), self.done, self.winner\n",
        "        else:\n",
        "            self.first_turn = False\n",
        "\n",
        "        # Déplacement du pion choisi\n",
        "        self.move_pawn(pawn_pos, pawn_direction)\n",
        "\n",
        "        # Passage au joueur suivant\n",
        "        self.current_player = 3 - self.current_player\n",
        "\n",
        "        return self.board.copy(), self.done, self.winner\n",
        "\n",
        "    def render(self):\n",
        "        \"\"\"Affiche le plateau avec les couleurs appropriées.\"\"\"\n",
        "        for i in range(self.board_size):\n",
        "            row_str = \"\"\n",
        "            for j in range(self.board_size):\n",
        "                cell = self.board[i, j]\n",
        "                if cell == 0:\n",
        "                    row_str += \". \"\n",
        "                elif cell == 1:\n",
        "                    row_str += GREEN + \"1 \" + RESET  # Joueur 1 en vert\n",
        "                elif cell == 2:\n",
        "                    row_str += RED + \"2 \" + RESET    # Joueur 2 en rouge\n",
        "                elif cell == 3:\n",
        "                    row_str += YELLOW + \"B \" + RESET # Bobail en jaune\n",
        "            print(row_str)\n",
        "        print(\"\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "# On suppose que la classe BobailEnv a été définie dans la cellule précédente\n",
        "\n",
        "def random_agent(env):\n",
        "    \"\"\"\n",
        "    Agent aléatoire qui choisit un mouvement valide pour le Bobail (si applicable)\n",
        "    et un mouvement valide pour un pion appartenant au joueur courant.\n",
        "    \"\"\"\n",
        "    while not env.done:\n",
        "        # Choix du mouvement du Bobail (s'il n'est pas le premier tour)\n",
        "        if env.first_turn:\n",
        "            bobail_move = None\n",
        "        else:\n",
        "            valid_bobail_moves = env.get_valid_bobail_moves()\n",
        "            if valid_bobail_moves:\n",
        "                bobail_move = random.choice(valid_bobail_moves)\n",
        "            else:\n",
        "                print(\"Aucun mouvement valide pour le Bobail pour le joueur\", env.current_player)\n",
        "                break\n",
        "\n",
        "        # Récupérer les mouvements valides pour les pions du joueur courant\n",
        "        possible_pawn_moves = []\n",
        "        for i in range(env.board_size):\n",
        "            for j in range(env.board_size):\n",
        "                if env.board[i, j] == env.current_player:\n",
        "                    valid_moves = env.get_valid_pawn_moves((i, j))\n",
        "                    for move in valid_moves:\n",
        "                        # move est de la forme (direction, position_finale)\n",
        "                        possible_pawn_moves.append(((i, j), move[0]))\n",
        "\n",
        "        if not possible_pawn_moves:\n",
        "            print(\"Aucun mouvement valide pour un pion pour le joueur\", env.current_player)\n",
        "            break\n",
        "\n",
        "        pawn_move = random.choice(possible_pawn_moves)\n",
        "\n",
        "        try:\n",
        "            env.step(bobail_move, pawn_move[0], pawn_move[1])\n",
        "        except Exception as e:\n",
        "            print(\"Erreur lors de l'exécution du mouvement:\", e)\n",
        "            break\n",
        "\n",
        "        # Affichage du plateau\n",
        "        env.render()\n",
        "\n",
        "    if env.done:\n",
        "        print(\"Partie terminée. Gagnant : Joueur\", env.winner)\n",
        "    else:\n",
        "        print(\"Partie interrompue.\")\n",
        "\n",
        "# Lancement de la partie\n",
        "env = BobailEnv()\n",
        "env.reset()\n",
        "print(\"Plateau initial :\")\n",
        "env.render()\n",
        "random_agent(env)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ihWWe2JUura_",
        "outputId": "5c5c3acb-af78-4295-957b-9950f4c6111f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Plateau initial :\n",
            "\u001b[92m1 \u001b[0m\u001b[92m1 \u001b[0m\u001b[92m1 \u001b[0m\u001b[92m1 \u001b[0m\u001b[92m1 \u001b[0m\n",
            ". . . . . \n",
            ". . \u001b[93mB \u001b[0m. . \n",
            ". . . . . \n",
            "\u001b[91m2 \u001b[0m\u001b[91m2 \u001b[0m\u001b[91m2 \u001b[0m\u001b[91m2 \u001b[0m\u001b[91m2 \u001b[0m\n",
            "\n",
            "\n",
            "\u001b[92m1 \u001b[0m\u001b[92m1 \u001b[0m\u001b[92m1 \u001b[0m. \u001b[92m1 \u001b[0m\n",
            ". . . . . \n",
            ". . \u001b[93mB \u001b[0m. . \n",
            ". . . \u001b[92m1 \u001b[0m. \n",
            "\u001b[91m2 \u001b[0m\u001b[91m2 \u001b[0m\u001b[91m2 \u001b[0m\u001b[91m2 \u001b[0m\u001b[91m2 \u001b[0m\n",
            "\n",
            "\n",
            "\u001b[92m1 \u001b[0m\u001b[92m1 \u001b[0m\u001b[92m1 \u001b[0m. \u001b[92m1 \u001b[0m\n",
            ". \u001b[93mB \u001b[0m. . . \n",
            ". . . . . \n",
            ". . . \u001b[92m1 \u001b[0m\u001b[91m2 \u001b[0m\n",
            "\u001b[91m2 \u001b[0m\u001b[91m2 \u001b[0m\u001b[91m2 \u001b[0m. \u001b[91m2 \u001b[0m\n",
            "\n",
            "\n",
            "\u001b[92m1 \u001b[0m\u001b[92m1 \u001b[0m\u001b[92m1 \u001b[0m. . \n",
            ". . \u001b[93mB \u001b[0m. . \n",
            ". . . . . \n",
            ". \u001b[92m1 \u001b[0m. \u001b[92m1 \u001b[0m\u001b[91m2 \u001b[0m\n",
            "\u001b[91m2 \u001b[0m\u001b[91m2 \u001b[0m\u001b[91m2 \u001b[0m. \u001b[91m2 \u001b[0m\n",
            "\n",
            "\n",
            "\u001b[92m1 \u001b[0m\u001b[92m1 \u001b[0m\u001b[92m1 \u001b[0m. . \n",
            ". \u001b[93mB \u001b[0m. . . \n",
            ". . . . . \n",
            ". \u001b[92m1 \u001b[0m. \u001b[92m1 \u001b[0m. \n",
            "\u001b[91m2 \u001b[0m\u001b[91m2 \u001b[0m\u001b[91m2 \u001b[0m\u001b[91m2 \u001b[0m\u001b[91m2 \u001b[0m\n",
            "\n",
            "\n",
            "Erreur lors de l'exécution du mouvement: Mouvement du pion invalide.\n",
            "Partie interrompue.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import random\n",
        "\n",
        "def run_episode(env):\n",
        "    \"\"\"\n",
        "    Exécute une partie complète avec l'agent random.\n",
        "    Renvoie :\n",
        "      - winner : le joueur gagnant (1 ou 2, -1 si indéfini)\n",
        "      - step_count : nombre de tours (steps) dans la partie\n",
        "      - total_move_time : temps total mis pour exécuter tous les coups (en secondes)\n",
        "    \"\"\"\n",
        "    env.reset()\n",
        "    step_count = 0\n",
        "    total_move_time = 0.0\n",
        "\n",
        "    while not env.done:\n",
        "        start_time = time.time()\n",
        "\n",
        "        # Si ce n'est pas le premier tour, déplacer le Bobail\n",
        "        if not env.first_turn:\n",
        "            valid_bobail_moves = env.get_valid_bobail_moves()\n",
        "            if valid_bobail_moves:\n",
        "                bobail_move = random.choice(valid_bobail_moves)\n",
        "                env.move_bobail(bobail_move)\n",
        "                if env.done:\n",
        "                    break\n",
        "            else:\n",
        "                # Si aucun coup pour le Bobail, le joueur perd son tour (ou l'adversaire gagne)\n",
        "                env.done = True\n",
        "                env.winner = 3 - env.current_player\n",
        "                break\n",
        "        else:\n",
        "            # Premier tour : ne pas déplacer le Bobail\n",
        "            env.first_turn = False\n",
        "\n",
        "        # **Recalculer** les mouvements valides pour les pions après le déplacement du Bobail\n",
        "        possible_pawn_moves = []\n",
        "        for i in range(env.board_size):\n",
        "            for j in range(env.board_size):\n",
        "                if env.board[i, j] == env.current_player:\n",
        "                    valid_moves = env.get_valid_pawn_moves((i, j))\n",
        "                    for move in valid_moves:\n",
        "                        # Chaque move est de la forme (direction, position_finale)\n",
        "                        possible_pawn_moves.append(((i, j), move[0]))\n",
        "\n",
        "        if not possible_pawn_moves:\n",
        "            # Si aucun mouvement possible pour un pion, le joueur perd\n",
        "            env.done = True\n",
        "            env.winner = 3 - env.current_player\n",
        "            break\n",
        "\n",
        "        # Choix aléatoire d'un mouvement pour un pion\n",
        "        pawn_move = random.choice(possible_pawn_moves)\n",
        "        env.move_pawn(pawn_move[0], pawn_move[1])\n",
        "\n",
        "        # Passage au joueur suivant\n",
        "        env.current_player = 3 - env.current_player\n",
        "\n",
        "        end_time = time.time()\n",
        "        total_move_time += (end_time - start_time)\n",
        "        step_count += 1\n",
        "\n",
        "    winner = env.winner if env.winner is not None else -1\n",
        "    return winner, step_count, total_move_time\n",
        "\n",
        "def evaluate_agent(num_episodes):\n",
        "    \"\"\"\n",
        "    Exécute num_episodes parties avec l'agent random et calcule :\n",
        "      - Taux de victoire pour le joueur 1 et le joueur 2\n",
        "      - Longueur moyenne d'une partie (en steps)\n",
        "      - Temps moyen par coup (en secondes)\n",
        "    Renvoie un dictionnaire contenant ces métriques.\n",
        "    \"\"\"\n",
        "    # Instanciation de l'environnement\n",
        "    env = BobailEnv()\n",
        "\n",
        "    wins = {1: 0, 2: 0, 'other': 0}\n",
        "    total_steps = 0\n",
        "    total_move_time = 0.0\n",
        "\n",
        "    for _ in range(num_episodes):\n",
        "        winner, steps, move_time = run_episode(env)\n",
        "        total_steps += steps\n",
        "        total_move_time += move_time\n",
        "        if winner in wins:\n",
        "            wins[winner] += 1\n",
        "        else:\n",
        "            wins['other'] += 1\n",
        "\n",
        "    avg_game_length = total_steps / num_episodes\n",
        "    avg_move_time = (total_move_time / total_steps) if total_steps > 0 else 0\n",
        "    win_rate_1 = wins[1] / num_episodes\n",
        "    win_rate_2 = wins[2] / num_episodes\n",
        "\n",
        "    return {\n",
        "        'episodes': num_episodes,\n",
        "        'win_rate_1': win_rate_1,\n",
        "        'win_rate_2': win_rate_2,\n",
        "        'avg_game_length': avg_game_length,\n",
        "        'avg_move_time': avg_move_time,\n",
        "    }\n",
        "\n",
        "# Liste des nombres d'épisodes pour lesquels on souhaite évaluer\n",
        "episode_counts = [1000, 10000, 100000]  # Vous pouvez ajouter d'autres valeurs (ex : 1_000_000) si nécessaire\n",
        "\n",
        "for count in episode_counts:\n",
        "    metrics = evaluate_agent(count)\n",
        "    print(f\"Après {metrics['episodes']} épisodes :\")\n",
        "    print(f\"  Taux de victoire Joueur 1 : {metrics['win_rate_1']*100:.2f}%\")\n",
        "    print(f\"  Taux de victoire Joueur 2 : {metrics['win_rate_2']*100:.2f}%\")\n",
        "    print(f\"  Longueur moyenne d'une partie : {metrics['avg_game_length']:.2f} steps\")\n",
        "    print(f\"  Temps moyen par coup : {metrics['avg_move_time']*1000:.2f} ms\")\n",
        "    print(\"-\"*50)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DNCDgeLxu2hk",
        "outputId": "4da83913-6600-4703-9b3c-1c1e7cbf1499"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Après 1000 épisodes :\n",
            "  Taux de victoire Joueur 1 : 54.60%\n",
            "  Taux de victoire Joueur 2 : 45.40%\n",
            "  Longueur moyenne d'une partie : 15.48 steps\n",
            "  Temps moyen par coup : 0.05 ms\n",
            "--------------------------------------------------\n",
            "Après 10000 épisodes :\n",
            "  Taux de victoire Joueur 1 : 52.26%\n",
            "  Taux de victoire Joueur 2 : 47.74%\n",
            "  Longueur moyenne d'une partie : 16.07 steps\n",
            "  Temps moyen par coup : 0.06 ms\n",
            "--------------------------------------------------\n",
            "Après 100000 épisodes :\n",
            "  Taux de victoire Joueur 1 : 52.33%\n",
            "  Taux de victoire Joueur 2 : 47.67%\n",
            "  Longueur moyenne d'une partie : 16.03 steps\n",
            "  Temps moyen par coup : 0.06 ms\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "import random\n",
        "\n",
        "def encode_state(env):\n",
        "    \"\"\"\n",
        "    Encode l’état de l’environnement sous forme d’un tuple :\n",
        "    (tuple du plateau aplati, joueur courant, first_turn)\n",
        "    \"\"\"\n",
        "    return (tuple(env.board.flatten()), env.current_player, env.first_turn)\n",
        "\n",
        "def get_valid_actions(env):\n",
        "    \"\"\"\n",
        "    Retourne la liste des actions valides dans l’état courant.\n",
        "\n",
        "    Pour un état donné, les actions sont représentées de la manière suivante :\n",
        "    - Si env.first_turn est True, seules les actions de type \"pawn\" sont possibles.\n",
        "      Chaque action est : (\"pawn\", (i, j), (dx, dy))\n",
        "      Où (i,j) est la position du pion à déplacer et (dx, dy) la direction du déplacement.\n",
        "\n",
        "    - Sinon, les actions sont de type \"full\" et représentent la combinaison du déplacement\n",
        "      du Bobail et du déplacement d’un pion.\n",
        "      Chaque action est : (\"full\", (bob_dx, bob_dy), (i, j), (pawn_dx, pawn_dy))\n",
        "    \"\"\"\n",
        "    actions = []\n",
        "    if env.first_turn:\n",
        "        # Seules les actions de pion sont possibles.\n",
        "        for i in range(env.board_size):\n",
        "            for j in range(env.board_size):\n",
        "                if env.board[i, j] == env.current_player:\n",
        "                    valid_moves = env.get_valid_pawn_moves((i, j))\n",
        "                    for move in valid_moves:\n",
        "                        actions.append((\"pawn\", (i, j), move[0]))\n",
        "    else:\n",
        "        # Pour chaque mouvement possible du Bobail, simuler le déplacement et\n",
        "        # extraire ensuite les actions de pion.\n",
        "        bobail_moves = env.get_valid_bobail_moves()\n",
        "        for bobail_move in bobail_moves:\n",
        "            # Copie temporaire pour simuler le déplacement du Bobail.\n",
        "            temp_env = copy.deepcopy(env)\n",
        "            try:\n",
        "                temp_env.move_bobail(bobail_move)\n",
        "            except Exception:\n",
        "                continue  # Si le mouvement échoue, on passe au suivant.\n",
        "            # On calcule ensuite les mouvements possibles pour un pion dans temp_env.\n",
        "            for i in range(temp_env.board_size):\n",
        "                for j in range(temp_env.board_size):\n",
        "                    if temp_env.board[i, j] == temp_env.current_player:\n",
        "                        valid_moves = temp_env.get_valid_pawn_moves((i, j))\n",
        "                        for move in valid_moves:\n",
        "                            actions.append((\"full\", bobail_move, (i, j), move[0]))\n",
        "    return actions\n",
        "\n",
        "class TabularQLearningAgent:\n",
        "    def __init__(self, alpha=0.1, gamma=0.9, epsilon=0.2):\n",
        "        self.alpha = alpha      # Taux d'apprentissage\n",
        "        self.gamma = gamma      # Facteur de discount\n",
        "        self.epsilon = epsilon  # Probabilité d'exploration\n",
        "        self.q_table = {}       # Dictionnaire ((state, action) -> Q-value)\n",
        "\n",
        "    def get_q(self, state, action):\n",
        "        return self.q_table.get((state, action), 0.0)\n",
        "\n",
        "    def set_q(self, state, action, value):\n",
        "        self.q_table[(state, action)] = value\n",
        "\n",
        "    def choose_action(self, state, valid_actions):\n",
        "        \"\"\"\n",
        "        Choisit une action avec une stratégie epsilon-greedy.\n",
        "        \"\"\"\n",
        "        if not valid_actions:\n",
        "            return None\n",
        "        if random.random() < self.epsilon:\n",
        "            return random.choice(valid_actions)\n",
        "        else:\n",
        "            q_values = [self.get_q(state, action) for action in valid_actions]\n",
        "            max_q = max(q_values)\n",
        "            # Choisir aléatoirement parmi les actions ayant le Q-value maximal.\n",
        "            best_actions = [action for action, q in zip(valid_actions, q_values) if q == max_q]\n",
        "            return random.choice(best_actions)\n",
        "\n",
        "    def update(self, state, action, reward, next_state, next_valid_actions):\n",
        "        current_q = self.get_q(state, action)\n",
        "        if next_valid_actions:\n",
        "            next_q = max([self.get_q(next_state, a) for a in next_valid_actions])\n",
        "        else:\n",
        "            next_q = 0.0\n",
        "        new_q = current_q + self.alpha * (reward + self.gamma * next_q - current_q)\n",
        "        self.set_q(state, action, new_q)\n",
        "\n",
        "def train_agent(agent, num_episodes):\n",
        "    \"\"\"\n",
        "    Entraîne l’agent sur num_episodes parties.\n",
        "    Pour chaque épisode, on met à jour la Q-table et on accumule la récompense totale.\n",
        "    \"\"\"\n",
        "    env = BobailEnv()\n",
        "    episode_rewards = []\n",
        "\n",
        "    for ep in range(num_episodes):\n",
        "        env.reset()\n",
        "        total_reward = 0.0\n",
        "        state = encode_state(env)\n",
        "\n",
        "        # Tant que la partie n'est pas terminée.\n",
        "        while not env.done:\n",
        "            valid_actions = get_valid_actions(env)\n",
        "            action = agent.choose_action(state, valid_actions)\n",
        "            if action is None:\n",
        "                break\n",
        "\n",
        "            # Exécuter l’action choisie.\n",
        "            try:\n",
        "                if action[0] == \"pawn\":\n",
        "                    # Action de type (\"pawn\", (i,j), (dx, dy))\n",
        "                    env.step(None, action[1], action[2])\n",
        "                elif action[0] == \"full\":\n",
        "                    # Action de type (\"full\", bobail_move, (i,j), (dx, dy))\n",
        "                    env.step(action[1], action[2], action[3])\n",
        "            except Exception as e:\n",
        "                # En cas d'erreur (théoriquement, cela ne devrait pas arriver),\n",
        "                # on affecte une pénalité et on sort de la boucle.\n",
        "                total_reward += -1\n",
        "                break\n",
        "\n",
        "            # Attribuer une récompense :\n",
        "            # - Si la partie se termine, +1 en cas de victoire pour l'agent ayant joué le coup, -1 sinon.\n",
        "            # - Sinon, une petite pénalité pour encourager des parties plus courtes.\n",
        "            if env.done:\n",
        "                # Le coup a été joué par le joueur qui vient d'agir.\n",
        "                # Après l'exécution de step, current_player est changé.\n",
        "                action_player = 3 - env.current_player\n",
        "                reward = 1.0 if env.winner == action_player else -1.0\n",
        "            else:\n",
        "                reward = -0.01\n",
        "\n",
        "            total_reward += reward\n",
        "            next_state = encode_state(env)\n",
        "            next_valid_actions = get_valid_actions(env)\n",
        "\n",
        "            # Mise à jour de la Q-table.\n",
        "            agent.update(state, action, reward, next_state, next_valid_actions)\n",
        "            state = next_state\n",
        "\n",
        "        episode_rewards.append(total_reward)\n",
        "        if ep % 1000 == 0:\n",
        "            print(f\"Episode {ep} terminé, récompense totale = {total_reward:.2f}\")\n",
        "\n",
        "    return episode_rewards\n",
        "\n",
        "# Exemple d'entraînement\n",
        "agent = TabularQLearningAgent(alpha=0.1, gamma=0.9, epsilon=0.2)\n",
        "num_training_episodes = 5000  # Vous pouvez augmenter ce nombre pour de meilleurs résultats.\n",
        "rewards = train_agent(agent, num_training_episodes)\n",
        "print(\"Entraînement terminé.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JkJuXDDlvJBi",
        "outputId": "cdaac177-3170-45f9-a652-83f7def4255e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 0 terminé, récompense totale = -1.20\n",
            "Episode 1000 terminé, récompense totale = -1.21\n",
            "Episode 2000 terminé, récompense totale = -1.02\n",
            "Episode 3000 terminé, récompense totale = -1.15\n",
            "Episode 4000 terminé, récompense totale = -1.15\n",
            "Entraînement terminé.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def run_evaluation_episode(env, agent):\n",
        "    \"\"\"\n",
        "    Exécute une partie complète en mode évaluation (sans exploration) avec l'agent.\n",
        "    Renvoie le gagnant et le nombre de coups (steps) joués.\n",
        "    \"\"\"\n",
        "    env.reset()\n",
        "    state = encode_state(env)\n",
        "    steps = 0\n",
        "    while not env.done:\n",
        "        valid_actions = get_valid_actions(env)\n",
        "        if not valid_actions:\n",
        "            break\n",
        "\n",
        "        # Choix de la meilleure action selon la Q-table (ε = 0)\n",
        "        best_action = None\n",
        "        best_q = -float('inf')\n",
        "        for action in valid_actions:\n",
        "            q = agent.get_q(state, action)\n",
        "            if q > best_q:\n",
        "                best_q = q\n",
        "                best_action = action\n",
        "\n",
        "        # Si aucune action n'a de valeur (ce qui est rare), on choisit aléatoirement.\n",
        "        if best_action is None:\n",
        "            best_action = random.choice(valid_actions)\n",
        "\n",
        "        # Exécuter l'action choisie\n",
        "        try:\n",
        "            if best_action[0] == \"pawn\":\n",
        "                env.step(None, best_action[1], best_action[2])\n",
        "            elif best_action[0] == \"full\":\n",
        "                env.step(best_action[1], best_action[2], best_action[3])\n",
        "        except Exception as e:\n",
        "            print(\"Erreur lors de l'évaluation:\", e)\n",
        "            break\n",
        "\n",
        "        state = encode_state(env)\n",
        "        steps += 1\n",
        "    return env.winner, steps\n",
        "\n",
        "def evaluate_policy(agent, num_episodes):\n",
        "    \"\"\"\n",
        "    Évalue la politique apprise par l'agent sur num_episodes parties en mode évaluation.\n",
        "    Affiche le taux de victoire pour le joueur 1 et le joueur 2 ainsi que la longueur moyenne d'une partie.\n",
        "    \"\"\"\n",
        "    # Sauvegarde de la valeur d'exploration actuelle et désactivation de l'exploration.\n",
        "    original_epsilon = agent.epsilon\n",
        "    agent.epsilon = 0.0\n",
        "\n",
        "    env = BobailEnv()\n",
        "    wins = {1: 0, 2: 0}\n",
        "    total_steps = 0\n",
        "    for _ in range(num_episodes):\n",
        "        winner, steps = run_evaluation_episode(env, agent)\n",
        "        if winner in wins:\n",
        "            wins[winner] += 1\n",
        "        total_steps += steps\n",
        "\n",
        "    avg_steps = total_steps / num_episodes\n",
        "    win_rate_1 = wins[1] / num_episodes * 100\n",
        "    win_rate_2 = wins[2] / num_episodes * 100\n",
        "\n",
        "    print(f\"Évaluation sur {num_episodes} épisodes :\")\n",
        "    print(f\"  Taux de victoire du Joueur 1 : {win_rate_1:.2f}%\")\n",
        "    print(f\"  Taux de victoire du Joueur 2 : {win_rate_2:.2f}%\")\n",
        "    print(f\"  Longueur moyenne d'une partie : {avg_steps:.2f} coups\")\n",
        "\n",
        "    # Restauration de la valeur d'exploration initiale.\n",
        "    agent.epsilon = original_epsilon\n",
        "\n",
        "# Exemple d'évaluation de la politique apprise\n",
        "evaluate_policy(agent, 1000)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xPYj6ng2vRf4",
        "outputId": "561a56ad-c36c-41cc-94b5-6489fb31fddf"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Évaluation sur 1000 épisodes :\n",
            "  Taux de victoire du Joueur 1 : 100.00%\n",
            "  Taux de victoire du Joueur 2 : 0.00%\n",
            "  Longueur moyenne d'une partie : 3.00 coups\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import random\n",
        "import copy\n",
        "\n",
        "# --- Paramètres globaux ---\n",
        "\n",
        "STATE_SIZE = 27  # 25 cases du plateau + 1 (joueur courant) + 1 (first_turn)\n",
        "\n",
        "# Définir l'espace d'action pour le Pawn :\n",
        "# Pour chaque case (5x5) et pour chacune des 8 directions.\n",
        "pawn_action_space = []\n",
        "directions_list = [(-1,-1), (-1,0), (-1,1), (0,-1), (0,1), (1,-1), (1,0), (1,1)]\n",
        "for i in range(5):\n",
        "    for j in range(5):\n",
        "        for d in directions_list:\n",
        "            pawn_action_space.append((i, j, d))\n",
        "num_pawn_actions = len(pawn_action_space)  # 5*5*8 = 200\n",
        "\n",
        "# Pour le Bobail, l'espace d'action est simplement les 8 directions.\n",
        "bobail_action_space = directions_list  # 8 actions\n",
        "num_bobail_actions = len(bobail_action_space)  # 8\n",
        "\n",
        "# --- Définition du réseau DQN ---\n",
        "class DQN(nn.Module):\n",
        "    def __init__(self, input_size, output_size, hidden_size=64):\n",
        "        super(DQN, self).__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(input_size, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size, output_size)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "# --- Agent Deep Q‑Learning ---\n",
        "class DQNAgent:\n",
        "    def __init__(self, state_size, action_space, lr=1e-3, gamma=0.99, epsilon=0.1):\n",
        "        self.state_size = state_size\n",
        "        self.action_space = action_space  # liste des actions globales\n",
        "        self.action_size = len(action_space)\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon\n",
        "\n",
        "        self.model = DQN(state_size, self.action_size)\n",
        "        self.optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
        "        self.loss_fn = nn.MSELoss()\n",
        "\n",
        "    def state_to_tensor(self, state):\n",
        "        # On suppose que state est un tuple (plateau aplati, current_player, first_turn)\n",
        "        state_array = np.array(state, dtype=np.float32)\n",
        "        return torch.tensor(state_array, dtype=torch.float32).unsqueeze(0)  # shape (1, state_size)\n",
        "\n",
        "    def predict(self, state):\n",
        "        state_tensor = self.state_to_tensor(state)\n",
        "        with torch.no_grad():\n",
        "            q_values = self.model(state_tensor).squeeze(0).numpy()\n",
        "        return q_values  # shape (action_size,)\n",
        "\n",
        "    def select_action(self, state, valid_actions_mask):\n",
        "        \"\"\"\n",
        "        valid_actions_mask : array numpy de forme (action_size,) avec True pour les actions valides.\n",
        "        Retourne l'indice de l'action choisie dans self.action_space.\n",
        "        \"\"\"\n",
        "        if np.random.rand() < self.epsilon:\n",
        "            valid_indices = np.where(valid_actions_mask)[0]\n",
        "            if len(valid_indices) == 0:\n",
        "                return None\n",
        "            return np.random.choice(valid_indices)\n",
        "        q_values = self.predict(state)\n",
        "        # Masquer les actions invalides\n",
        "        masked_q = np.where(valid_actions_mask, q_values, -np.inf)\n",
        "        return int(np.argmax(masked_q))\n",
        "\n",
        "    def update(self, state, action_index, reward, next_state, done, next_valid_mask):\n",
        "        state_tensor = self.state_to_tensor(state)\n",
        "        next_state_tensor = self.state_to_tensor(next_state)\n",
        "        q_values = self.model(state_tensor)  # shape (1, action_size)\n",
        "        q_value = q_values[0, action_index]\n",
        "\n",
        "        with torch.no_grad():\n",
        "            next_q_values = self.model(next_state_tensor).squeeze(0)\n",
        "            # Masquer les actions invalides pour le prochain état\n",
        "            masked_next_q = torch.where(torch.tensor(next_valid_mask, dtype=torch.bool),\n",
        "                                        next_q_values, torch.tensor(-1e9))\n",
        "            max_next_q = torch.max(masked_next_q)\n",
        "            target = reward + (0 if done else self.gamma * max_next_q)\n",
        "        loss = self.loss_fn(q_value, target)\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "        return loss.item()\n",
        "\n",
        "# Création des deux agents :\n",
        "agent_bobail = DQNAgent(state_size=STATE_SIZE, action_space=bobail_action_space, lr=1e-3, gamma=0.99, epsilon=0.1)\n",
        "agent_pawn = DQNAgent(state_size=STATE_SIZE, action_space=pawn_action_space, lr=1e-3, gamma=0.99, epsilon=0.1)\n",
        "\n",
        "# --- Fonctions utilitaires pour la gestion des masques d'actions ---\n",
        "\n",
        "def encode_state(env):\n",
        "    \"\"\"\n",
        "    Encode l'état de l'environnement en un vecteur de taille 27.\n",
        "    On utilise :\n",
        "      - l'état du plateau aplati (25 valeurs)\n",
        "      - le joueur courant (1 ou 2) normalisé (par exemple, 0 ou 1)\n",
        "      - first_turn (0 ou 1)\n",
        "    \"\"\"\n",
        "    board_flat = env.board.flatten().astype(np.float32)\n",
        "    # Normalisation : par exemple, on peut mapper 1 -> 0 et 2 -> 1 pour le joueur courant.\n",
        "    current_player = np.array([0.0 if env.current_player == 1 else 1.0], dtype=np.float32)\n",
        "    first_turn = np.array([1.0 if env.first_turn else 0.0], dtype=np.float32)\n",
        "    return tuple(np.concatenate([board_flat, current_player, first_turn]))\n",
        "\n",
        "def get_valid_mask_bobail(env):\n",
        "    \"\"\"\n",
        "    Renvoie un masque booléen de taille 8 pour l'agent Bobail.\n",
        "    Chaque élément est True si la direction correspondante (dans bobail_action_space)\n",
        "    est valide dans l'état actuel.\n",
        "    \"\"\"\n",
        "    valid_moves = env.get_valid_bobail_moves()\n",
        "    mask = np.array([move in valid_moves for move in bobail_action_space], dtype=bool)\n",
        "    return mask\n",
        "\n",
        "def get_valid_mask_pawn(env):\n",
        "    \"\"\"\n",
        "    Renvoie un masque booléen de taille num_pawn_actions (200) pour l'agent Pawn.\n",
        "    Pour chaque action candidate (i, j, d), on vérifie si :\n",
        "      - La case (i, j) contient bien un pion du joueur courant.\n",
        "      - La direction d figure dans la liste des mouvements valides pour ce pion.\n",
        "    \"\"\"\n",
        "    mask = np.zeros(num_pawn_actions, dtype=bool)\n",
        "    for idx, (i, j, d) in enumerate(pawn_action_space):\n",
        "        if env.board[i, j] == env.current_player:\n",
        "            valid_moves = env.get_valid_pawn_moves((i, j))\n",
        "            # valid_moves est une liste de tuples (direction, final_pos)\n",
        "            valid_dirs = [move[0] for move in valid_moves]\n",
        "            if d in valid_dirs:\n",
        "                mask[idx] = True\n",
        "    return mask\n"
      ],
      "metadata": {
        "id": "D79XGYYgwDqz"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_deep_q_learning(num_episodes):\n",
        "    from bobail_env import BobailEnv  # Assurez-vous que BobailEnv est défini ou importé dans la session\n",
        "    env = BobailEnv()\n",
        "    episode_losses = []\n",
        "    for ep in range(num_episodes):\n",
        "        env.reset()\n",
        "        total_loss = 0.0\n",
        "        done = False\n",
        "        # Encodage initial de l'état\n",
        "        state = encode_state(env)\n",
        "\n",
        "        while not env.done:\n",
        "            # Si premier tour, on ne déplace que le pion.\n",
        "            if env.first_turn:\n",
        "                # Pour l'agent Pawn, calculer le masque d'actions valides\n",
        "                valid_mask_pawn = get_valid_mask_pawn(env)\n",
        "                action_idx = agent_pawn.select_action(state, valid_mask_pawn)\n",
        "                if action_idx is None:\n",
        "                    break\n",
        "                chosen_action = pawn_action_space[action_idx]\n",
        "                # Exécuter l'action de type (\"pawn\", (i,j), (dx,dy))\n",
        "                try:\n",
        "                    env.step(None, (chosen_action[0], chosen_action[1]), chosen_action[2])\n",
        "                except Exception as e:\n",
        "                    # On attribue une pénalité en cas d'erreur\n",
        "                    reward = -1.0\n",
        "                    agent_pawn.update(state, action_idx, reward, state, True, valid_mask_pawn)\n",
        "                    break\n",
        "                reward = -0.01  # petite pénalité pour encourager des parties plus courtes\n",
        "                next_state = encode_state(env)\n",
        "                next_valid_mask = get_valid_mask_pawn(env)\n",
        "                loss = agent_pawn.update(state, action_idx, reward, next_state, env.done, next_valid_mask)\n",
        "                total_loss += loss\n",
        "                state = next_state\n",
        "            else:\n",
        "                # Pour un tour complet, on doit choisir un mouvement pour le Bobail, puis pour le pion.\n",
        "                # 1. Sélection du mouvement du Bobail\n",
        "                valid_mask_bobail = get_valid_mask_bobail(env)\n",
        "                action_idx_bobail = agent_bobail.select_action(state, valid_mask_bobail)\n",
        "                if action_idx_bobail is None:\n",
        "                    break\n",
        "                chosen_bobail_move = bobail_action_space[action_idx_bobail]\n",
        "                # Exécuter le mouvement du Bobail\n",
        "                try:\n",
        "                    env.move_bobail(chosen_bobail_move)\n",
        "                except Exception as e:\n",
        "                    reward = -1.0\n",
        "                    agent_bobail.update(state, action_idx_bobail, reward, state, True, valid_mask_bobail)\n",
        "                    break\n",
        "                reward = -0.01\n",
        "                next_state = encode_state(env)\n",
        "                next_valid_mask_bobail = get_valid_mask_bobail(env)\n",
        "                loss_bobail = agent_bobail.update(state, action_idx_bobail, reward, next_state, env.done, next_valid_mask_bobail)\n",
        "                total_loss += loss_bobail\n",
        "                state = next_state\n",
        "\n",
        "                if env.done:\n",
        "                    break\n",
        "\n",
        "                # 2. Sélection du mouvement du Pawn\n",
        "                valid_mask_pawn = get_valid_mask_pawn(env)\n",
        "                action_idx_pawn = agent_pawn.select_action(state, valid_mask_pawn)\n",
        "                if action_idx_pawn is None:\n",
        "                    break\n",
        "                chosen_pawn_action = pawn_action_space[action_idx_pawn]\n",
        "                try:\n",
        "                    env.move_pawn((chosen_pawn_action[0], chosen_pawn_action[1]), chosen_pawn_action[2])\n",
        "                except Exception as e:\n",
        "                    reward = -1.0\n",
        "                    agent_pawn.update(state, action_idx_pawn, reward, state, True, valid_mask_pawn)\n",
        "                    break\n",
        "                reward = -0.01\n",
        "                next_state = encode_state(env)\n",
        "                next_valid_mask_pawn = get_valid_mask_pawn(env)\n",
        "                loss_pawn = agent_pawn.update(state, action_idx_pawn, reward, next_state, env.done, next_valid_mask_pawn)\n",
        "                total_loss += loss_pawn\n",
        "                state = next_state\n",
        "\n",
        "                # Passage au joueur suivant\n",
        "                env.current_player = 3 - env.current_player\n",
        "            # Fin du while\n",
        "        episode_losses.append(total_loss)\n",
        "        if ep % 100 == 0:\n",
        "            print(f\"Episode {ep} terminé, perte totale = {total_loss:.4f}\")\n",
        "    return episode_losses\n",
        "\n",
        "# Exemple d'entraînement pour 500 épisodes\n",
        "losses = train_deep_q_learning(500)\n",
        "print(\"Entraînement Deep Q‑Learning terminé.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 507
        },
        "id": "Grrg3udSwKO7",
        "outputId": "7eaa7539-cbc8-43dd-e5e0-5dbbbea16e46"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'bobail_env'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-7c97017f8489>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;31m# Exemple d'entraînement pour 500 épisodes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_deep_q_learning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Entraînement Deep Q‑Learning terminé.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-7c97017f8489>\u001b[0m in \u001b[0;36mtrain_deep_q_learning\u001b[0;34m(num_episodes)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrain_deep_q_learning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_episodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mbobail_env\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBobailEnv\u001b[0m  \u001b[0;31m# Assurez-vous que BobailEnv est défini ou importé dans la session\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBobailEnv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mepisode_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_episodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'bobail_env'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "agent DoubleDeepQLearning"
      ],
      "metadata": {
        "id": "wlgF0kzNwQhZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import random\n",
        "import copy\n",
        "\n",
        "# ----- Définition du réseau DQN -----\n",
        "class DQN(nn.Module):\n",
        "    def __init__(self, input_size, output_size, hidden_size=64):\n",
        "        super(DQN, self).__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(input_size, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size, output_size)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "# ----- Double DQN Agent -----\n",
        "class DoubleDQNAgent:\n",
        "    def __init__(self, state_size, action_space, lr=1e-3, gamma=0.99, epsilon=0.1, target_update_freq=100):\n",
        "        self.state_size = state_size\n",
        "        self.action_space = action_space  # liste des actions possibles\n",
        "        self.action_size = len(action_space)\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon\n",
        "        self.target_update_freq = target_update_freq\n",
        "        self.step_count = 0\n",
        "\n",
        "        # Réseau principal et target network\n",
        "        self.model = DQN(state_size, self.action_size)\n",
        "        self.target_model = DQN(state_size, self.action_size)\n",
        "        self.target_model.load_state_dict(self.model.state_dict())\n",
        "\n",
        "        self.optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
        "        self.loss_fn = nn.MSELoss()\n",
        "\n",
        "    def state_to_tensor(self, state):\n",
        "        state_array = np.array(state, dtype=np.float32)\n",
        "        return torch.tensor(state_array, dtype=torch.float32).unsqueeze(0)\n",
        "\n",
        "    def predict(self, state):\n",
        "        state_tensor = self.state_to_tensor(state)\n",
        "        with torch.no_grad():\n",
        "            q_values = self.model(state_tensor).squeeze(0).numpy()\n",
        "        return q_values\n",
        "\n",
        "    def select_action(self, state, valid_actions_mask):\n",
        "        \"\"\"\n",
        "        valid_actions_mask : numpy array de forme (action_size,) avec True pour les actions valides.\n",
        "        Retourne l'indice de l'action sélectionnée.\n",
        "        \"\"\"\n",
        "        if np.random.rand() < self.epsilon:\n",
        "            valid_indices = np.where(valid_actions_mask)[0]\n",
        "            if len(valid_indices) == 0:\n",
        "                return None\n",
        "            return np.random.choice(valid_indices)\n",
        "        q_values = self.predict(state)\n",
        "        masked_q = np.where(valid_actions_mask, q_values, -np.inf)\n",
        "        return int(np.argmax(masked_q))\n",
        "\n",
        "    def update(self, state, action_index, reward, next_state, done, next_valid_mask):\n",
        "        state_tensor = self.state_to_tensor(state)\n",
        "        next_state_tensor = self.state_to_tensor(next_state)\n",
        "\n",
        "        q_values = self.model(state_tensor)  # forme (1, action_size)\n",
        "        current_q = q_values[0, action_index]\n",
        "\n",
        "        with torch.no_grad():\n",
        "            # Sélection de l'action suivante avec le réseau principal\n",
        "            next_q_values = self.model(next_state_tensor).squeeze(0)\n",
        "            masked_next_q = torch.where(\n",
        "                torch.tensor(next_valid_mask, dtype=torch.bool),\n",
        "                next_q_values,\n",
        "                torch.tensor(-1e9, dtype=next_q_values.dtype)\n",
        "            )\n",
        "            best_next_action = torch.argmax(masked_next_q).item()\n",
        "            # Évaluation de la valeur de cette action avec le target network\n",
        "            target_q_values = self.target_model(next_state_tensor).squeeze(0)\n",
        "            best_next_q = target_q_values[best_next_action]\n",
        "            if done:\n",
        "                target = torch.tensor(reward, dtype=current_q.dtype, device=current_q.device)\n",
        "            else:\n",
        "                target = torch.tensor(reward, dtype=current_q.dtype, device=current_q.device) + self.gamma * best_next_q\n",
        "\n",
        "        loss = self.loss_fn(current_q, target)\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        self.step_count += 1\n",
        "        if self.step_count % self.target_update_freq == 0:\n",
        "            self.target_model.load_state_dict(self.model.state_dict())\n",
        "\n",
        "        return loss.item()\n",
        "\n",
        "# ----- Helpers pour l'encodage d'état et le masquage des actions -----\n",
        "\n",
        "def encode_state(env):\n",
        "    \"\"\"\n",
        "    Encode l'état en un vecteur de taille 27 :\n",
        "      - 25 valeurs correspondant au plateau (aplati)\n",
        "      - 1 valeur pour le joueur courant (0 pour le joueur 1, 1 pour le joueur 2)\n",
        "      - 1 valeur indiquant si c'est le premier tour (1 si True, 0 sinon)\n",
        "    \"\"\"\n",
        "    board_flat = env.board.flatten().astype(np.float32)\n",
        "    current_player = np.array([0.0 if env.current_player == 1 else 1.0], dtype=np.float32)\n",
        "    first_turn = np.array([1.0 if env.first_turn else 0.0], dtype=np.float32)\n",
        "    return tuple(np.concatenate([board_flat, current_player, first_turn]))\n",
        "\n",
        "# Définition des espaces d'actions\n",
        "directions_list = [(-1,-1), (-1,0), (-1,1), (0,-1), (0,1), (1,-1), (1,0), (1,1)]\n",
        "bobail_action_space = directions_list  # 8 actions\n",
        "num_bobail_actions = len(bobail_action_space)\n",
        "\n",
        "pawn_action_space = []\n",
        "for i in range(5):\n",
        "    for j in range(5):\n",
        "        for d in directions_list:\n",
        "            pawn_action_space.append((i, j, d))\n",
        "num_pawn_actions = len(pawn_action_space)  # 5*5*8 = 200\n",
        "\n",
        "def get_valid_mask_bobail(env):\n",
        "    \"\"\"\n",
        "    Renvoie un masque booléen de taille 8 pour l'agent Bobail.\n",
        "    Chaque élément vaut True si le mouvement correspondant est valide dans l'état actuel.\n",
        "    \"\"\"\n",
        "    valid_moves = env.get_valid_bobail_moves()\n",
        "    mask = np.array([move in valid_moves for move in bobail_action_space], dtype=bool)\n",
        "    return mask\n",
        "\n",
        "def get_valid_mask_pawn(env):\n",
        "    \"\"\"\n",
        "    Renvoie un masque booléen de taille 200 pour l'agent Pawn.\n",
        "    Pour chaque action candidate (i, j, d), on vérifie que :\n",
        "      - La case (i, j) contient un pion du joueur courant.\n",
        "      - La direction d figure parmi les mouvements valides pour ce pion.\n",
        "    \"\"\"\n",
        "    mask = np.zeros(num_pawn_actions, dtype=bool)\n",
        "    for idx, (i, j, d) in enumerate(pawn_action_space):\n",
        "        if env.board[i, j] == env.current_player:\n",
        "            valid_moves = env.get_valid_pawn_moves((i, j))\n",
        "            valid_dirs = [move[0] for move in valid_moves]\n",
        "            if d in valid_dirs:\n",
        "                mask[idx] = True\n",
        "    return mask\n",
        "\n",
        "# Taille de l'état (25 cases + 1 joueur + 1 first_turn) = 27\n",
        "STATE_SIZE = 27\n",
        "\n",
        "# Création des agents Double DQN pour le Bobail et pour les pions\n",
        "agent_bobail_ddqn = DoubleDQNAgent(state_size=STATE_SIZE, action_space=bobail_action_space, lr=1e-3, gamma=0.99, epsilon=0.1, target_update_freq=100)\n",
        "agent_pawn_ddqn = DoubleDQNAgent(state_size=STATE_SIZE, action_space=pawn_action_space, lr=1e-3, gamma=0.99, epsilon=0.1, target_update_freq=100)\n"
      ],
      "metadata": {
        "id": "m-qR8IyCwREA"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_ddqn(num_episodes):\n",
        "    \"\"\"\n",
        "    Entraîne les agents Double DQN sur 'num_episodes' parties.\n",
        "    Pour chaque épisode, on met à jour d'abord l'agent qui contrôle le Bobail (si applicable)\n",
        "    puis celui qui contrôle les pions. On accumule et affiche périodiquement la perte totale.\n",
        "    \"\"\"\n",
        "    env = BobailEnv()\n",
        "    episode_losses = []\n",
        "\n",
        "    for ep in range(num_episodes):\n",
        "        env.reset()\n",
        "        total_loss = 0.0\n",
        "        state = encode_state(env)\n",
        "\n",
        "        while not env.done:\n",
        "            # --- Premier tour : seul le mouvement d'un pion est effectué ---\n",
        "            if env.first_turn:\n",
        "                valid_mask_pawn = get_valid_mask_pawn(env)\n",
        "                action_idx = agent_pawn_ddqn.select_action(state, valid_mask_pawn)\n",
        "                if action_idx is None:\n",
        "                    break\n",
        "                chosen_action = pawn_action_space[action_idx]\n",
        "                try:\n",
        "                    env.step(None, (chosen_action[0], chosen_action[1]), chosen_action[2])\n",
        "                except Exception as e:\n",
        "                    reward = -1.0\n",
        "                    agent_pawn_ddqn.update(state, action_idx, reward, state, True, valid_mask_pawn)\n",
        "                    break\n",
        "                reward = -0.01  # petite pénalité pour encourager des parties plus courtes\n",
        "                next_state = encode_state(env)\n",
        "                next_valid_mask = get_valid_mask_pawn(env)\n",
        "                loss = agent_pawn_ddqn.update(state, action_idx, reward, next_state, env.done, next_valid_mask)\n",
        "                total_loss += loss\n",
        "                state = next_state\n",
        "            else:\n",
        "                # --- Tour complet : déplacement du Bobail, puis déplacement d'un pion ---\n",
        "                # 1. Mouvement du Bobail\n",
        "                valid_mask_bobail = get_valid_mask_bobail(env)\n",
        "                action_idx_bobail = agent_bobail_ddqn.select_action(state, valid_mask_bobail)\n",
        "                if action_idx_bobail is None:\n",
        "                    break\n",
        "                chosen_bobail_move = bobail_action_space[action_idx_bobail]\n",
        "                try:\n",
        "                    env.move_bobail(chosen_bobail_move)\n",
        "                except Exception as e:\n",
        "                    reward = -1.0\n",
        "                    agent_bobail_ddqn.update(state, action_idx_bobail, reward, state, True, valid_mask_bobail)\n",
        "                    break\n",
        "                reward = -0.01\n",
        "                next_state = encode_state(env)\n",
        "                next_valid_mask_bobail = get_valid_mask_bobail(env)\n",
        "                loss_bobail = agent_bobail_ddqn.update(state, action_idx_bobail, reward, next_state, env.done, next_valid_mask_bobail)\n",
        "                total_loss += loss_bobail\n",
        "                state = next_state\n",
        "\n",
        "                if env.done:\n",
        "                    break\n",
        "\n",
        "                # 2. Mouvement du Pawn\n",
        "                valid_mask_pawn = get_valid_mask_pawn(env)\n",
        "                action_idx_pawn = agent_pawn_ddqn.select_action(state, valid_mask_pawn)\n",
        "                if action_idx_pawn is None:\n",
        "                    break\n",
        "                chosen_pawn_action = pawn_action_space[action_idx_pawn]\n",
        "                try:\n",
        "                    env.move_pawn((chosen_pawn_action[0], chosen_pawn_action[1]), chosen_pawn_action[2])\n",
        "                except Exception as e:\n",
        "                    reward = -1.0\n",
        "                    agent_pawn_ddqn.update(state, action_idx_pawn, reward, state, True, valid_mask_pawn)\n",
        "                    break\n",
        "                reward = -0.01\n",
        "                next_state = encode_state(env)\n",
        "                next_valid_mask_pawn = get_valid_mask_pawn(env)\n",
        "                loss_pawn = agent_pawn_ddqn.update(state, action_idx_pawn, reward, next_state, env.done, next_valid_mask_pawn)\n",
        "                total_loss += loss_pawn\n",
        "                state = next_state\n",
        "\n",
        "                # Passage au joueur suivant\n",
        "                env.current_player = 3 - env.current_player\n",
        "\n",
        "        episode_losses.append(total_loss)\n",
        "        if ep % 100 == 0:\n",
        "            print(f\"Episode {ep} terminé, perte totale = {total_loss:.4f}\")\n",
        "\n",
        "    return episode_losses\n",
        "\n",
        "def run_ddqn_evaluation_episode(env, agent_bobail, agent_pawn):\n",
        "    \"\"\"\n",
        "    Exécute une partie en mode évaluation (sans exploration) en utilisant les agents DDQN.\n",
        "    Pour chaque décision, on choisit l'action ayant la Q-value maximale parmi les actions valides.\n",
        "    Renvoie le gagnant et le nombre de coups joués.\n",
        "    \"\"\"\n",
        "    env.reset()\n",
        "    state = encode_state(env)\n",
        "    steps = 0\n",
        "\n",
        "    while not env.done:\n",
        "        if env.first_turn:\n",
        "            valid_mask_pawn = get_valid_mask_pawn(env)\n",
        "            q_values = agent_pawn_ddqn.predict(state)\n",
        "            # Choix de l'action avec la Q-value maximale parmi les actions valides\n",
        "            best_idx = None\n",
        "            best_q = -float('inf')\n",
        "            for idx, valid in enumerate(valid_mask_pawn):\n",
        "                if valid and q_values[idx] > best_q:\n",
        "                    best_q = q_values[idx]\n",
        "                    best_idx = idx\n",
        "            if best_idx is None:\n",
        "                break\n",
        "            chosen_action = pawn_action_space[best_idx]\n",
        "            try:\n",
        "                env.step(None, (chosen_action[0], chosen_action[1]), chosen_action[2])\n",
        "            except Exception as e:\n",
        "                break\n",
        "            state = encode_state(env)\n",
        "            steps += 1\n",
        "        else:\n",
        "            # Mouvement du Bobail\n",
        "            valid_mask_bobail = get_valid_mask_bobail(env)\n",
        "            q_values = agent_bobail_ddqn.predict(state)\n",
        "            best_idx = None\n",
        "            best_q = -float('inf')\n",
        "            for idx, valid in enumerate(valid_mask_bobail):\n",
        "                if valid and q_values[idx] > best_q:\n",
        "                    best_q = q_values[idx]\n",
        "                    best_idx = idx\n",
        "            if best_idx is None:\n",
        "                break\n",
        "            chosen_bobail_move = bobail_action_space[best_idx]\n",
        "            try:\n",
        "                env.move_bobail(chosen_bobail_move)\n",
        "            except Exception as e:\n",
        "                break\n",
        "            state = encode_state(env)\n",
        "            steps += 1\n",
        "            if env.done:\n",
        "                break\n",
        "            # Mouvement du Pawn\n",
        "            valid_mask_pawn = get_valid_mask_pawn(env)\n",
        "            q_values = agent_pawn_ddqn.predict(state)\n",
        "            best_idx = None\n",
        "            best_q = -float('inf')\n",
        "            for idx, valid in enumerate(valid_mask_pawn):\n",
        "                if valid and q_values[idx] > best_q:\n",
        "                    best_q = q_values[idx]\n",
        "                    best_idx = idx\n",
        "            if best_idx is None:\n",
        "                break\n",
        "            chosen_pawn_action = pawn_action_space[best_idx]\n",
        "            try:\n",
        "                env.move_pawn((chosen_pawn_action[0], chosen_pawn_action[1]), chosen_pawn_action[2])\n",
        "            except Exception as e:\n",
        "                break\n",
        "            state = encode_state(env)\n",
        "            steps += 1\n",
        "            env.current_player = 3 - env.current_player\n",
        "    return env.winner, steps\n",
        "\n",
        "def evaluate_ddqn_policy(agent_bobail, agent_pawn, num_episodes):\n",
        "    \"\"\"\n",
        "    Évalue la politique apprise par les agents DDQN sur 'num_episodes' parties.\n",
        "    Affiche le taux de victoire pour le joueur 1 et le joueur 2 ainsi que la longueur moyenne d'une partie.\n",
        "    \"\"\"\n",
        "    wins = {1: 0, 2: 0}\n",
        "    total_steps = 0\n",
        "    env = BobailEnv()\n",
        "\n",
        "    for _ in range(num_episodes):\n",
        "        winner, steps = run_ddqn_evaluation_episode(env, agent_bobail, agent_pawn)\n",
        "        if winner in wins:\n",
        "            wins[winner] += 1\n",
        "        total_steps += steps\n",
        "\n",
        "    avg_steps = total_steps / num_episodes\n",
        "    win_rate_1 = wins[1] / num_episodes * 100\n",
        "    win_rate_2 = wins[2] / num_episodes * 100\n",
        "\n",
        "    print(f\"Évaluation sur {num_episodes} épisodes :\")\n",
        "    print(f\"  Taux de victoire Joueur 1 : {win_rate_1:.2f}%\")\n",
        "    print(f\"  Taux de victoire Joueur 2 : {win_rate_2:.2f}%\")\n",
        "    print(f\"  Longueur moyenne d'une partie : {avg_steps:.2f} coups\")\n",
        "\n",
        "# ----- Lancement de l'entraînement et de l'évaluation -----\n",
        "# Par exemple, entraînement sur 500 épisodes, puis évaluation sur 1000 épisodes.\n",
        "losses = train_ddqn(500)\n",
        "print(\"Entraînement Double DQN terminé.\")\n",
        "\n",
        "evaluate_ddqn_policy(agent_bobail_ddqn, agent_pawn_ddqn, 1000)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vQJd_2tfwVWY",
        "outputId": "f7a14790-86b7-403f-eccc-6723816635ca"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 0 terminé, perte totale = 1.0306\n",
            "Episode 100 terminé, perte totale = 0.0329\n",
            "Episode 200 terminé, perte totale = 0.1824\n",
            "Episode 300 terminé, perte totale = 0.0170\n",
            "Episode 400 terminé, perte totale = 0.0048\n",
            "Entraînement Double DQN terminé.\n",
            "Évaluation sur 1000 épisodes :\n",
            "  Taux de victoire Joueur 1 : 100.00%\n",
            "  Taux de victoire Joueur 2 : 0.00%\n",
            "  Longueur moyenne d'une partie : 4.00 coups\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "DoubleDeepQLearningWithExperienceReplay\n"
      ],
      "metadata": {
        "id": "h4aDfm3FwaNa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import random\n",
        "import copy\n",
        "from collections import deque\n",
        "\n",
        "# ----- Définition du réseau DQN -----\n",
        "class DQN(nn.Module):\n",
        "    def __init__(self, input_size, output_size, hidden_size=64):\n",
        "        super(DQN, self).__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(input_size, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size, output_size)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "# ----- Replay Buffer -----\n",
        "class ReplayBuffer:\n",
        "    def __init__(self, capacity):\n",
        "        self.buffer = deque(maxlen=capacity)\n",
        "\n",
        "    def push(self, state, action_index, reward, next_state, done, next_valid_mask):\n",
        "        self.buffer.append((state, action_index, reward, next_state, done, next_valid_mask))\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        batch = random.sample(self.buffer, batch_size)\n",
        "        state, action_index, reward, next_state, done, next_valid_mask = map(np.array, zip(*batch))\n",
        "        return state, action_index, reward, next_state, done, next_valid_mask\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.buffer)\n",
        "\n",
        "# ----- Double DQN Agent With Experience Replay -----\n",
        "class DoubleDQNAgentER:\n",
        "    def __init__(self, state_size, action_space, lr=1e-3, gamma=0.99, epsilon=0.1,\n",
        "                 target_update_freq=100, replay_buffer_capacity=10000, batch_size=32):\n",
        "        self.state_size = state_size\n",
        "        self.action_space = action_space  # liste des actions possibles\n",
        "        self.action_size = len(action_space)\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon\n",
        "        self.target_update_freq = target_update_freq\n",
        "        self.batch_size = batch_size\n",
        "        self.step_count = 0\n",
        "\n",
        "        # Réseau principal et target network\n",
        "        self.model = DQN(state_size, self.action_size)\n",
        "        self.target_model = DQN(state_size, self.action_size)\n",
        "        self.target_model.load_state_dict(self.model.state_dict())\n",
        "\n",
        "        self.optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
        "        self.loss_fn = nn.MSELoss()\n",
        "\n",
        "        # Replay Buffer\n",
        "        self.replay_buffer = ReplayBuffer(replay_buffer_capacity)\n",
        "\n",
        "    def state_to_tensor(self, state):\n",
        "        state_array = np.array(state, dtype=np.float32)\n",
        "        return torch.tensor(state_array, dtype=torch.float32).unsqueeze(0)\n",
        "\n",
        "    def predict(self, state):\n",
        "        state_tensor = self.state_to_tensor(state)\n",
        "        with torch.no_grad():\n",
        "            q_values = self.model(state_tensor).squeeze(0).numpy()\n",
        "        return q_values\n",
        "\n",
        "    def select_action(self, state, valid_actions_mask):\n",
        "        \"\"\"\n",
        "        valid_actions_mask : numpy array de forme (action_size,) avec True pour les actions valides.\n",
        "        Retourne l'indice de l'action sélectionnée.\n",
        "        \"\"\"\n",
        "        if np.random.rand() < self.epsilon:\n",
        "            valid_indices = np.where(valid_actions_mask)[0]\n",
        "            if len(valid_indices) == 0:\n",
        "                return None\n",
        "            return np.random.choice(valid_indices)\n",
        "        q_values = self.predict(state)\n",
        "        masked_q = np.where(valid_actions_mask, q_values, -np.inf)\n",
        "        return int(np.argmax(masked_q))\n",
        "\n",
        "    def update_from_batch(self):\n",
        "        if len(self.replay_buffer) < self.batch_size:\n",
        "            return 0.0\n",
        "        states, actions, rewards, next_states, dones, next_valid_masks = self.replay_buffer.sample(self.batch_size)\n",
        "\n",
        "        # Conversion en tenseurs\n",
        "        states_tensor = torch.tensor(np.array(states), dtype=torch.float32)\n",
        "        actions_tensor = torch.tensor(actions, dtype=torch.long)\n",
        "        rewards_tensor = torch.tensor(rewards, dtype=torch.float32)\n",
        "        next_states_tensor = torch.tensor(np.array(next_states), dtype=torch.float32)\n",
        "        dones_tensor = torch.tensor(dones, dtype=torch.float32)\n",
        "        # next_valid_masks est un tableau numpy de booléens de forme (batch_size, action_size)\n",
        "        next_valid_masks_tensor = torch.tensor(next_valid_masks, dtype=torch.bool)\n",
        "\n",
        "        # Q-values pour les états actuels\n",
        "        q_values = self.model(states_tensor)  # forme (batch_size, action_size)\n",
        "        current_q = q_values.gather(1, actions_tensor.unsqueeze(1)).squeeze(1)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            next_q_values = self.model(next_states_tensor)  # forme (batch_size, action_size)\n",
        "            # On masque les actions invalides\n",
        "            masked_next_q = next_q_values.clone()\n",
        "            masked_next_q[~next_valid_masks_tensor] = -1e9\n",
        "            best_next_actions = masked_next_q.argmax(dim=1)\n",
        "\n",
        "            target_q_values = self.target_model(next_states_tensor)\n",
        "            best_next_q = target_q_values.gather(1, best_next_actions.unsqueeze(1)).squeeze(1)\n",
        "\n",
        "            target = rewards_tensor + (1 - dones_tensor) * self.gamma * best_next_q\n",
        "\n",
        "        loss = self.loss_fn(current_q, target)\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        self.step_count += 1\n",
        "        if self.step_count % self.target_update_freq == 0:\n",
        "            self.target_model.load_state_dict(self.model.state_dict())\n",
        "\n",
        "        return loss.item()\n",
        "\n",
        "    def store_experience(self, state, action_index, reward, next_state, done, next_valid_mask):\n",
        "        self.replay_buffer.push(state, action_index, reward, next_state, done, next_valid_mask)\n",
        "\n",
        "# ----- Helpers pour l'encodage d'état et le masquage des actions -----\n",
        "\n",
        "def encode_state(env):\n",
        "    \"\"\"\n",
        "    Encode l'état en un vecteur de taille 27 :\n",
        "      - 25 valeurs correspondant au plateau (aplati)\n",
        "      - 1 valeur pour le joueur courant (0 pour le joueur 1, 1 pour le joueur 2)\n",
        "      - 1 valeur indiquant si c'est le premier tour (1 si True, 0 sinon)\n",
        "    \"\"\"\n",
        "    board_flat = env.board.flatten().astype(np.float32)\n",
        "    current_player = np.array([0.0 if env.current_player == 1 else 1.0], dtype=np.float32)\n",
        "    first_turn = np.array([1.0 if env.first_turn else 0.0], dtype=np.float32)\n",
        "    return tuple(np.concatenate([board_flat, current_player, first_turn]))\n",
        "\n",
        "# Définition des espaces d'actions\n",
        "directions_list = [(-1,-1), (-1,0), (-1,1), (0,-1), (0,1), (1,-1), (1,0), (1,1)]\n",
        "bobail_action_space = directions_list  # 8 actions\n",
        "\n",
        "pawn_action_space = []\n",
        "for i in range(5):\n",
        "    for j in range(5):\n",
        "        for d in directions_list:\n",
        "            pawn_action_space.append((i, j, d))\n",
        "\n",
        "def get_valid_mask_bobail(env):\n",
        "    \"\"\"\n",
        "    Renvoie un masque booléen de taille 8 pour l'agent Bobail.\n",
        "    Chaque élément vaut True si le mouvement correspondant est valide dans l'état actuel.\n",
        "    \"\"\"\n",
        "    valid_moves = env.get_valid_bobail_moves()\n",
        "    mask = np.array([move in valid_moves for move in bobail_action_space], dtype=bool)\n",
        "    return mask\n",
        "\n",
        "def get_valid_mask_pawn(env):\n",
        "    \"\"\"\n",
        "    Renvoie un masque booléen de taille len(pawn_action_space) pour l'agent Pawn.\n",
        "    Pour chaque action candidate (i, j, d), on vérifie que :\n",
        "      - La case (i, j) contient un pion du joueur courant.\n",
        "      - La direction d figure parmi les mouvements valides pour ce pion.\n",
        "    \"\"\"\n",
        "    mask = np.zeros(len(pawn_action_space), dtype=bool)\n",
        "    for idx, (i, j, d) in enumerate(pawn_action_space):\n",
        "        if env.board[i, j] == env.current_player:\n",
        "            valid_moves = env.get_valid_pawn_moves((i, j))\n",
        "            valid_dirs = [move[0] for move in valid_moves]\n",
        "            if d in valid_dirs:\n",
        "                mask[idx] = True\n",
        "    return mask\n",
        "\n",
        "# Taille de l'état (25 cases + 1 joueur + 1 first_turn) = 27\n",
        "STATE_SIZE = 27\n",
        "\n",
        "# Création des agents Double DQN with Experience Replay pour le Bobail et les pions\n",
        "agent_bobail_ddqn_er = DoubleDQNAgentER(state_size=STATE_SIZE, action_space=bobail_action_space, lr=1e-3, gamma=0.99, epsilon=0.1, target_update_freq=100, replay_buffer_capacity=10000, batch_size=32)\n",
        "agent_pawn_ddqn_er = DoubleDQNAgentER(state_size=STATE_SIZE, action_space=pawn_action_space, lr=1e-3, gamma=0.99, epsilon=0.1, target_update_freq=100, replay_buffer_capacity=10000, batch_size=32)\n"
      ],
      "metadata": {
        "id": "wDkDutnnwbPH"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_ddqn_er(num_episodes):\n",
        "    \"\"\"\n",
        "    Entraîne les agents Double DQN avec Experience Replay sur 'num_episodes' parties.\n",
        "    À chaque action, l'expérience est stockée dans le replay buffer et une mise à jour en batch est effectuée.\n",
        "    \"\"\"\n",
        "    env = BobailEnv()\n",
        "    episode_losses = []\n",
        "\n",
        "    for ep in range(num_episodes):\n",
        "        env.reset()\n",
        "        total_loss = 0.0\n",
        "        state = encode_state(env)\n",
        "\n",
        "        while not env.done:\n",
        "            if env.first_turn:\n",
        "                valid_mask_pawn = get_valid_mask_pawn(env)\n",
        "                action_idx = agent_pawn_ddqn_er.select_action(state, valid_mask_pawn)\n",
        "                if action_idx is None:\n",
        "                    break\n",
        "                chosen_action = pawn_action_space[action_idx]\n",
        "                try:\n",
        "                    env.step(None, (chosen_action[0], chosen_action[1]), chosen_action[2])\n",
        "                except Exception as e:\n",
        "                    reward = -1.0\n",
        "                    agent_pawn_ddqn_er.store_experience(state, action_idx, reward, state, True, valid_mask_pawn)\n",
        "                    break\n",
        "                reward = -0.01\n",
        "                next_state = encode_state(env)\n",
        "                next_valid_mask = get_valid_mask_pawn(env)\n",
        "                agent_pawn_ddqn_er.store_experience(state, action_idx, reward, next_state, env.done, next_valid_mask)\n",
        "                loss = agent_pawn_ddqn_er.update_from_batch()\n",
        "                total_loss += loss\n",
        "                state = next_state\n",
        "            else:\n",
        "                # Mouvement du Bobail\n",
        "                valid_mask_bobail = get_valid_mask_bobail(env)\n",
        "                action_idx_bobail = agent_bobail_ddqn_er.select_action(state, valid_mask_bobail)\n",
        "                if action_idx_bobail is None:\n",
        "                    break\n",
        "                chosen_bobail_move = bobail_action_space[action_idx_bobail]\n",
        "                try:\n",
        "                    env.move_bobail(chosen_bobail_move)\n",
        "                except Exception as e:\n",
        "                    reward = -1.0\n",
        "                    agent_bobail_ddqn_er.store_experience(state, action_idx_bobail, reward, state, True, valid_mask_bobail)\n",
        "                    break\n",
        "                reward = -0.01\n",
        "                next_state = encode_state(env)\n",
        "                next_valid_mask_bobail = get_valid_mask_bobail(env)\n",
        "                agent_bobail_ddqn_er.store_experience(state, action_idx_bobail, reward, next_state, env.done, next_valid_mask_bobail)\n",
        "                loss_bobail = agent_bobail_ddqn_er.update_from_batch()\n",
        "                total_loss += loss_bobail\n",
        "                state = next_state\n",
        "\n",
        "                if env.done:\n",
        "                    break\n",
        "\n",
        "                # Mouvement du Pawn\n",
        "                valid_mask_pawn = get_valid_mask_pawn(env)\n",
        "                action_idx_pawn = agent_pawn_ddqn_er.select_action(state, valid_mask_pawn)\n",
        "                if action_idx_pawn is None:\n",
        "                    break\n",
        "                chosen_pawn_action = pawn_action_space[action_idx_pawn]\n",
        "                try:\n",
        "                    env.move_pawn((chosen_pawn_action[0], chosen_pawn_action[1]), chosen_pawn_action[2])\n",
        "                except Exception as e:\n",
        "                    reward = -1.0\n",
        "                    agent_pawn_ddqn_er.store_experience(state, action_idx_pawn, reward, state, True, valid_mask_pawn)\n",
        "                    break\n",
        "                reward = -0.01\n",
        "                next_state = encode_state(env)\n",
        "                next_valid_mask_pawn = get_valid_mask_pawn(env)\n",
        "                agent_pawn_ddqn_er.store_experience(state, action_idx_pawn, reward, next_state, env.done, next_valid_mask_pawn)\n",
        "                loss_pawn = agent_pawn_ddqn_er.update_from_batch()\n",
        "                total_loss += loss_pawn\n",
        "                state = next_state\n",
        "\n",
        "                env.current_player = 3 - env.current_player\n",
        "        episode_losses.append(total_loss)\n",
        "        if ep % 100 == 0:\n",
        "            print(f\"Episode {ep} terminé, perte totale = {total_loss:.4f}\")\n",
        "    return episode_losses\n",
        "\n",
        "def run_ddqn_er_evaluation_episode(env, agent_bobail, agent_pawn):\n",
        "    \"\"\"\n",
        "    Exécute une partie en mode évaluation (sans exploration) en utilisant les agents DDQN avec Experience Replay.\n",
        "    \"\"\"\n",
        "    env.reset()\n",
        "    state = encode_state(env)\n",
        "    steps = 0\n",
        "\n",
        "    while not env.done:\n",
        "        if env.first_turn:\n",
        "            valid_mask_pawn = get_valid_mask_pawn(env)\n",
        "            q_values = agent_pawn.predict(state)\n",
        "            best_idx = None\n",
        "            best_q = -float('inf')\n",
        "            for idx, valid in enumerate(valid_mask_pawn):\n",
        "                if valid and q_values[idx] > best_q:\n",
        "                    best_q = q_values[idx]\n",
        "                    best_idx = idx\n",
        "            if best_idx is None:\n",
        "                break\n",
        "            chosen_action = pawn_action_space[best_idx]\n",
        "            try:\n",
        "                env.step(None, (chosen_action[0], chosen_action[1]), chosen_action[2])\n",
        "            except Exception as e:\n",
        "                break\n",
        "            state = encode_state(env)\n",
        "            steps += 1\n",
        "        else:\n",
        "            valid_mask_bobail = get_valid_mask_bobail(env)\n",
        "            q_values = agent_bobail.predict(state)\n",
        "            best_idx = None\n",
        "            best_q = -float('inf')\n",
        "            for idx, valid in enumerate(valid_mask_bobail):\n",
        "                if valid and q_values[idx] > best_q:\n",
        "                    best_q = q_values[idx]\n",
        "                    best_idx = idx\n",
        "            if best_idx is None:\n",
        "                break\n",
        "            chosen_bobail_move = bobail_action_space[best_idx]\n",
        "            try:\n",
        "                env.move_bobail(chosen_bobail_move)\n",
        "            except Exception as e:\n",
        "                break\n",
        "            state = encode_state(env)\n",
        "            steps += 1\n",
        "            if env.done:\n",
        "                break\n",
        "            valid_mask_pawn = get_valid_mask_pawn(env)\n",
        "            q_values = agent_pawn.predict(state)\n",
        "            best_idx = None\n",
        "            best_q = -float('inf')\n",
        "            for idx, valid in enumerate(valid_mask_pawn):\n",
        "                if valid and q_values[idx] > best_q:\n",
        "                    best_q = q_values[idx]\n",
        "                    best_idx = idx\n",
        "            if best_idx is None:\n",
        "                break\n",
        "            chosen_pawn_action = pawn_action_space[best_idx]\n",
        "            try:\n",
        "                env.move_pawn((chosen_pawn_action[0], chosen_pawn_action[1]), chosen_pawn_action[2])\n",
        "            except Exception as e:\n",
        "                break\n",
        "            state = encode_state(env)\n",
        "            steps += 1\n",
        "            env.current_player = 3 - env.current_player\n",
        "    return env.winner, steps\n",
        "\n",
        "def evaluate_ddqn_er_policy(agent_bobail, agent_pawn, num_episodes):\n",
        "    wins = {1: 0, 2: 0}\n",
        "    total_steps = 0\n",
        "    env = BobailEnv()\n",
        "    for _ in range(num_episodes):\n",
        "        winner, steps = run_ddqn_er_evaluation_episode(env, agent_bobail, agent_pawn)\n",
        "        if winner in wins:\n",
        "            wins[winner] += 1\n",
        "        total_steps += steps\n",
        "    avg_steps = total_steps / num_episodes\n",
        "    win_rate_1 = wins[1] / num_episodes * 100\n",
        "    win_rate_2 = wins[2] / num_episodes * 100\n",
        "    print(f\"Évaluation sur {num_episodes} épisodes :\")\n",
        "    print(f\"  Taux de victoire Joueur 1 : {win_rate_1:.2f}%\")\n",
        "    print(f\"  Taux de victoire Joueur 2 : {win_rate_2:.2f}%\")\n",
        "    print(f\"  Longueur moyenne d'une partie : {avg_steps:.2f} coups\")\n",
        "\n",
        "# Lancement de l'entraînement et de l'évaluation\n",
        "losses_er = train_ddqn_er(500)\n",
        "print(\"Entraînement Double DQN avec Experience Replay terminé.\")\n",
        "\n",
        "evaluate_ddqn_er_policy(agent_bobail_ddqn_er, agent_pawn_ddqn_er, 1000)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "twMhRYSHwhDN",
        "outputId": "266558c9-e648-4fec-ee81-2cda1e4ebcaf"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 0 terminé, perte totale = 0.0000\n",
            "Episode 100 terminé, perte totale = 0.0004\n",
            "Episode 200 terminé, perte totale = 0.0017\n",
            "Episode 300 terminé, perte totale = 0.0032\n",
            "Episode 400 terminé, perte totale = 0.0039\n",
            "Entraînement Double DQN avec Experience Replay terminé.\n",
            "Évaluation sur 1000 épisodes :\n",
            "  Taux de victoire Joueur 1 : 100.00%\n",
            "  Taux de victoire Joueur 2 : 0.00%\n",
            "  Longueur moyenne d'une partie : 4.00 coups\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "DoubleDeepQLearningWithPrioritizedExperienceReplay"
      ],
      "metadata": {
        "id": "3zbwJVD2wlMP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import random\n",
        "import copy\n",
        "\n",
        "# ----- Définition du réseau DQN -----\n",
        "class DQN(nn.Module):\n",
        "    def __init__(self, input_size, output_size, hidden_size=64):\n",
        "        super(DQN, self).__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(input_size, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size, output_size)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "# ----- Prioritized Replay Buffer -----\n",
        "class PrioritizedReplayBuffer:\n",
        "    def __init__(self, capacity, alpha=0.6):\n",
        "        self.capacity = capacity\n",
        "        self.buffer = []\n",
        "        self.priorities = []\n",
        "        self.alpha = alpha\n",
        "        self.pos = 0\n",
        "\n",
        "    def push(self, state, action_index, reward, next_state, done, next_valid_mask):\n",
        "        max_priority = max(self.priorities) if self.buffer else 1.0\n",
        "        experience = (state, action_index, reward, next_state, done, next_valid_mask)\n",
        "        if len(self.buffer) < self.capacity:\n",
        "            self.buffer.append(experience)\n",
        "            self.priorities.append(max_priority)\n",
        "        else:\n",
        "            self.buffer[self.pos] = experience\n",
        "            self.priorities[self.pos] = max_priority\n",
        "            self.pos = (self.pos + 1) % self.capacity\n",
        "\n",
        "    def sample(self, batch_size, beta=0.4):\n",
        "        if len(self.buffer) == self.capacity:\n",
        "            prios = np.array(self.priorities)\n",
        "        else:\n",
        "            prios = np.array(self.priorities[:len(self.buffer)])\n",
        "        probabilities = prios ** self.alpha\n",
        "        probabilities /= probabilities.sum()\n",
        "        indices = np.random.choice(len(self.buffer), batch_size, p=probabilities)\n",
        "        samples = [self.buffer[i] for i in indices]\n",
        "        total = len(self.buffer)\n",
        "        # Calcul des poids d'importance (importance-sampling weights)\n",
        "        weights = (total * probabilities[indices]) ** (-beta)\n",
        "        weights /= weights.max()  # Normalisation\n",
        "        weights = np.array(weights, dtype=np.float32)\n",
        "        return samples, indices, weights\n",
        "\n",
        "    def update_priorities(self, indices, priorities):\n",
        "        for idx, priority in zip(indices, priorities):\n",
        "            self.priorities[idx] = priority\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.buffer)\n",
        "\n",
        "# ----- Agent Double DQN avec Prioritized Experience Replay -----\n",
        "class DoubleDQNAgentPER:\n",
        "    def __init__(self, state_size, action_space, lr=1e-3, gamma=0.99, epsilon=0.1,\n",
        "                 target_update_freq=100, replay_buffer_capacity=10000, batch_size=32):\n",
        "        self.state_size = state_size\n",
        "        self.action_space = action_space  # liste des actions possibles\n",
        "        self.action_size = len(action_space)\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon\n",
        "        self.target_update_freq = target_update_freq\n",
        "        self.batch_size = batch_size\n",
        "        self.step_count = 0\n",
        "\n",
        "        self.model = DQN(state_size, self.action_size)\n",
        "        self.target_model = DQN(state_size, self.action_size)\n",
        "        self.target_model.load_state_dict(self.model.state_dict())\n",
        "\n",
        "        self.optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
        "        self.loss_fn = nn.MSELoss(reduction='none')  # On récupère la perte pour chaque élément\n",
        "\n",
        "        self.replay_buffer = PrioritizedReplayBuffer(replay_buffer_capacity, alpha=0.6)\n",
        "\n",
        "    def state_to_tensor(self, state):\n",
        "        state_array = np.array(state, dtype=np.float32)\n",
        "        return torch.tensor(state_array, dtype=torch.float32).unsqueeze(0)\n",
        "\n",
        "    def predict(self, state):\n",
        "        state_tensor = self.state_to_tensor(state)\n",
        "        with torch.no_grad():\n",
        "            q_values = self.model(state_tensor).squeeze(0).numpy()\n",
        "        return q_values\n",
        "\n",
        "    def select_action(self, state, valid_actions_mask):\n",
        "        if np.random.rand() < self.epsilon:\n",
        "            valid_indices = np.where(valid_actions_mask)[0]\n",
        "            if len(valid_indices) == 0:\n",
        "                return None\n",
        "            return np.random.choice(valid_indices)\n",
        "        q_values = self.predict(state)\n",
        "        masked_q = np.where(valid_actions_mask, q_values, -np.inf)\n",
        "        return int(np.argmax(masked_q))\n",
        "\n",
        "    def update_from_batch(self, beta=0.4):\n",
        "        if len(self.replay_buffer) < self.batch_size:\n",
        "            return 0.0\n",
        "        samples, indices, is_weights = self.replay_buffer.sample(self.batch_size, beta=beta)\n",
        "        states, actions, rewards, next_states, dones, next_valid_masks = map(np.array, zip(*samples))\n",
        "\n",
        "        states_tensor = torch.tensor(states, dtype=torch.float32)\n",
        "        actions_tensor = torch.tensor(actions, dtype=torch.long)\n",
        "        rewards_tensor = torch.tensor(rewards, dtype=torch.float32)\n",
        "        next_states_tensor = torch.tensor(next_states, dtype=torch.float32)\n",
        "        dones_tensor = torch.tensor(dones, dtype=torch.float32)\n",
        "        is_weights_tensor = torch.tensor(is_weights, dtype=torch.float32)\n",
        "        next_valid_masks_tensor = torch.tensor(next_valid_masks, dtype=torch.bool)\n",
        "\n",
        "        q_values = self.model(states_tensor)  # (batch_size, action_size)\n",
        "        current_q = q_values.gather(1, actions_tensor.unsqueeze(1)).squeeze(1)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            next_q_values = self.model(next_states_tensor)  # (batch_size, action_size)\n",
        "            masked_next_q = next_q_values.clone()\n",
        "            masked_next_q[~next_valid_masks_tensor] = -1e9\n",
        "            best_next_actions = masked_next_q.argmax(dim=1)\n",
        "\n",
        "            target_q_values = self.target_model(next_states_tensor)\n",
        "            best_next_q = target_q_values.gather(1, best_next_actions.unsqueeze(1)).squeeze(1)\n",
        "\n",
        "            target = rewards_tensor + (1 - dones_tensor) * self.gamma * best_next_q\n",
        "\n",
        "        # Calcul de la perte pour chaque échantillon\n",
        "        loss_all = self.loss_fn(current_q, target)\n",
        "        # On pondère chaque perte avec le poids d'importance\n",
        "        loss_weighted = loss_all * is_weights_tensor\n",
        "        loss = loss_weighted.mean()\n",
        "\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        self.step_count += 1\n",
        "        if self.step_count % self.target_update_freq == 0:\n",
        "            self.target_model.load_state_dict(self.model.state_dict())\n",
        "\n",
        "        # Mise à jour des priorités dans le buffer : utiliser l'erreur absolue + une petite constante\n",
        "        new_priorities = loss_all.detach().cpu().numpy() + 1e-6\n",
        "        self.replay_buffer.update_priorities(indices, new_priorities)\n",
        "\n",
        "        return loss.item()\n",
        "\n",
        "    def store_experience(self, state, action_index, reward, next_state, done, next_valid_mask):\n",
        "        self.replay_buffer.push(state, action_index, reward, next_state, done, next_valid_mask)\n",
        "\n",
        "# ----- Helpers pour l'encodage d'état et les masques d'actions -----\n",
        "\n",
        "def encode_state(env):\n",
        "    \"\"\"\n",
        "    Encode l'état en un vecteur de taille 27 :\n",
        "      - 25 valeurs correspondant au plateau (aplati)\n",
        "      - 1 valeur pour le joueur courant (0 pour joueur 1, 1 pour joueur 2)\n",
        "      - 1 valeur indiquant si c'est le premier tour (1 si True, 0 sinon)\n",
        "    \"\"\"\n",
        "    board_flat = env.board.flatten().astype(np.float32)\n",
        "    current_player = np.array([0.0 if env.current_player == 1 else 1.0], dtype=np.float32)\n",
        "    first_turn = np.array([1.0 if env.first_turn else 0.0], dtype=np.float32)\n",
        "    return tuple(np.concatenate([board_flat, current_player, first_turn]))\n",
        "\n",
        "# Définition des espaces d'actions\n",
        "directions_list = [(-1,-1), (-1,0), (-1,1), (0,-1), (0,1), (1,-1), (1,0), (1,1)]\n",
        "bobail_action_space = directions_list  # 8 actions\n",
        "\n",
        "pawn_action_space = []\n",
        "for i in range(5):\n",
        "    for j in range(5):\n",
        "        for d in directions_list:\n",
        "            pawn_action_space.append((i, j, d))\n",
        "\n",
        "def get_valid_mask_bobail(env):\n",
        "    valid_moves = env.get_valid_bobail_moves()\n",
        "    mask = np.array([move in valid_moves for move in bobail_action_space], dtype=bool)\n",
        "    return mask\n",
        "\n",
        "def get_valid_mask_pawn(env):\n",
        "    mask = np.zeros(len(pawn_action_space), dtype=bool)\n",
        "    for idx, (i, j, d) in enumerate(pawn_action_space):\n",
        "        if env.board[i, j] == env.current_player:\n",
        "            valid_moves = env.get_valid_pawn_moves((i, j))\n",
        "            valid_dirs = [move[0] for move in valid_moves]\n",
        "            if d in valid_dirs:\n",
        "                mask[idx] = True\n",
        "    return mask\n",
        "\n",
        "STATE_SIZE = 27\n",
        "\n",
        "# Création des agents Double DQN avec PER pour le Bobail et les pions\n",
        "agent_bobail_ddqn_per = DoubleDQNAgentPER(state_size=STATE_SIZE,\n",
        "                                          action_space=bobail_action_space,\n",
        "                                          lr=1e-3, gamma=0.99, epsilon=0.1,\n",
        "                                          target_update_freq=100, replay_buffer_capacity=10000, batch_size=32)\n",
        "agent_pawn_ddqn_per = DoubleDQNAgentPER(state_size=STATE_SIZE,\n",
        "                                        action_space=pawn_action_space,\n",
        "                                        lr=1e-3, gamma=0.99, epsilon=0.1,\n",
        "                                        target_update_freq=100, replay_buffer_capacity=10000, batch_size=32)\n"
      ],
      "metadata": {
        "id": "RJpBDdpvwluP"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_ddqn_per(num_episodes):\n",
        "    \"\"\"\n",
        "    Entraîne les agents Double DQN avec PER sur num_episodes parties.\n",
        "    Pour chaque action, l'expérience est stockée dans le buffer priorisé et\n",
        "    une mise à jour par mini-batch est effectuée.\n",
        "    \"\"\"\n",
        "    env = BobailEnv()\n",
        "    episode_losses = []\n",
        "\n",
        "    for ep in range(num_episodes):\n",
        "        env.reset()\n",
        "        total_loss = 0.0\n",
        "        state = encode_state(env)\n",
        "\n",
        "        while not env.done:\n",
        "            if env.first_turn:\n",
        "                valid_mask_pawn = get_valid_mask_pawn(env)\n",
        "                action_idx = agent_pawn_ddqn_per.select_action(state, valid_mask_pawn)\n",
        "                if action_idx is None:\n",
        "                    break\n",
        "                chosen_action = pawn_action_space[action_idx]\n",
        "                try:\n",
        "                    env.step(None, (chosen_action[0], chosen_action[1]), chosen_action[2])\n",
        "                except Exception as e:\n",
        "                    reward = -1.0\n",
        "                    agent_pawn_ddqn_per.store_experience(state, action_idx, reward, state, True, valid_mask_pawn)\n",
        "                    break\n",
        "                reward = -0.01\n",
        "                next_state = encode_state(env)\n",
        "                next_valid_mask = get_valid_mask_pawn(env)\n",
        "                agent_pawn_ddqn_per.store_experience(state, action_idx, reward, next_state, env.done, next_valid_mask)\n",
        "                loss = agent_pawn_ddqn_per.update_from_batch()\n",
        "                total_loss += loss\n",
        "                state = next_state\n",
        "            else:\n",
        "                # Mouvement du Bobail\n",
        "                valid_mask_bobail = get_valid_mask_bobail(env)\n",
        "                action_idx_bobail = agent_bobail_ddqn_per.select_action(state, valid_mask_bobail)\n",
        "                if action_idx_bobail is None:\n",
        "                    break\n",
        "                chosen_bobail_move = bobail_action_space[action_idx_bobail]\n",
        "                try:\n",
        "                    env.move_bobail(chosen_bobail_move)\n",
        "                except Exception as e:\n",
        "                    reward = -1.0\n",
        "                    agent_bobail_ddqn_per.store_experience(state, action_idx_bobail, reward, state, True, valid_mask_bobail)\n",
        "                    break\n",
        "                reward = -0.01\n",
        "                next_state = encode_state(env)\n",
        "                next_valid_mask_bobail = get_valid_mask_bobail(env)\n",
        "                agent_bobail_ddqn_per.store_experience(state, action_idx_bobail, reward, next_state, env.done, next_valid_mask_bobail)\n",
        "                loss_bobail = agent_bobail_ddqn_per.update_from_batch()\n",
        "                total_loss += loss_bobail\n",
        "                state = next_state\n",
        "\n",
        "                if env.done:\n",
        "                    break\n",
        "\n",
        "                # Mouvement du Pawn\n",
        "                valid_mask_pawn = get_valid_mask_pawn(env)\n",
        "                action_idx_pawn = agent_pawn_ddqn_per.select_action(state, valid_mask_pawn)\n",
        "                if action_idx_pawn is None:\n",
        "                    break\n",
        "                chosen_pawn_action = pawn_action_space[action_idx_pawn]\n",
        "                try:\n",
        "                    env.move_pawn((chosen_pawn_action[0], chosen_pawn_action[1]), chosen_pawn_action[2])\n",
        "                except Exception as e:\n",
        "                    reward = -1.0\n",
        "                    agent_pawn_ddqn_per.store_experience(state, action_idx_pawn, reward, state, True, valid_mask_pawn)\n",
        "                    break\n",
        "                reward = -0.01\n",
        "                next_state = encode_state(env)\n",
        "                next_valid_mask_pawn = get_valid_mask_pawn(env)\n",
        "                agent_pawn_ddqn_per.store_experience(state, action_idx_pawn, reward, next_state, env.done, next_valid_mask_pawn)\n",
        "                loss_pawn = agent_pawn_ddqn_per.update_from_batch()\n",
        "                total_loss += loss_pawn\n",
        "                state = next_state\n",
        "\n",
        "                env.current_player = 3 - env.current_player\n",
        "        episode_losses.append(total_loss)\n",
        "        if ep % 100 == 0:\n",
        "            print(f\"Episode {ep} terminé, perte totale = {total_loss:.4f}\")\n",
        "    return episode_losses\n",
        "\n",
        "def run_ddqn_per_evaluation_episode(env, agent_bobail, agent_pawn):\n",
        "    \"\"\"\n",
        "    Exécute une partie en mode évaluation (sans exploration) en utilisant les agents DDQN avec PER.\n",
        "    \"\"\"\n",
        "    env.reset()\n",
        "    state = encode_state(env)\n",
        "    steps = 0\n",
        "    while not env.done:\n",
        "        if env.first_turn:\n",
        "            valid_mask_pawn = get_valid_mask_pawn(env)\n",
        "            q_values = agent_pawn.predict(state)\n",
        "            best_idx = None\n",
        "            best_q = -float('inf')\n",
        "            for idx, valid in enumerate(valid_mask_pawn):\n",
        "                if valid and q_values[idx] > best_q:\n",
        "                    best_q = q_values[idx]\n",
        "                    best_idx = idx\n",
        "            if best_idx is None:\n",
        "                break\n",
        "            chosen_action = pawn_action_space[best_idx]\n",
        "            try:\n",
        "                env.step(None, (chosen_action[0], chosen_action[1]), chosen_action[2])\n",
        "            except Exception as e:\n",
        "                break\n",
        "            state = encode_state(env)\n",
        "            steps += 1\n",
        "        else:\n",
        "            valid_mask_bobail = get_valid_mask_bobail(env)\n",
        "            q_values = agent_bobail.predict(state)\n",
        "            best_idx = None\n",
        "            best_q = -float('inf')\n",
        "            for idx, valid in enumerate(valid_mask_bobail):\n",
        "                if valid and q_values[idx] > best_q:\n",
        "                    best_q = q_values[idx]\n",
        "                    best_idx = idx\n",
        "            if best_idx is None:\n",
        "                break\n",
        "            chosen_bobail_move = bobail_action_space[best_idx]\n",
        "            try:\n",
        "                env.move_bobail(chosen_bobail_move)\n",
        "            except Exception as e:\n",
        "                break\n",
        "            state = encode_state(env)\n",
        "            steps += 1\n",
        "            if env.done:\n",
        "                break\n",
        "            valid_mask_pawn = get_valid_mask_pawn(env)\n",
        "            q_values = agent_pawn.predict(state)\n",
        "            best_idx = None\n",
        "            best_q = -float('inf')\n",
        "            for idx, valid in enumerate(valid_mask_pawn):\n",
        "                if valid and q_values[idx] > best_q:\n",
        "                    best_q = q_values[idx]\n",
        "                    best_idx = idx\n",
        "            if best_idx is None:\n",
        "                break\n",
        "            chosen_pawn_action = pawn_action_space[best_idx]\n",
        "            try:\n",
        "                env.move_pawn((chosen_pawn_action[0], chosen_pawn_action[1]), chosen_pawn_action[2])\n",
        "            except Exception as e:\n",
        "                break\n",
        "            state = encode_state(env)\n",
        "            steps += 1\n",
        "            env.current_player = 3 - env.current_player\n",
        "    return env.winner, steps\n",
        "\n",
        "def evaluate_ddqn_per_policy(agent_bobail, agent_pawn, num_episodes):\n",
        "    wins = {1: 0, 2: 0}\n",
        "    total_steps = 0\n",
        "    env = BobailEnv()\n",
        "    for _ in range(num_episodes):\n",
        "        winner, steps = run_ddqn_per_evaluation_episode(env, agent_bobail, agent_pawn)\n",
        "        if winner in wins:\n",
        "            wins[winner] += 1\n",
        "        total_steps += steps\n",
        "    avg_steps = total_steps / num_episodes\n",
        "    win_rate_1 = wins[1] / num_episodes * 100\n",
        "    win_rate_2 = wins[2] / num_episodes * 100\n",
        "    print(f\"Évaluation sur {num_episodes} épisodes :\")\n",
        "    print(f\"  Taux de victoire Joueur 1 : {win_rate_1:.2f}%\")\n",
        "    print(f\"  Taux de victoire Joueur 2 : {win_rate_2:.2f}%\")\n",
        "    print(f\"  Longueur moyenne d'une partie : {avg_steps:.2f} coups\")\n",
        "\n",
        "# Lancement de l'entraînement et de l'évaluation\n",
        "losses_per = train_ddqn_per(500)\n",
        "print(\"Entraînement Double DQN avec Prioritized Experience Replay terminé.\")\n",
        "\n",
        "evaluate_ddqn_per_policy(agent_bobail_ddqn_per, agent_pawn_ddqn_per, 1000)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Irm8j2xdwpYq",
        "outputId": "5ad5cba1-e907-4595-9ae6-840792da15c0"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 0 terminé, perte totale = 0.0000\n",
            "Episode 100 terminé, perte totale = 0.0003\n",
            "Episode 200 terminé, perte totale = 0.0004\n",
            "Episode 300 terminé, perte totale = 0.0018\n",
            "Episode 400 terminé, perte totale = 0.0029\n",
            "Entraînement Double DQN avec Prioritized Experience Replay terminé.\n",
            "Évaluation sur 1000 épisodes :\n",
            "  Taux de victoire Joueur 1 : 100.00%\n",
            "  Taux de victoire Joueur 2 : 0.00%\n",
            "  Longueur moyenne d'une partie : 4.00 coups\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "REINFORCE"
      ],
      "metadata": {
        "id": "9CzBtA_Gwvb3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "# Réseau de politique pour les pions\n",
        "class PawnPolicy(nn.Module):\n",
        "    def __init__(self, state_size, action_size, hidden_size=64):\n",
        "        super(PawnPolicy, self).__init__()\n",
        "        self.fc1 = nn.Linear(state_size, hidden_size)\n",
        "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.fc3 = nn.Linear(hidden_size, action_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        logits = self.fc3(x)\n",
        "        return logits\n",
        "\n",
        "# Réseau de politique pour le Bobail\n",
        "class BobailPolicy(nn.Module):\n",
        "    def __init__(self, state_size, action_size, hidden_size=64):\n",
        "        super(BobailPolicy, self).__init__()\n",
        "        self.fc1 = nn.Linear(state_size, hidden_size)\n",
        "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.fc3 = nn.Linear(hidden_size, action_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        logits = self.fc3(x)\n",
        "        return logits\n",
        "\n",
        "# Paramètres\n",
        "state_size = 27  # 25 cases du plateau + 1 pour le joueur courant + 1 pour first_turn\n",
        "pawn_action_size = len(pawn_action_space)    # Par exemple, 200 actions pour les pions\n",
        "bobail_action_size = len(bobail_action_space)  # 8 actions pour le Bobail\n",
        "\n",
        "# Instanciation des réseaux\n",
        "pawn_policy = PawnPolicy(state_size, pawn_action_size)\n",
        "bobail_policy = BobailPolicy(state_size, bobail_action_size)\n",
        "\n",
        "# Optimiseurs\n",
        "pawn_optimizer = optim.Adam(pawn_policy.parameters(), lr=1e-3)\n",
        "bobail_optimizer = optim.Adam(bobail_policy.parameters(), lr=1e-3)\n",
        "\n",
        "# Fonction de sélection d'action avec masquage\n",
        "def select_action(policy, state_tensor, valid_mask):\n",
        "    \"\"\"\n",
        "    state_tensor : tenseur de forme (1, state_size)\n",
        "    valid_mask : array numpy booléen de forme (action_size,)\n",
        "    Renvoie (action_index, log_prob)\n",
        "    \"\"\"\n",
        "    logits = policy(state_tensor)  # (1, action_size)\n",
        "    valid_mask_tensor = torch.tensor(valid_mask, dtype=torch.bool, device=logits.device)\n",
        "    # Fixer les logits invalides à -infinity\n",
        "    logits[0][~valid_mask_tensor] = -float('inf')\n",
        "    probs = F.softmax(logits, dim=1)\n",
        "    m = torch.distributions.Categorical(probs)\n",
        "    action = m.sample()\n",
        "    return action.item(), m.log_prob(action)\n",
        "\n",
        "# Fonction pour calculer les retours discountés\n",
        "def compute_returns(rewards, gamma=0.99):\n",
        "    R = 0\n",
        "    returns = []\n",
        "    for r in reversed(rewards):\n",
        "        R = r + gamma * R\n",
        "        returns.insert(0, R)\n",
        "    returns = torch.tensor(returns, dtype=torch.float32)\n",
        "    if returns.std() > 0:\n",
        "        returns = (returns - returns.mean()) / (returns.std() + 1e-9)\n",
        "    return returns\n"
      ],
      "metadata": {
        "id": "NCmBRw27wv88"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def select_action(policy, state_tensor, valid_mask):\n",
        "    \"\"\"\n",
        "    state_tensor : tenseur de forme (1, state_size)\n",
        "    valid_mask : tableau numpy booléen de forme (action_size,)\n",
        "    Renvoie (action_index, log_prob)\n",
        "    \"\"\"\n",
        "    # Si aucune action n'est valide, on retourne une action par défaut (ici l'action 0)\n",
        "    if not valid_mask.any():\n",
        "        # On peut également choisir d'interrompre l'épisode ou de gérer autrement cette situation.\n",
        "        return 0, torch.tensor(0.0)\n",
        "\n",
        "    logits = policy(state_tensor)  # forme (1, action_size)\n",
        "    valid_mask_tensor = torch.tensor(valid_mask, dtype=torch.bool, device=logits.device)\n",
        "    # Mettre les logits des actions invalides à -infinity\n",
        "    logits[0][~valid_mask_tensor] = -float('inf')\n",
        "\n",
        "    # Calcul du softmax\n",
        "    probs = F.softmax(logits, dim=1)\n",
        "\n",
        "    # En cas d'anomalie (probs contenant des nan), on renvoie une action par défaut\n",
        "    if torch.isnan(probs).any():\n",
        "        return 0, torch.tensor(0.0)\n",
        "\n",
        "    m = torch.distributions.Categorical(probs)\n",
        "    action = m.sample()\n",
        "    return action.item(), m.log_prob(action)\n"
      ],
      "metadata": {
        "id": "UnNdFehuw2k-"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_reinforce(num_episodes, gamma=0.99):\n",
        "    \"\"\"\n",
        "    Entraîne les politiques REINFORCE pour le Bobail et les pions.\n",
        "    Pour chaque épisode, on collecte les log-probabilités et récompenses, calcule les retours\n",
        "    discountés et met à jour les réseaux par descente de gradient.\n",
        "    \"\"\"\n",
        "    env = BobailEnv()\n",
        "    all_rewards = []\n",
        "\n",
        "    for ep in range(num_episodes):\n",
        "        env.reset()\n",
        "        log_probs = []  # Liste de tuples (log_prob, type) où type est \"pawn\" ou \"bobail\"\n",
        "        rewards = []\n",
        "\n",
        "        state = encode_state(env)\n",
        "        state_tensor = torch.tensor(np.array(state), dtype=torch.float32).unsqueeze(0)\n",
        "\n",
        "        while not env.done:\n",
        "            if env.first_turn:\n",
        "                # Premier tour : seul le mouvement d'un pion est réalisé\n",
        "                valid_mask = get_valid_mask_pawn(env)\n",
        "                action_idx, log_prob = select_action(pawn_policy, state_tensor, valid_mask)\n",
        "                try:\n",
        "                    env.step(None, (pawn_action_space[action_idx][0],\n",
        "                                    pawn_action_space[action_idx][1]),\n",
        "                             pawn_action_space[action_idx][2])\n",
        "                except Exception as e:\n",
        "                    rewards.append(-1.0)\n",
        "                    break\n",
        "                log_probs.append((log_prob, \"pawn\"))\n",
        "            else:\n",
        "                # Tour complet : déplacement du Bobail, puis déplacement d'un pion\n",
        "                valid_mask = get_valid_mask_bobail(env)\n",
        "                action_idx, log_prob = select_action(bobail_policy, state_tensor, valid_mask)\n",
        "                try:\n",
        "                    env.move_bobail(bobail_action_space[action_idx])\n",
        "                except Exception as e:\n",
        "                    rewards.append(-1.0)\n",
        "                    break\n",
        "                log_probs.append((log_prob, \"bobail\"))\n",
        "\n",
        "                valid_mask = get_valid_mask_pawn(env)\n",
        "                action_idx, log_prob = select_action(pawn_policy, state_tensor, valid_mask)\n",
        "                try:\n",
        "                    env.move_pawn((pawn_action_space[action_idx][0],\n",
        "                                   pawn_action_space[action_idx][1]),\n",
        "                                  pawn_action_space[action_idx][2])\n",
        "                except Exception as e:\n",
        "                    rewards.append(-1.0)\n",
        "                    break\n",
        "                log_probs.append((log_prob, \"pawn\"))\n",
        "\n",
        "                # Passage au joueur suivant\n",
        "                env.current_player = 3 - env.current_player\n",
        "\n",
        "            # Attribution de récompense : petite pénalité par coup, récompense terminale en fin d'épisode\n",
        "            if env.done:\n",
        "                if env.winner is not None:\n",
        "                    reward = 1.0 if env.winner == 1 else -1.0\n",
        "                else:\n",
        "                    reward = -1.0\n",
        "            else:\n",
        "                reward = -0.01\n",
        "            rewards.append(reward)\n",
        "\n",
        "            state = encode_state(env)\n",
        "            state_tensor = torch.tensor(np.array(state), dtype=torch.float32).unsqueeze(0)\n",
        "\n",
        "        returns = compute_returns(rewards, gamma)\n",
        "        policy_loss = 0\n",
        "        for (log_prob, _), R in zip(log_probs, returns):\n",
        "            policy_loss -= log_prob * R\n",
        "\n",
        "        pawn_optimizer.zero_grad()\n",
        "        bobail_optimizer.zero_grad()\n",
        "        policy_loss.backward()\n",
        "        pawn_optimizer.step()\n",
        "        bobail_optimizer.step()\n",
        "\n",
        "        total_reward = sum(rewards)\n",
        "        all_rewards.append(total_reward)\n",
        "        if ep % 10 == 0:\n",
        "            print(f\"Episode {ep}: Total Reward = {total_reward:.2f}\")\n",
        "\n",
        "    return all_rewards\n",
        "\n",
        "# Exemple d'entraînement REINFORCE sur 100 épisodes\n",
        "rewards_reinforce = train_reinforce(100)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "573sTkQmw9Wy",
        "outputId": "4f68d956-f21c-4680-8d25-6a46170abeeb"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 0: Total Reward = 0.90\n",
            "Episode 10: Total Reward = 0.76\n",
            "Episode 20: Total Reward = -1.19\n",
            "Episode 30: Total Reward = 0.92\n",
            "Episode 40: Total Reward = -1.23\n",
            "Episode 50: Total Reward = 0.96\n",
            "Episode 60: Total Reward = -1.13\n",
            "Episode 70: Total Reward = 0.84\n",
            "Episode 80: Total Reward = -1.05\n",
            "Episode 90: Total Reward = 0.82\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_reinforce_policy(num_episodes):\n",
        "    \"\"\"\n",
        "    Évalue les politiques apprises en REINFORCE sur num_episodes parties.\n",
        "    Pour chaque épisode, le choix d'action se fait en sélectionnant l'action de probabilité maximale.\n",
        "    Affiche le taux de victoire pour le joueur 1, le taux de victoire pour le joueur 2\n",
        "    et la longueur moyenne des parties.\n",
        "    \"\"\"\n",
        "    env = BobailEnv()\n",
        "    wins = {1: 0, 2: 0}\n",
        "    total_steps = 0\n",
        "\n",
        "    for _ in range(num_episodes):\n",
        "        env.reset()\n",
        "        state = encode_state(env)\n",
        "        state_tensor = torch.tensor(np.array(state), dtype=torch.float32).unsqueeze(0)\n",
        "        steps = 0\n",
        "\n",
        "        while not env.done:\n",
        "            if env.first_turn:\n",
        "                valid_mask = get_valid_mask_pawn(env)\n",
        "                logits = pawn_policy(state_tensor)\n",
        "                valid_mask_tensor = torch.tensor(valid_mask, dtype=torch.bool, device=logits.device)\n",
        "                logits[0][~valid_mask_tensor] = -float('inf')\n",
        "                probs = F.softmax(logits, dim=1)\n",
        "                action_idx = int(torch.argmax(probs, dim=1).item())\n",
        "                try:\n",
        "                    env.step(None, (pawn_action_space[action_idx][0],\n",
        "                                    pawn_action_space[action_idx][1]),\n",
        "                             pawn_action_space[action_idx][2])\n",
        "                except Exception as e:\n",
        "                    break\n",
        "            else:\n",
        "                valid_mask = get_valid_mask_bobail(env)\n",
        "                logits = bobail_policy(state_tensor)\n",
        "                valid_mask_tensor = torch.tensor(valid_mask, dtype=torch.bool, device=logits.device)\n",
        "                logits[0][~valid_mask_tensor] = -float('inf')\n",
        "                probs = F.softmax(logits, dim=1)\n",
        "                action_idx = int(torch.argmax(probs, dim=1).item())\n",
        "                try:\n",
        "                    env.move_bobail(bobail_action_space[action_idx])\n",
        "                except Exception as e:\n",
        "                    break\n",
        "\n",
        "                if env.done:\n",
        "                    break\n",
        "\n",
        "                valid_mask = get_valid_mask_pawn(env)\n",
        "                logits = pawn_policy(state_tensor)\n",
        "                valid_mask_tensor = torch.tensor(valid_mask, dtype=torch.bool, device=logits.device)\n",
        "                logits[0][~valid_mask_tensor] = -float('inf')\n",
        "                probs = F.softmax(logits, dim=1)\n",
        "                action_idx = int(torch.argmax(probs, dim=1).item())\n",
        "                try:\n",
        "                    env.move_pawn((pawn_action_space[action_idx][0],\n",
        "                                   pawn_action_space[action_idx][1]),\n",
        "                                  pawn_action_space[action_idx][2])\n",
        "                except Exception as e:\n",
        "                    break\n",
        "\n",
        "                env.current_player = 3 - env.current_player\n",
        "\n",
        "            steps += 1\n",
        "            state = encode_state(env)\n",
        "            state_tensor = torch.tensor(np.array(state), dtype=torch.float32).unsqueeze(0)\n",
        "\n",
        "        if env.winner in wins:\n",
        "            wins[env.winner] += 1\n",
        "        total_steps += steps\n",
        "\n",
        "    avg_steps = total_steps / num_episodes\n",
        "    win_rate_1 = wins[1] / num_episodes * 100\n",
        "    win_rate_2 = wins[2] / num_episodes * 100\n",
        "\n",
        "    print(f\"Évaluation sur {num_episodes} épisodes :\")\n",
        "    print(f\"  Taux de victoire Joueur 1 : {win_rate_1:.2f}%\")\n",
        "    print(f\"  Taux de victoire Joueur 2 : {win_rate_2:.2f}%\")\n",
        "    print(f\"  Longueur moyenne d'une partie : {avg_steps:.2f} coups\")\n",
        "\n",
        "# Exemple d'évaluation sur 100 épisodes\n",
        "evaluate_reinforce_policy(100)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "izYyEvEIw__n",
        "outputId": "60c0d9c5-bd52-4807-d5fa-14da426af4b8"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Évaluation sur 100 épisodes :\n",
            "  Taux de victoire Joueur 1 : 0.00%\n",
            "  Taux de victoire Joueur 2 : 0.00%\n",
            "  Longueur moyenne d'une partie : 16.00 coups\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test jouer humain vs machine"
      ],
      "metadata": {
        "id": "N-7Dy4XoxGAY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Supposons que vous avez déjà défini ou entraîné vos agents.\n",
        "# Ici, on définit deux options :\n",
        "#   - Agent Random (None)\n",
        "#   - Agent REINFORCE (dont la politique est représentée par pawn_policy)\n",
        "agents_available = {\n",
        "    \"1\": {\"name\": \"Agent Random\", \"agent\": None},\n",
        "    \"2\": {\"name\": \"Agent REINFORCE\", \"agent\": pawn_policy}  # On suppose que pawn_policy est défini et entraîné\n",
        "}\n",
        "\n",
        "def choose_agent():\n",
        "    print(\"Choisissez l'agent à affronter :\")\n",
        "    for key, info in agents_available.items():\n",
        "        print(f\"{key}. {info['name']}\")\n",
        "    choix = input(\"Votre choix : \")\n",
        "    if choix in agents_available:\n",
        "        print(f\"Vous allez affronter {agents_available[choix]['name']}.\")\n",
        "        return agents_available[choix][\"agent\"], agents_available[choix][\"name\"]\n",
        "    else:\n",
        "        print(\"Choix invalide, on choisit par défaut l'agent Random.\")\n",
        "        return None, \"Agent Random\"\n",
        "\n",
        "selected_agent, agent_name = choose_agent()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B3WN8EsKxGqo",
        "outputId": "5965800d-0cc0-43ff-d2b3-0550e1f89897"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Choisissez l'agent à affronter :\n",
            "1. Agent Random\n",
            "2. Agent REINFORCE\n",
            "Votre choix : 2\n",
            "Vous allez affronter Agent REINFORCE.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nouvel Environnement + test jeu"
      ],
      "metadata": {
        "id": "g3l-Sfp4ximE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Codes couleur ANSI\n",
        "GREEN = \"\\033[92m\"   # Joueur 1\n",
        "RED = \"\\033[91m\"     # Joueur 2\n",
        "YELLOW = \"\\033[93m\"  # Bobail\n",
        "RESET = \"\\033[0m\"    # Réinitialisation\n",
        "\n",
        "class BobailEnv:\n",
        "    def __init__(self):\n",
        "        self.board_size = 5\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        # Plateau vide (0)\n",
        "        self.board = np.zeros((self.board_size, self.board_size), dtype=int)\n",
        "        # Placement des pions :\n",
        "        # Joueur 1 (1) sur la première ligne et Joueur 2 (2) sur la dernière ligne.\n",
        "        for j in range(self.board_size):\n",
        "            self.board[0, j] = 1\n",
        "            self.board[self.board_size - 1, j] = 2\n",
        "        # Placement du Bobail (3) au centre.\n",
        "        self.bobail_pos = (self.board_size // 2, self.board_size // 2)\n",
        "        self.board[self.bobail_pos] = 3\n",
        "        # Positions initiales (conditions de victoire) :\n",
        "        self.initial_positions = {\n",
        "            1: {(0, j) for j in range(self.board_size)},\n",
        "            2: {(self.board_size - 1, j) for j in range(self.board_size)}\n",
        "        }\n",
        "        self.current_player = 1  # Le joueur 1 commence\n",
        "        self.first_turn = True   # Premier tour : seul un pion est déplacé\n",
        "        self.done = False\n",
        "        self.winner = None\n",
        "        return self.board.copy()\n",
        "\n",
        "    def in_bounds(self, x, y):\n",
        "        return 0 <= x < self.board_size and 0 <= y < self.board_size\n",
        "\n",
        "    def move_bobail(self, direction):\n",
        "        if self.first_turn:\n",
        "            raise ValueError(\"Le Bobail ne doit pas être déplacé lors du premier tour.\")\n",
        "        x, y = self.bobail_pos\n",
        "        new_x, new_y = x + direction[0], y + direction[1]\n",
        "        if not self.in_bounds(new_x, new_y) or self.board[new_x, new_y] != 0:\n",
        "            raise ValueError(\"Mouvement du Bobail invalide.\")\n",
        "        # Mettre à jour le plateau\n",
        "        self.board[x, y] = 0\n",
        "        self.bobail_pos = (new_x, new_y)\n",
        "        self.board[new_x, new_y] = 3\n",
        "        # Vérifier la victoire : si le Bobail arrive sur une case initiale de n'importe quel joueur\n",
        "        for player, positions in self.initial_positions.items():\n",
        "            if (new_x, new_y) in positions:\n",
        "                print(f\"{YELLOW}Victoire détectée pour le Joueur {player} ! Le Bobail est arrivé en {new_x, new_y}.{RESET}\")\n",
        "                self.done = True\n",
        "                self.winner = player\n",
        "                break\n",
        "\n",
        "    def get_valid_bobail_moves(self):\n",
        "        directions = [(-1,-1), (-1,0), (-1,1), (0,-1), (0,1), (1,-1), (1,0), (1,1)]\n",
        "        valid_moves = []\n",
        "        x, y = self.bobail_pos\n",
        "        for d in directions:\n",
        "            new_x, new_y = x + d[0], y + d[1]\n",
        "            if self.in_bounds(new_x, new_y) and self.board[new_x, new_y] == 0:\n",
        "                valid_moves.append(d)\n",
        "        return valid_moves\n",
        "\n",
        "    def get_valid_pawn_moves(self, pawn_pos):\n",
        "        directions = [(-1,-1), (-1,0), (-1,1), (0,-1), (0,1), (1,-1), (1,0), (1,1)]\n",
        "        valid_moves = []\n",
        "        x, y = pawn_pos\n",
        "        for d in directions:\n",
        "            new_x, new_y = x, y\n",
        "            next_x, next_y = new_x + d[0], new_y + d[1]\n",
        "            if not self.in_bounds(next_x, next_y) or self.board[next_x, next_y] != 0:\n",
        "                continue\n",
        "            while self.in_bounds(new_x + d[0], new_y + d[1]) and self.board[new_x + d[0], new_y + d[1]] == 0:\n",
        "                new_x += d[0]\n",
        "                new_y += d[1]\n",
        "            valid_moves.append((d, (new_x, new_y)))\n",
        "        return valid_moves\n",
        "\n",
        "    def move_pawn(self, pawn_pos, direction):\n",
        "        if self.board[pawn_pos] != self.current_player:\n",
        "            raise ValueError(\"Ce pion n'appartient pas au joueur courant.\")\n",
        "        valid_moves = self.get_valid_pawn_moves(pawn_pos)\n",
        "        move_final = None\n",
        "        for d, pos in valid_moves:\n",
        "            if d == direction:\n",
        "                move_final = pos\n",
        "                break\n",
        "        if move_final is None:\n",
        "            raise ValueError(\"Mouvement du pion invalide.\")\n",
        "        self.board[pawn_pos] = 0\n",
        "        self.board[move_final] = self.current_player\n",
        "\n",
        "    def step(self, bobail_direction, pawn_move):\n",
        "        if not self.done:\n",
        "            if self.first_turn:\n",
        "                pawn_pos, pawn_direction = pawn_move\n",
        "                self.move_pawn(pawn_pos, pawn_direction)\n",
        "                self.first_turn = False\n",
        "            else:\n",
        "                self.move_bobail(bobail_direction)\n",
        "                if self.done:\n",
        "                    self.render()\n",
        "                    print(f\"{YELLOW}Partie terminée. Le joueur {self.winner} a gagné !{RESET}\")\n",
        "                    self.ask_replay()\n",
        "                    return\n",
        "                pawn_pos, pawn_direction = pawn_move\n",
        "                self.move_pawn(pawn_pos, pawn_direction)\n",
        "                self.current_player = 3 - self.current_player\n",
        "            self.render()\n",
        "            if self.done:\n",
        "                print(f\"{YELLOW}Partie terminée. Le joueur {self.winner} a gagné !{RESET}\")\n",
        "                self.ask_replay()\n",
        "\n",
        "    def ask_replay(self):\n",
        "        response = input(\"Souhaitez-vous rejouer ? (o/n) : \").strip().lower()\n",
        "        if response == 'o':\n",
        "            self.reset()\n",
        "            self.render()\n",
        "\n",
        "    def render(self):\n",
        "        for i in range(self.board_size):\n",
        "            row_str = \"\"\n",
        "            for j in range(self.board_size):\n",
        "                cell = self.board[i, j]\n",
        "                if cell == 0:\n",
        "                    row_str += \". \"\n",
        "                elif cell == 1:\n",
        "                    row_str += GREEN + \"1 \" + RESET\n",
        "                elif cell == 2:\n",
        "                    row_str += RED + \"2 \" + RESET\n",
        "                elif cell == 3:\n",
        "                    row_str += YELLOW + \"B \" + RESET\n",
        "            print(row_str)\n",
        "        print()\n"
      ],
      "metadata": {
        "id": "ZUV_BWWAxjH6"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "# On suppose ici que encode_state, get_valid_mask_pawn et get_valid_mask_bobail\n",
        "# ainsi que les listes d'actions pawn_action_space et bobail_action_space sont définies.\n",
        "# Pour cet exemple, on reconstruit pawn_action_space et bobail_action_space :\n",
        "pawn_action_space = []\n",
        "directions_list = [(-1,-1), (-1,0), (-1,1), (0,-1), (0,1), (1,-1), (1,0), (1,1)]\n",
        "for i in range(5):\n",
        "    for j in range(5):\n",
        "        for d in directions_list:\n",
        "            pawn_action_space.append((i, j, d))\n",
        "bobail_action_space = directions_list\n",
        "\n",
        "def get_valid_mask_bobail(env):\n",
        "    valid_moves = env.get_valid_bobail_moves()\n",
        "    mask = np.array([move in valid_moves for move in bobail_action_space], dtype=bool)\n",
        "    return mask\n",
        "\n",
        "def get_valid_mask_pawn(env):\n",
        "    mask = np.zeros(len(pawn_action_space), dtype=bool)\n",
        "    for idx, (i, j, d) in enumerate(pawn_action_space):\n",
        "        if env.board[i, j] == env.current_player:\n",
        "            valid_moves = env.get_valid_pawn_moves((i, j))\n",
        "            valid_dirs = [move[0] for move in valid_moves]\n",
        "            if d in valid_dirs:\n",
        "                mask[idx] = True\n",
        "    return mask\n",
        "\n",
        "def encode_state(env):\n",
        "    board_flat = env.board.flatten().astype(np.float32)\n",
        "    current_player = np.array([0.0 if env.current_player == 1 else 1.0], dtype=np.float32)\n",
        "    first_turn = np.array([1.0 if env.first_turn else 0.0], dtype=np.float32)\n",
        "    return tuple(np.concatenate([board_flat, current_player, first_turn]))\n",
        "\n",
        "def play_against_agent_simplified(trained_agent, agent_name, human_player=1):\n",
        "    \"\"\"\n",
        "    Interface interactive simplifiée pour jouer contre un agent.\n",
        "    Si trained_agent est None, l'agent adverse jouera de manière aléatoire.\n",
        "    Vous jouez toujours en tant que Joueur 1.\n",
        "    \"\"\"\n",
        "    env = BobailEnv()\n",
        "    env.reset()\n",
        "    print(\"Plateau initial :\")\n",
        "    env.render()\n",
        "\n",
        "    print(f\"Vous êtes le Joueur {human_player}.\")\n",
        "    print(f\"L'agent adverse est {agent_name} (Joueur {3 - human_player}).\")\n",
        "\n",
        "    # Mapping pour simplifier la saisie des directions.\n",
        "    direction_mapping = {\n",
        "        \"U\": (-1, 0),\n",
        "        \"D\": (1, 0),\n",
        "        \"L\": (0, -1),\n",
        "        \"R\": (0, 1),\n",
        "        \"UL\": (-1, -1),\n",
        "        \"UR\": (-1, 1),\n",
        "        \"DL\": (1, -1),\n",
        "        \"DR\": (1, 1)\n",
        "    }\n",
        "\n",
        "    while not env.done:\n",
        "        if env.current_player == human_player:\n",
        "            print(\"\\n--- Votre tour ---\")\n",
        "            if env.first_turn:\n",
        "                print(\"Premier tour : déplacez uniquement un pion.\")\n",
        "                pos_str = input(\"Entrez la position du pion (ligne col, ex: '0 2') : \")\n",
        "                try:\n",
        "                    row, col = map(int, pos_str.split())\n",
        "                except:\n",
        "                    print(\"Entrée invalide. Réessayez.\")\n",
        "                    continue\n",
        "                print(\"Choisissez une direction parmi : U, D, L, R, UL, UR, DL, DR\")\n",
        "                d_input = input(\"Votre direction : \").upper().strip()\n",
        "                if d_input not in direction_mapping:\n",
        "                    print(\"Direction invalide.\")\n",
        "                    continue\n",
        "                pawn_direction = direction_mapping[d_input]\n",
        "                try:\n",
        "                    # Ici, on passe une action pour le pion sous forme de tuple ((row, col), pawn_direction)\n",
        "                    env.step(None, ((row, col), pawn_direction))\n",
        "                except Exception as e:\n",
        "                    print(\"Mouvement invalide:\", e)\n",
        "                    continue\n",
        "            else:\n",
        "                print(\"Tour complet : d'abord déplacez le Bobail, puis un pion.\")\n",
        "                print(\"Pour le Bobail, choisissez une direction parmi : U, D, L, R, UL, UR, DL, DR\")\n",
        "                d_input = input(\"Direction du Bobail : \").upper().strip()\n",
        "                if d_input not in direction_mapping:\n",
        "                    print(\"Direction invalide pour le Bobail.\")\n",
        "                    continue\n",
        "                bobail_direction = direction_mapping[d_input]\n",
        "                try:\n",
        "                    env.move_bobail(bobail_direction)\n",
        "                except Exception as e:\n",
        "                    print(\"Mouvement du Bobail invalide:\", e)\n",
        "                    continue\n",
        "                print(\"Pour le pion, entrez la position du pion à déplacer (ligne col, ex: '4 2') :\")\n",
        "                pos_str = input(\"Position du pion : \")\n",
        "                try:\n",
        "                    row, col = map(int, pos_str.split())\n",
        "                except:\n",
        "                    print(\"Entrée invalide. Réessayez.\")\n",
        "                    continue\n",
        "                print(\"Choisissez une direction pour le pion parmi : U, D, L, R, UL, UR, DL, DR\")\n",
        "                d_input = input(\"Direction du pion : \").upper().strip()\n",
        "                if d_input not in direction_mapping:\n",
        "                    print(\"Direction invalide.\")\n",
        "                    continue\n",
        "                pawn_direction = direction_mapping[d_input]\n",
        "                try:\n",
        "                    env.move_pawn((row, col), pawn_direction)\n",
        "                except Exception as e:\n",
        "                    print(\"Mouvement du pion invalide:\", e)\n",
        "                    continue\n",
        "                env.current_player = 3 - env.current_player\n",
        "        else:\n",
        "            print(f\"\\n--- Tour de l'agent ({agent_name}) ---\")\n",
        "            state = encode_state(env)\n",
        "            state_tensor = torch.tensor(np.array(state), dtype=torch.float32).unsqueeze(0)\n",
        "            if env.first_turn:\n",
        "                valid_mask = get_valid_mask_pawn(env)\n",
        "                if trained_agent is None:\n",
        "                    valid_indices = np.where(valid_mask)[0]\n",
        "                    if len(valid_indices) == 0:\n",
        "                        print(\"Aucun mouvement valide pour l'agent.\")\n",
        "                        break\n",
        "                    action_idx = int(np.random.choice(valid_indices))\n",
        "                else:\n",
        "                    logits = trained_agent(state_tensor)\n",
        "                    valid_mask_tensor = torch.tensor(valid_mask, dtype=torch.bool, device=logits.device)\n",
        "                    logits[0][~valid_mask_tensor] = -float('inf')\n",
        "                    probs = F.softmax(logits, dim=1)\n",
        "                    action_idx = int(torch.argmax(probs, dim=1).item())\n",
        "                try:\n",
        "                    env.step(None, ((pawn_action_space[action_idx][0],\n",
        "                                     pawn_action_space[action_idx][1]),\n",
        "                                    pawn_action_space[action_idx][2]))\n",
        "                except Exception as e:\n",
        "                    print(\"Erreur lors du mouvement de l'agent :\", e)\n",
        "                    break\n",
        "            else:\n",
        "                # Tour complet pour l'agent : d'abord déplacer le Bobail, puis le pion.\n",
        "                valid_mask = get_valid_mask_bobail(env)\n",
        "                if trained_agent is None:\n",
        "                    valid_indices = np.where(valid_mask)[0]\n",
        "                    if len(valid_indices) == 0:\n",
        "                        print(\"Aucun mouvement valide pour l'agent.\")\n",
        "                        break\n",
        "                    action_idx = int(np.random.choice(valid_indices))\n",
        "                    bobail_move = bobail_action_space[action_idx]\n",
        "                else:\n",
        "                    logits = bobail_policy(state_tensor)\n",
        "                    valid_mask_tensor = torch.tensor(valid_mask, dtype=torch.bool, device=logits.device)\n",
        "                    logits[0][~valid_mask_tensor] = -float('inf')\n",
        "                    probs = F.softmax(logits, dim=1)\n",
        "                    action_idx = int(torch.argmax(probs, dim=1).item())\n",
        "                    bobail_move = bobail_action_space[action_idx]\n",
        "                try:\n",
        "                    env.move_bobail(bobail_move)\n",
        "                except Exception as e:\n",
        "                    print(\"Erreur lors du mouvement du Bobail par l'agent :\", e)\n",
        "                    break\n",
        "                valid_mask = get_valid_mask_pawn(env)\n",
        "                if trained_agent is None:\n",
        "                    valid_indices = np.where(valid_mask)[0]\n",
        "                    if len(valid_indices) == 0:\n",
        "                        print(\"Aucun mouvement valide pour l'agent.\")\n",
        "                        break\n",
        "                    action_idx = int(np.random.choice(valid_indices))\n",
        "                else:\n",
        "                    logits = pawn_policy(state_tensor)\n",
        "                    valid_mask_tensor = torch.tensor(valid_mask, dtype=torch.bool, device=logits.device)\n",
        "                    logits[0][~valid_mask_tensor] = -float('inf')\n",
        "                    probs = F.softmax(logits, dim=1)\n",
        "                    action_idx = int(torch.argmax(probs, dim=1).item())\n",
        "                try:\n",
        "                    env.move_pawn((pawn_action_space[action_idx][0],\n",
        "                                   pawn_action_space[action_idx][1]),\n",
        "                                  pawn_action_space[action_idx][2])\n",
        "                except Exception as e:\n",
        "                    print(\"Erreur lors du mouvement du pion par l'agent :\", e)\n",
        "                    break\n",
        "                env.current_player = 3 - env.current_player\n",
        "\n",
        "            print(\"\\nPlateau actuel :\")\n",
        "            env.render()\n",
        "\n",
        "        # Fin de la boucle interactive\n",
        "    print(\"\\nPartie terminée.\")\n",
        "    if env.winner:\n",
        "        print(\"Le gagnant est le Joueur\", env.winner)\n",
        "    else:\n",
        "        print(\"Partie terminée sans vainqueur.\")\n",
        "\n",
        "# Lancer la partie interactive simplifiée.\n",
        "play_against_agent_simplified(selected_agent, agent_name, human_player=1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3WnL_upixoig",
        "outputId": "23e653a0-2166-48cf-e17c-828d270073ea"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Plateau initial :\n",
            "\u001b[92m1 \u001b[0m\u001b[92m1 \u001b[0m\u001b[92m1 \u001b[0m\u001b[92m1 \u001b[0m\u001b[92m1 \u001b[0m\n",
            ". . . . . \n",
            ". . \u001b[93mB \u001b[0m. . \n",
            ". . . . . \n",
            "\u001b[91m2 \u001b[0m\u001b[91m2 \u001b[0m\u001b[91m2 \u001b[0m\u001b[91m2 \u001b[0m\u001b[91m2 \u001b[0m\n",
            "\n",
            "Vous êtes le Joueur 1.\n",
            "L'agent adverse est Agent REINFORCE (Joueur 2).\n",
            "\n",
            "--- Votre tour ---\n",
            "Premier tour : déplacez uniquement un pion.\n",
            "Entrez la position du pion (ligne col, ex: '0 2') : 0 0\n",
            "Choisissez une direction parmi : U, D, L, R, UL, UR, DL, DR\n",
            "Votre direction : dl\n",
            "Mouvement invalide: Mouvement du pion invalide.\n",
            "\n",
            "--- Votre tour ---\n",
            "Premier tour : déplacez uniquement un pion.\n",
            "Entrez la position du pion (ligne col, ex: '0 2') : 0 0\n",
            "Choisissez une direction parmi : U, D, L, R, UL, UR, DL, DR\n",
            "Votre direction : d\n",
            ". \u001b[92m1 \u001b[0m\u001b[92m1 \u001b[0m\u001b[92m1 \u001b[0m\u001b[92m1 \u001b[0m\n",
            ". . . . . \n",
            ". . \u001b[93mB \u001b[0m. . \n",
            "\u001b[92m1 \u001b[0m. . . . \n",
            "\u001b[91m2 \u001b[0m\u001b[91m2 \u001b[0m\u001b[91m2 \u001b[0m\u001b[91m2 \u001b[0m\u001b[91m2 \u001b[0m\n",
            "\n",
            "\n",
            "--- Votre tour ---\n",
            "Tour complet : d'abord déplacez le Bobail, puis un pion.\n",
            "Pour le Bobail, choisissez une direction parmi : U, D, L, R, UL, UR, DL, DR\n",
            "Direction du Bobail : d\n",
            "Pour le pion, entrez la position du pion à déplacer (ligne col, ex: '4 2') :\n",
            "Position du pion : 0 4\n",
            "Choisissez une direction pour le pion parmi : U, D, L, R, UL, UR, DL, DR\n",
            "Direction du pion : d\n",
            "\n",
            "--- Tour de l'agent (Agent REINFORCE) ---\n",
            "\n",
            "Plateau actuel :\n",
            ". \u001b[92m1 \u001b[0m\u001b[92m1 \u001b[0m\u001b[92m1 \u001b[0m. \n",
            ". . . . . \n",
            ". . \u001b[93mB \u001b[0m. . \n",
            "\u001b[92m1 \u001b[0m\u001b[91m2 \u001b[0m. . \u001b[92m1 \u001b[0m\n",
            ". \u001b[91m2 \u001b[0m\u001b[91m2 \u001b[0m\u001b[91m2 \u001b[0m\u001b[91m2 \u001b[0m\n",
            "\n",
            "\n",
            "--- Votre tour ---\n",
            "Tour complet : d'abord déplacez le Bobail, puis un pion.\n",
            "Pour le Bobail, choisissez une direction parmi : U, D, L, R, UL, UR, DL, DR\n",
            "Direction du Bobail : ur\n",
            "Pour le pion, entrez la position du pion à déplacer (ligne col, ex: '4 2') :\n",
            "Position du pion : 3 4\n",
            "Choisissez une direction pour le pion parmi : U, D, L, R, UL, UR, DL, DR\n",
            "Direction du pion : l\n",
            "\n",
            "--- Tour de l'agent (Agent REINFORCE) ---\n",
            "\u001b[93mVictoire détectée pour le Joueur 1 ! Le Bobail est arrivé en (0, 4).\u001b[0m\n",
            "\n",
            "Plateau actuel :\n",
            ". \u001b[92m1 \u001b[0m\u001b[92m1 \u001b[0m\u001b[92m1 \u001b[0m\u001b[93mB \u001b[0m\n",
            ". . . . \u001b[91m2 \u001b[0m\n",
            ". . . . . \n",
            "\u001b[92m1 \u001b[0m\u001b[91m2 \u001b[0m\u001b[92m1 \u001b[0m. . \n",
            ". \u001b[91m2 \u001b[0m\u001b[91m2 \u001b[0m\u001b[91m2 \u001b[0m. \n",
            "\n",
            "\n",
            "Partie terminée.\n",
            "Le gagnant est le Joueur 1\n"
          ]
        }
      ]
    }
  ]
}
